{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make file file .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading file tokenizer.json from cache at None\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.2/snapshots/67c9c25b46986521ca33df05d8540da1210b3256/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from make_data_masked import make_masked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Started...\n",
      "Data Processing done for conllsrl. File saved at ./dataMeddle/bert-base-uncased_prediction_data/ner_coNLL_testa_abolish_predicate.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Eval: 100%|██████████| 1/1 [00:02<00:00,  2.68s/it]\n"
     ]
    }
   ],
   "source": [
    "allIdsPre, allLabelsPre, allPredictPre, allIndexChangePre, allSequenceOutputPre = make_masked_data(\n",
    "    pred_file_path='./ner_coNLL_testa_abolish_predicate.tsv',\n",
    "    out_dir='./dataMeddle',\n",
    "    has_labels=True,\n",
    "    task_name='conllsrl',\n",
    "    saved_model_path='./SRL_model/conll_ner_pos_bert_base/multi_task_model_3_2828.pt',\n",
    "    type_mask='predicate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Started...\n",
      "Data Processing done for conllsrl. File saved at ./dataMeddle/bert-base-uncased_prediction_data/ner_coNLL_testa_abolish_depen.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "Eval: 100%|██████████| 1/1 [00:08<00:00,  8.45s/it]\n"
     ]
    }
   ],
   "source": [
    "allIdsDep, allLabelsDep, allPredictDep, allIndexChangeDep, allSequenceOutputDep = make_masked_data(\n",
    "    pred_file_path='./ner_coNLL_testa_abolish_depen.tsv',\n",
    "    out_dir='./dataMeddle',\n",
    "    has_labels=True,\n",
    "    task_name='conllsrl',\n",
    "    saved_model_path='./SRL_model/conll_ner_pos_bert_base/multi_task_model_3_2828.pt',\n",
    "    type_mask='depen'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Processing done for conllsrl. File saved at ./dataMeddle/bert-base-uncased_prediction_data/ner_coNLL_testa_abolish.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "allIdsOri, allLabelsOri, allPredictOri, allIndexChangeOri, allSequenceOutputOri = make_masked_data(\n",
    "    pred_file_path='ner_make_masked/ner_coNLL_testa_abolish.tsv',\n",
    "    out_dir='./dataMeddle',\n",
    "    has_labels=True,\n",
    "    task_name='conllsrl',\n",
    "    saved_model_path='./SRL_model/conll_ner_pos_bert_base/multi_task_model_3_2828.pt',\n",
    "    type_mask='origin'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataWord/ner_coNLL_testa_abolish_masked_origin.pkl', 'rb') as f:\n",
    "    allIdsOrigin, allLabelsOrigin, allLPredictionOrigin, allIndexChangeOrigin, allSequenceOutputOrigin = pkl.load(f)\n",
    "\n",
    "with open('./dataWord/ner_coNLL_testa_abolish_predicate_masked_predicate.pkl', 'rb') as f:\n",
    "    allIdsPredicate, allLabelsPredicate, allLPredictionPredicate, allIndexChangePredicate, allSequenceOutputPredicate = pkl.load(f)\n",
    "\n",
    "with open('./dataWord/ner_coNLL_testa_abolish_depen_masked_depen.pkl', 'rb') as f:\n",
    "    allIdsDepen, allLabelsDepen, allLPredictionDepen, allIndexChangeDepen, allSequenceOutputDepen = pkl.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cách check masked là A0 hay A1\n",
    "- so sánh số lượng phần tử A0 của DEPEN và ORIGIN nếu cái nào bị miss thì nó bị masked\n",
    "- get ra index của đối số A0 và A1 cũng như uid\n",
    "\n",
    "với\n",
    "- file depen:\n",
    "\t+ lấy ra predicate và xem là A0 hay A1 bị mất (lưu vào biến isMasked)\n",
    "\t+ Nếu isMasked=A0 -> thế vector wordEmbedding của depen vào ORIGIN ứng với item bị mask là A0\n",
    "\t+ tương tự với A1\n",
    "\n",
    "- file predicate\n",
    "\t+ lấy ra vector wordEmbedding của A0 và A1\n",
    "\t+ sau khi check ở trên isMasked là gì \n",
    "\t\t+Nếu là A0 thì thay tất cả các vector WE A0 vào ORIGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_elements_with_label(labels, indexChange):\n",
    "    groups = {}\n",
    "    current_group_label = None\n",
    "    current_group = []\n",
    "    for i, label in enumerate(labels):\n",
    "        if label.startswith('B-A') or label.startswith('I-A') or label == 'B-V':\n",
    "            if current_group:\n",
    "                groups.setdefault(current_group_label, []).extend(current_group)\n",
    "            if label == 'B-V':\n",
    "                current_group_label = label\n",
    "            else:\n",
    "                current_group_label = label[2:]\n",
    "            current_group = [indexChange[i]]\n",
    "        else:\n",
    "            if current_group_label and current_group:\n",
    "                groups.setdefault(current_group_label, []).extend(current_group)\n",
    "            current_group_label = None\n",
    "            current_group = []\n",
    "    if current_group_label and current_group:\n",
    "        groups.setdefault(current_group_label, []).extend(current_group)\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_differences(origin_dict, compared_dict):\n",
    "    different_values = {}\n",
    "    for key in set(origin_dict.keys()) & set(compared_dict.keys()):\n",
    "        if origin_dict[key] != compared_dict[key]:\n",
    "            if key != 'B-V' and len(origin_dict[key]) != len(compared_dict[key]):\n",
    "                different_values[key] = list(set(origin_dict[key]) ^ set(compared_dict[key]))\n",
    "    return list(different_values.keys()) if different_values else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_masked_data(allIdsOrigin, allLabelsOrigin, allIndexChangeOrigin, allLabelsPredicate, allIndexChangePredicate, allIdsDepen, allLabelsDepen, allIndexChangeDepen, allSequenceOutputOrigin, allSequenceOutputPredicate, allSequenceOutputDepen):\n",
    "    masked_data = []\n",
    "    jump_count = 0\n",
    "    \n",
    "    for i in range(len(allIdsOrigin)):\n",
    "        elements_origin = group_elements_with_label(allLabelsOrigin[i], allIndexChangeOrigin[i])\n",
    "        elements_predicate = group_elements_with_label(allLabelsPredicate[i], allIndexChangePredicate[i])\n",
    "        \n",
    "        for j in range(jump_count, len(allIdsDepen)):\n",
    "            words_embedding_data_masked = torch.tensor(allSequenceOutputOrigin[i])\n",
    "            \n",
    "            if allIdsDepen[j] == allIdsOrigin[i]:\n",
    "                elements_depen = group_elements_with_label(allLabelsDepen[j], allIndexChangeDepen[j])\n",
    "                is_different = find_differences(elements_origin, elements_depen)\n",
    "                \n",
    "                if is_different and len(is_different) == 1:\n",
    "                    arg_is_masked = is_different[0]\n",
    "                    assert len(elements_origin['B-V']) == 1, 'Predicate must be unique'\n",
    "                    \n",
    "                    words_embedding_data_masked[elements_origin['B-V'][0]] = torch.tensor(allSequenceOutputDepen[j][elements_depen['B-V'][0]])\n",
    "                    \n",
    "                    for idx_, item in enumerate(elements_origin[arg_is_masked]):\n",
    "                        words_embedding_data_masked[item] = torch.tensor(allSequenceOutputPredicate[i][elements_predicate[arg_is_masked][idx_]])\n",
    "                    \n",
    "                    masked_data.append({'uid': allIdsOrigin[i], 'label': allLabelsOrigin[i], 'masked_data': words_embedding_data_masked, 'arg_masked': arg_is_masked})\n",
    "            else:\n",
    "                jump_count = j\n",
    "                break\n",
    "    return masked_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22386/2264326547.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  words_embedding_data_masked = torch.tensor(allSequenceOutputOrigin[i])\n",
      "/tmp/ipykernel_22386/2264326547.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  words_embedding_data_masked[elements_origin['B-V'][0]] = torch.tensor(allSequenceOutputDepen[j][elements_depen['B-V'][0]])\n",
      "/tmp/ipykernel_22386/2264326547.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  words_embedding_data_masked[item] = torch.tensor(allSequenceOutputPredicate[i][elements_predicate[arg_is_masked][idx_]])\n"
     ]
    }
   ],
   "source": [
    "masked_data__ = create_masked_data(allIdsOrigin, allLabelsOrigin, allIndexChangeOrigin, allLabelsPredicate, allIndexChangePredicate, allIdsDepen, allLabelsDepen, allIndexChangeDepen, allSequenceOutputOrigin, allSequenceOutputPredicate, allSequenceOutputDepen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2719, -0.8740,  0.4671, -0.1337, -0.4177, -0.0981, -0.0940, -1.2219,\n",
       "        -0.0607,  0.0308,  0.5968,  0.8594,  0.1753, -0.3364,  0.7975,  0.3354,\n",
       "        -1.0529,  0.4115, -0.7669,  0.3295])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_data__[0]['masked_data'][1][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Data Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_data_masked import MaskedDataCreator\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 50, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/WORK/Thesis/src/model/SRL_Single_Task/generate_data_masked.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  words_embedding_data_masked = torch.tensor(allSequenceOutputOrigin[i])\n",
      "/mnt/d/WORK/Thesis/src/model/SRL_Single_Task/generate_data_masked.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  words_embedding_data_masked[elements_origin['B-V'][0]] = torch.tensor(allSequenceOutputDepen[j][elements_depen['B-V'][0]])\n",
      "/mnt/d/WORK/Thesis/src/model/SRL_Single_Task/generate_data_masked.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  words_embedding_data_masked[item] = torch.tensor(allSequenceOutputPredicate[i][elements_predicate[arg_is_masked][idx_]])\n",
      "/tmp/ipykernel_25448/3902703575.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  masked_data = torch.stack([torch.tensor(item['masked_data']) for item in masked_data_depen], dim=0)\n"
     ]
    }
   ],
   "source": [
    "masked_data_creator = MaskedDataCreator('./dataWord/ner_coNLL_testa_abolish_masked_origin.pkl', './dataWord/ner_coNLL_testa_abolish_predicate_masked_predicate.pkl', './dataWord/ner_coNLL_testa_abolish_depen_masked_depen.pkl', './ner_coNLL_testa_abolish.tsv')\n",
    "masked_data_depen = masked_data_creator.create_masked_data_depen()\n",
    "masked_data = torch.stack([torch.tensor(item['masked_data']) for item in masked_data_depen], dim=0)\n",
    "print(masked_data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from make_data_masked import WordEmbeddingAssigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/tiendat/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wordAssigner = WordEmbeddingAssigner(pred_file_path='./ner_coNLL_testa_abolish.tsv', out_dir='./pathToPredict', has_labels=True, task_name='conllsrl', saved_model_path='./SRL_model/conll_ner_pos_bert_base/pt_30_epoch/multi_task_model_19_408.pt', masked_data=masked_data)\n",
    "labMap = wordAssigner.get_labelMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Started...\n",
      "Data Processing done for conllsrl. File saved at ./pathToPredict/bert-base-uncased_prediction_data/ner_coNLL_testa_abolish.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiendat/anaconda3/envs/thesis/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "wordAssigner.load_and_create_data(isMasked=True, masked_data_type='depen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_uids: ['0', '0', '1', '2', '2', '3', '3', '4', '4', '5', '5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resultwordMasked = wordAssigner.assign_word_embedding_data()\n",
    "resultwordMasked = {\n",
    "    'uids': resultwordMasked[0],\n",
    "    'labels': resultwordMasked[1],\n",
    "    'preds': resultwordMasked[2],\n",
    "    'scores': resultwordMasked[3],\n",
    "    'logits': resultwordMasked[4],\n",
    "    'sequence_output': resultwordMasked[5],\n",
    "    'index_change': resultwordMasked[6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Started...\n",
      "Data Processing done for conllsrl. File saved at ./pathToPredict/bert-base-uncased_prediction_data/ner_coNLL_testa_abolish.json\n"
     ]
    }
   ],
   "source": [
    "wordAssigner.load_and_create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_uids: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "resultwordOrigin = wordAssigner.assign_word_embedding_data()\n",
    "resultwordOrigin = {\n",
    "    'uids': resultwordOrigin[0],\n",
    "    'labels': resultwordOrigin[1],\n",
    "    'preds': resultwordOrigin[2],\n",
    "    'scores': resultwordOrigin[3],\n",
    "    'logits': resultwordOrigin[4],\n",
    "    'sequence_output': resultwordOrigin[5],\n",
    "    'index_change': resultwordOrigin[6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_arg_preds(preds_origin, preds_masked, label_origin=None): # label_origin: nhãn gold\n",
    "    list_idx_arg_change = []\n",
    "    assert len(preds_origin) == len(preds_masked), 'Length of preds_origin and preds_masked must be the same'\n",
    "    for i in range(len(preds_origin)):\n",
    "        test = preds_origin[i].startswith('B-A') or preds_origin[i].startswith('I-A') or preds_masked[i].startswith('B-A') or preds_masked[i].startswith('I-A')\n",
    "        if label_origin:\n",
    "            assert len(preds_origin) == len(label_origin), 'Length of preds_origin and label_origin must be the same'\n",
    "            if test or label_origin[i].startswith('B-A') or label_origin[i].startswith('I-A'):\n",
    "                list_idx_arg_change.append(i)\n",
    "        else:\n",
    "            if test:\n",
    "                list_idx_arg_change.append(i)\n",
    "    return list_idx_arg_change\n",
    "\n",
    "def get_idx_labels_arg(labels):\n",
    "    list_idx_label_arg = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i].startswith('B-A') or labels[i].startswith('I-A'):\n",
    "            list_idx_label_arg.append(i)\n",
    "    return list_idx_label_arg\n",
    "\n",
    "def calculateInfluenceScore(outLogitsSigmoid_original, outLogitsSigmoid_meddle, list_arg_change):\n",
    "    influence_score = []\n",
    "    weight = []\n",
    "    assert len(outLogitsSigmoid_original) == len(outLogitsSigmoid_meddle) \n",
    "    for i in range(len(outLogitsSigmoid_original)):\n",
    "        if i not in list_arg_change:\n",
    "            continue\n",
    "        max_index_original = torch.argmax(outLogitsSigmoid_original[i])\n",
    "        max_index_meddle = torch.argmax(outLogitsSigmoid_meddle[i])\n",
    "        if max_index_original == max_index_meddle:\n",
    "            influence_score.append((outLogitsSigmoid_original[i][max_index_original] - outLogitsSigmoid_meddle[i][max_index_meddle]) / max(outLogitsSigmoid_original[i][max_index_original], outLogitsSigmoid_meddle[i][max_index_meddle]))\n",
    "            weight.append(1)\n",
    "        else:\n",
    "            influ_old_label = (outLogitsSigmoid_original[i][max_index_original] - outLogitsSigmoid_meddle[i][max_index_original]) / max(outLogitsSigmoid_original[i][max_index_original], outLogitsSigmoid_meddle[i][max_index_original])\n",
    "            influ_new_label = (outLogitsSigmoid_meddle[i][max_index_meddle] - outLogitsSigmoid_original[i][max_index_meddle]) / max(outLogitsSigmoid_original[i][max_index_meddle], outLogitsSigmoid_meddle[i][max_index_meddle])\n",
    "            influence_score.append(influ_old_label + influ_new_label)\n",
    "            weight.append(2)\n",
    "    return influence_score, weight\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25448/1644676194.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  resultwordOrigin['logits'][i] = torch.tensor(resultwordOrigin['logits'][i])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for i in range(len(resultwordOrigin['logits'])):\n",
    "    resultwordOrigin['logits'][i] = torch.tensor(resultwordOrigin['logits'][i])\n",
    "for i in range(len(resultwordMasked['logits'])):\n",
    "    resultwordMasked['logits'][i] = torch.tensor(resultwordMasked['logits'][i])\n",
    "    \n",
    "influenceScore, weight = calculateInfluenceScore(resultwordOrigin['logits'][2], resultwordMasked['logits'][3], get_idx_arg_preds(resultwordOrigin['preds'][2], resultwordMasked['preds'][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0396)\n"
     ]
    }
   ],
   "source": [
    "print(sum(influenceScore) / sum(weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-A1',\n",
       " 1: 'I-A1',\n",
       " 2: 'O',\n",
       " 3: 'B-V',\n",
       " 4: 'B-A0',\n",
       " 5: 'I-A0',\n",
       " 6: 'B-A4',\n",
       " 7: 'I-A4',\n",
       " 8: 'I-A3',\n",
       " 9: 'B-A2',\n",
       " 10: 'I-A2',\n",
       " 11: 'B-A3',\n",
       " 12: '[CLS]',\n",
       " 13: '[SEP]',\n",
       " 14: 'X'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-A1': 0,\n",
       " 'I-A1': 1,\n",
       " 'O': 2,\n",
       " 'B-V': 3,\n",
       " 'B-A0': 4,\n",
       " 'I-A0': 5,\n",
       " 'B-A4': 6,\n",
       " 'I-A4': 7,\n",
       " 'I-A3': 8,\n",
       " 'B-A2': 9,\n",
       " 'I-A2': 10,\n",
       " 'B-A3': 11,\n",
       " '[CLS]': 12,\n",
       " '[SEP]': 13,\n",
       " 'X': 14}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labMap = {v: k for k, v in labMap.items()}\n",
    "labMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_score(prob_origin_, prob_masked_, label_gold, label_origin, label_masked):\n",
    "    relevance = []\n",
    "    weight = []\n",
    "    jud_space = get_idx_arg_preds(label_origin, label_masked, label_gold)\n",
    "    for i in range(len(prob_origin_)):\n",
    "        if i not in jud_space:\n",
    "            continue\n",
    "        print(i)\n",
    "        max_index_origin = torch.argmax(prob_origin_[i])\n",
    "        max_index_masked = torch.argmax(prob_masked_[i])\n",
    "        idx_label_gold = labMap[str(label_gold[i])]\n",
    "        if label_gold[i] == label_origin[i] and label_gold[i] == label_masked[i]:\n",
    "            score_increase_gold = (prob_origin_[i][idx_label_gold] - prob_masked_[i][idx_label_gold])/max(prob_origin_[i][idx_label_gold], prob_masked_[i][idx_label_gold])\n",
    "            relevance.append(score_increase_gold)\n",
    "            weight.append(1)\n",
    "            print(score_increase_gold)\n",
    "        elif label_gold[i] != label_origin[i] and label_origin[i] == label_masked[i]:\n",
    "            score_increase_gold = (prob_origin_[i][idx_label_gold] - prob_masked_[i][idx_label_gold])/max(prob_origin_[i][idx_label_gold], prob_masked_[i][idx_label_gold])\n",
    "            score_decrease_mask = (prob_masked_[i][max_index_masked] - prob_origin_[i][max_index_masked])/max(prob_masked_[i][max_index_masked], prob_origin_[i][max_index_masked])\n",
    "            relevance.append((score_increase_gold + score_decrease_mask)/2)\n",
    "            weight.append(2)\n",
    "            print(score_increase_gold, score_decrease_mask)\n",
    "        elif label_origin[i] != label_masked[i] and label_masked[i] == label_gold[i]:\n",
    "            score_increase_gold = (prob_origin_[i][idx_label_gold] - prob_masked_[i][idx_label_gold])/max(prob_origin_[i][idx_label_gold], prob_masked_[i][idx_label_gold])\n",
    "            score_decrease_origin = (prob_masked_[i][max_index_origin] - prob_origin_[i][max_index_origin])/max(prob_masked_[i][max_index_origin], prob_origin_[i][max_index_origin])\n",
    "            relevance.append((score_increase_gold + score_decrease_origin)/2)\n",
    "            weight.append(2)\n",
    "            print(score_increase_gold, score_decrease_origin)\n",
    "        elif label_gold[i] != label_origin[i] and label_masked[i] != label_gold[i]:\n",
    "            score_increase_gold = (prob_origin_[i][idx_label_gold] - prob_masked_[i][idx_label_gold])/max(prob_origin_[i][idx_label_gold], prob_masked_[i][idx_label_gold])\n",
    "            score_decrease_mask = (prob_masked_[i][max_index_masked] - prob_origin_[i][max_index_masked])/max(prob_masked_[i][max_index_masked], prob_origin_[i][max_index_masked])\n",
    "            score_decrease_origin = (prob_masked_[i][max_index_origin] - prob_origin_[i][max_index_origin])/max(prob_masked_[i][max_index_origin], prob_origin_[i][max_index_origin])\n",
    "            relevance.append((score_increase_gold + score_decrease_mask + score_decrease_origin)/3)\n",
    "            weight.append(1)\n",
    "            print(score_increase_gold, score_decrease_mask, score_decrease_origin)\n",
    "        else:\n",
    "            relevance.append(0)\n",
    "        print('------------------------')\n",
    "    return relevance, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "1\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "6\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "7\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "11\n",
      "tensor(0.5635) tensor(0.1558)\n",
      "------------------------\n",
      "12\n",
      "tensor(-0.2643) tensor(0.0338) tensor(-0.3626)\n",
      "------------------------\n",
      "13\n",
      "tensor(0.3114) tensor(0.0147)\n",
      "------------------------\n",
      "14\n",
      "tensor(0.5334) tensor(-0.0566)\n",
      "------------------------\n",
      "15\n",
      "tensor(0.5534) tensor(0.3409)\n",
      "------------------------\n",
      "16\n",
      "tensor(0.6816) tensor(0.0017)\n",
      "------------------------\n",
      "17\n",
      "tensor(0.5182) tensor(0.0827)\n",
      "------------------------\n",
      "18\n",
      "tensor(0.0564) tensor(0.0944)\n",
      "------------------------\n",
      "19\n",
      "tensor(0.5379) tensor(-0.1610)\n",
      "------------------------\n",
      "20\n",
      "tensor(0.3743) tensor(0.0083)\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "relevance_in_all_words, weights = relevance_score(resultwordOrigin['logits'][0], resultwordMasked['logits'][0], resultwordOrigin['labels'][0], resultwordOrigin['preds'][0], resultwordMasked['preds'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0781)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(relevance_in_all_words) / sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def competence_score(origin, masked):\n",
    "    result = []\n",
    "    jump_count = 0\n",
    "    for i in range(len(origin['uids'])):\n",
    "        for j in range(jump_count, len(masked['uids'])):\n",
    "            if origin['uids'][i] == masked['uids'][j]:\n",
    "                influenceScore, weight_influ = calculateInfluenceScore(origin['logits'][i], masked['logits'][j], get_idx_arg_preds(origin['preds'][i], masked['preds'][j]))\n",
    "                relevance_in_all_words, weight_relevance = relevance_score(origin['logits'][i], masked['logits'][j], origin['labels'][i], origin['preds'][i], masked['preds'][j])\n",
    "                result.append(\n",
    "                    {\n",
    "                        'uid': masked['uids'][j],\n",
    "                        'influence': sum(influenceScore) / sum(weight_influ),\n",
    "                        'relevance': sum(relevance_in_all_words) / sum(weight_relevance)\n",
    "                    }\n",
    "                )\n",
    "            else :\n",
    "                jump_count = j\n",
    "                break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "1\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "6\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "7\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "11\n",
      "tensor(0.5635) tensor(0.1558)\n",
      "------------------------\n",
      "12\n",
      "tensor(-0.2643) tensor(0.0338) tensor(-0.3626)\n",
      "------------------------\n",
      "13\n",
      "tensor(0.3114) tensor(0.0147)\n",
      "------------------------\n",
      "14\n",
      "tensor(0.5334) tensor(-0.0566)\n",
      "------------------------\n",
      "15\n",
      "tensor(0.5534) tensor(0.3409)\n",
      "------------------------\n",
      "16\n",
      "tensor(0.6816) tensor(0.0017)\n",
      "------------------------\n",
      "17\n",
      "tensor(0.5182) tensor(0.0827)\n",
      "------------------------\n",
      "18\n",
      "tensor(0.0564) tensor(0.0944)\n",
      "------------------------\n",
      "19\n",
      "tensor(0.5379) tensor(-0.1610)\n",
      "------------------------\n",
      "20\n",
      "tensor(0.3743) tensor(0.0083)\n",
      "------------------------\n",
      "0\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "1\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "6\n",
      "tensor(0.1575) tensor(0.0436)\n",
      "------------------------\n",
      "7\n",
      "tensor(0.0323) tensor(0.1494)\n",
      "------------------------\n",
      "11\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "12\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "13\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "14\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "15\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "16\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "17\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "18\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "19\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "20\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "0\n",
      "tensor(0.)\n",
      "------------------------\n",
      "1\n",
      "tensor(0.)\n",
      "------------------------\n",
      "2\n",
      "tensor(0.)\n",
      "------------------------\n",
      "3\n",
      "tensor(0.)\n",
      "------------------------\n",
      "4\n",
      "tensor(0.)\n",
      "------------------------\n",
      "5\n",
      "tensor(0.)\n",
      "------------------------\n",
      "6\n",
      "tensor(0.)\n",
      "------------------------\n",
      "7\n",
      "tensor(0.)\n",
      "------------------------\n",
      "8\n",
      "tensor(0.)\n",
      "------------------------\n",
      "9\n",
      "tensor(0.)\n",
      "------------------------\n",
      "10\n",
      "tensor(0.)\n",
      "------------------------\n",
      "11\n",
      "tensor(0.)\n",
      "------------------------\n",
      "12\n",
      "tensor(0.)\n",
      "------------------------\n",
      "17\n",
      "tensor(-0.1173)\n",
      "------------------------\n",
      "18\n",
      "tensor(-0.1598) tensor(0.0345)\n",
      "------------------------\n",
      "19\n",
      "tensor(-0.1894) tensor(-0.1267)\n",
      "------------------------\n",
      "20\n",
      "tensor(0.0440)\n",
      "------------------------\n",
      "21\n",
      "tensor(0.1993) tensor(0.0610)\n",
      "------------------------\n",
      "22\n",
      "tensor(0.1202)\n",
      "------------------------\n",
      "23\n",
      "tensor(0.0436)\n",
      "------------------------\n",
      "25\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "0\n",
      "tensor(-0.1252) tensor(-0.0282)\n",
      "------------------------\n",
      "1\n",
      "tensor(0.1584)\n",
      "------------------------\n",
      "5\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "6\n",
      "tensor(0.)\n",
      "------------------------\n",
      "7\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "8\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "9\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "10\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "11\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "0\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "1\n",
      "tensor(0.)\n",
      "------------------------\n",
      "5\n",
      "tensor(0.8225) tensor(0.8733) tensor(-0.8845)\n",
      "------------------------\n",
      "6\n",
      "------------------------\n",
      "7\n",
      "tensor(0.6500) tensor(0.2613) tensor(-0.0731)\n",
      "------------------------\n",
      "8\n",
      "tensor(0.2375) tensor(0.0152)\n",
      "------------------------\n",
      "9\n",
      "tensor(0.6599) tensor(0.0196)\n",
      "------------------------\n",
      "10\n",
      "tensor(0.7640) tensor(0.0350)\n",
      "------------------------\n",
      "11\n",
      "tensor(0.3104) tensor(0.0904)\n",
      "------------------------\n",
      "3\n",
      "tensor(-0.1505) tensor(-0.1716)\n",
      "------------------------\n",
      "4\n",
      "tensor(0.2780) tensor(0.1313)\n",
      "------------------------\n",
      "5\n",
      "tensor(0.3915) tensor(0.1483)\n",
      "------------------------\n",
      "6\n",
      "tensor(0.2586) tensor(0.1404) tensor(-0.1439)\n",
      "------------------------\n",
      "7\n",
      "tensor(0.3895) tensor(0.0038)\n",
      "------------------------\n",
      "8\n",
      "tensor(0.4421) tensor(0.0064)\n",
      "------------------------\n",
      "9\n",
      "tensor(0.2749) tensor(0.0558)\n",
      "------------------------\n",
      "11\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "12\n",
      "tensor(0.)\n",
      "------------------------\n",
      "29\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "3\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "4\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "5\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "6\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "7\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "8\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "9\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "11\n",
      "tensor(0.4806) tensor(0.0123)\n",
      "------------------------\n",
      "12\n",
      "------------------------\n",
      "29\n",
      "tensor(0.1059) tensor(0.0132)\n",
      "------------------------\n",
      "2\n",
      "tensor(0.1324) tensor(0.0305)\n",
      "------------------------\n",
      "3\n",
      "tensor(-0.0787) tensor(0.1841)\n",
      "------------------------\n",
      "6\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "11\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "12\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "13\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "14\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "15\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "16\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "17\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "18\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "19\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "20\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "31\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "32\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "2\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "3\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "6\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "11\n",
      "tensor(-0.2545) tensor(0.4960) tensor(-0.5854)\n",
      "------------------------\n",
      "12\n",
      "tensor(0.3638) tensor(0.2235) tensor(0.0869)\n",
      "------------------------\n",
      "13\n",
      "tensor(-0.2825) tensor(-0.0784)\n",
      "------------------------\n",
      "14\n",
      "tensor(0.4267) tensor(0.1794)\n",
      "------------------------\n",
      "15\n",
      "tensor(0.2756) tensor(0.1239)\n",
      "------------------------\n",
      "16\n",
      "tensor(0.6613) tensor(0.1050)\n",
      "------------------------\n",
      "17\n",
      "tensor(0.4779) tensor(0.0220)\n",
      "------------------------\n",
      "18\n",
      "tensor(0.0248) tensor(0.1796) tensor(-0.1860)\n",
      "------------------------\n",
      "19\n",
      "tensor(0.4785) tensor(-0.0685)\n",
      "------------------------\n",
      "20\n",
      "tensor(0.4806) tensor(0.0563)\n",
      "------------------------\n",
      "31\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "32\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "3\n",
      "tensor(-0.0632) tensor(-0.0732)\n",
      "------------------------\n",
      "4\n",
      "tensor(0.0160)\n",
      "------------------------\n",
      "5\n",
      "tensor(0.0002)\n",
      "------------------------\n",
      "6\n",
      "tensor(0.0800) tensor(0.0096)\n",
      "------------------------\n",
      "7\n",
      "tensor(-0.0004)\n",
      "------------------------\n",
      "8\n",
      "tensor(0.1055) tensor(0.0203)\n",
      "------------------------\n",
      "9\n",
      "tensor(0.0561)\n",
      "------------------------\n",
      "10\n",
      "tensor(0.0382)\n",
      "------------------------\n",
      "13\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "14\n",
      "tensor(0.)\n",
      "------------------------\n",
      "15\n",
      "tensor(0.)\n",
      "------------------------\n",
      "19\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "20\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "22\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "26\n",
      "tensor(-0.1679) tensor(-0.0306)\n",
      "------------------------\n",
      "3\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "4\n",
      "tensor(0.)\n",
      "------------------------\n",
      "5\n",
      "tensor(0.)\n",
      "------------------------\n",
      "6\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "7\n",
      "tensor(0.)\n",
      "------------------------\n",
      "8\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "9\n",
      "tensor(0.)\n",
      "------------------------\n",
      "10\n",
      "tensor(0.)\n",
      "------------------------\n",
      "13\n",
      "tensor(0.1319) tensor(0.1103)\n",
      "------------------------\n",
      "14\n",
      "------------------------\n",
      "15\n",
      "tensor(0.0416)\n",
      "------------------------\n",
      "19\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "20\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "22\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n",
      "26\n",
      "tensor(0.) tensor(0.)\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "competence = competence_score(resultwordOrigin, resultwordMasked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uid': '0', 'influence': tensor(0.0402), 'relevance': tensor(0.0781)},\n",
       " {'uid': '0', 'influence': tensor(0.), 'relevance': tensor(0.0068)},\n",
       " {'uid': '1', 'influence': tensor(0.0091), 'relevance': tensor(-3.7138e-06)},\n",
       " {'uid': '2', 'influence': tensor(0.0396), 'relevance': tensor(0.0051)},\n",
       " {'uid': '2', 'influence': tensor(0.3514), 'relevance': tensor(0.1243)},\n",
       " {'uid': '3', 'influence': tensor(0.0858), 'relevance': tensor(0.0547)},\n",
       " {'uid': '3', 'influence': tensor(0.2511), 'relevance': tensor(0.0170)},\n",
       " {'uid': '4', 'influence': tensor(0.), 'relevance': tensor(0.0045)},\n",
       " {'uid': '4', 'influence': tensor(0.1140), 'relevance': tensor(0.0572)},\n",
       " {'uid': '5', 'influence': tensor(0.0166), 'relevance': tensor(0.0022)},\n",
       " {'uid': '5', 'influence': tensor(0.0747), 'relevance': tensor(0.0074)}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation coefficient: 0.7744894808829488\n",
      "p-value: 0.005131181460128413\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr, permutation_test\n",
    "\n",
    "influence_values = [item['influence'] for item in competence]\n",
    "relevance_values = [item['relevance'] for item in competence]\n",
    "\n",
    "# Calculate Spearman correlation coefficient\n",
    "correlation_coefficient, p_value = spearmanr(influence_values, relevance_values)\n",
    "\n",
    "\n",
    "print(\"Spearman correlation coefficient:\", correlation_coefficient)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0088, 0.005131181460128413)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def statistic(x):  # permute only `x`\n",
    "     return spearmanr(x, relevance_values).statistic\n",
    "\n",
    "res_exact = permutation_test((influence_values,), statistic, permutation_type=\"pairings\")\n",
    "res_asymptotic = spearmanr(influence_values, relevance_values)\n",
    "res_exact.pvalue, res_asymptotic.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the influence and relevance score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "influence = [item['influence'] for item in competence]\n",
    "relevance = [item['relevance'] for item in competence]\n",
    "\n",
    "# line chart\n",
    "plt.plot(influence, label='influence')\n",
    "plt.plot(relevance, label='relevance')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# read file and calculate f1\n",
    "path_predict = 'dataMeddle/ner_coNLL_testa_abolish_masked_origin.tsv'\n",
    "# this file have 3 column\n",
    "# uid   label    predict \n",
    "# have header\n",
    "data = pd.read_csv(path_predict, sep='\\t', header=None)\n",
    "# promote header this first row\n",
    "data.columns = data.iloc[0]\n",
    "# drop first row\n",
    "data = data.drop(0)\n",
    "f1 = f1_score(data['label'], data['prediction'], average='macro')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python ../data_preparation.py \\\n",
    "#     --task_file 'tasks_file_SRL.yml' \\\n",
    "#     --data_dir 'content/data/' \\\n",
    "#     --max_seq_len 50\n",
    "#     --has_labels True\n",
    "#     # --data_dir '../../data' \\\n",
    "\n",
    "\n",
    "# !python ../train.py \\\n",
    "#     --data_dir 'content/data/bert-base-uncased_prepared_data' \\\n",
    "#     --task_file 'tasks_file_SRL.yml' \\\n",
    "#     --out_dir 'conll_ner_pos_bert_base' \\\n",
    "#     --epochs 50 \\\n",
    "#     --learning_rate 0.00002\n",
    "#     --max_seq_len 50 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def relevance_score(prob_gold_, prob_origin_, prob_masked_, label_gold, label_origin, label_masked):\n",
    "#     relevance = []\n",
    "#     weight = []\n",
    "#     for i in range(len(prob_gold_)):\n",
    "#         if i not in jud_space:\n",
    "#             continue\n",
    "#         print(i)\n",
    "#         if label_gold[i] == label_origin[i] and label_gold[i] == label_masked[i]:\n",
    "#             relevance.append(prob_gold_[i])\n",
    "#             # check prob_gold is tensor ?\n",
    "#             print(prob_gold_[i])\n",
    "#             weight.append(1)\n",
    "#         elif label_gold[i] != label_origin[i] and label_origin[i] == label_masked[i]:\n",
    "#             relevance.append((prob_gold_[i] + prob_masked_[i])/2)\n",
    "#             print(prob_gold_[i], prob_masked_[i])\n",
    "#             weight.append(2)\n",
    "#         elif label_origin[i] != label_masked[i] and label_masked[i] == label_gold[i]:\n",
    "#             relevance.append((prob_gold_[i] + prob_origin_[i])/2)\n",
    "#             print(prob_gold_[i], prob_origin_[i])\n",
    "#             weight.append(2)\n",
    "#         elif label_gold[i] != label_origin[i] and label_masked[i] != label_gold[i]:\n",
    "#             relevance.append((prob_gold_[i] + prob_origin_[i] + prob_masked_[i])/3)\n",
    "#             print(prob_gold_[i], prob_origin_[i], prob_masked_[i])\n",
    "#             weight.append(1)\n",
    "#         else:\n",
    "#             relevance.append(0)\n",
    "#         print('------------------------')\n",
    "#     return relevance, weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
