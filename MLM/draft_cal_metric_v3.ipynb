{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-05-15 23:56:31.621248: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-15 23:56:32.117782: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-15 23:56:33.662318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-15 23:56:35.436532: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from tqdm import tqdm\n",
    "from mlm_utils.metric_func import cosine_sim, cosine_module\n",
    "from mlm_utils.transform_func import get_files\n",
    "import multiprocessing as mp\n",
    "import dask\n",
    "from dask.delayed import delayed\n",
    "sys.path.append('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_dask(readPath):\n",
    "    df = dd.read_json(readPath, lines=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_df(df_dir):\n",
    "    df_split_1 = dd.read_parquet('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/data_mlm/split_data/split_1.parquet')\n",
    "    df_split_2 = dd.read_parquet('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/data_mlm/split_data/split_2.parquet')\n",
    "    df_split_3 = dd.read_parquet('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/data_mlm/split_data/split_3.parquet')\n",
    "    return df_split_1, df_split_2, df_split_3\n",
    "\n",
    "\n",
    "def preprocess_df_predicate(df_predicate):\n",
    "    def first_nonzero(lst):\n",
    "        nonzero_elements = filter(lambda x: x != 0, lst)\n",
    "        return next(nonzero_elements, 0)\n",
    "    \n",
    "    df_predicate['tag_id'] = df_predicate['pos_tag_id'].apply(first_nonzero, meta=('pos_tag_id', 'int64')) \n",
    "   \n",
    "    return df_predicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    # check if a and b are not list, convert it\n",
    "    if not isinstance(a, list):\n",
    "        a = ast.literal_eval(a)\n",
    "    if not isinstance(b, list):\n",
    "        b = ast.literal_eval(b)\n",
    "            \n",
    "    return round(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)), 4)\n",
    "\n",
    "\n",
    "def cosine_module(a, b, cosine_sum):\n",
    "    norm_array1 = np.linalg.norm(a)\n",
    "    norm_array2 = np.linalg.norm(b)\n",
    "    \n",
    "    module_similarity = 1 - (np.abs(norm_array1 - norm_array2) / (norm_array1 + norm_array2))\n",
    "    \n",
    "    return module_similarity * cosine_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cosine_similarities(df_predicate, vector_type, df_content, metric = 'cosine'):\n",
    "    # Convert pandas DataFrames to Dask DataFrames\n",
    "    num_processes = mp.cpu_count() - 1\n",
    "    similarities = pd.DataFrame(index=df_predicate.index, columns=df_content.index)\n",
    "   \n",
    "    # Define the delayed computation\n",
    "    def compute_similarity(i, j):\n",
    "        vec1 = df_predicate.at[i, '{}_vector'.format(vector_type)]\n",
    "        vec2 = df_content.at[j, '{}_vector'.format(vector_type)]\n",
    "        \n",
    "        if metric == 'cosine':\n",
    "            return cosine_sim(vec1, vec2)\n",
    "        elif metric == 'cosine_module':\n",
    "            return cosine_module(vec1, vec2, cosine_sim(vec1, vec2))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid metric\")\n",
    "    \n",
    "    # Create delayed tasks for all combinations of i and j\n",
    "    tasks = []\n",
    "    for i in df_predicate.index:\n",
    "        for j in df_content.index:\n",
    "            tasks.append(delayed(compute_similarity)(i, j))\n",
    "    \n",
    "    # Compute the tasks in parallel\n",
    "    results = dask.compute(*tasks, num_workers=num_processes)\n",
    "    \n",
    "    # Fill the similarities DataFrame with the computed results\n",
    "    idx = 0\n",
    "    for i in df_predicate.index:\n",
    "        for j in df_content.index:\n",
    "            similarities.at[i, j] = results[idx]\n",
    "            idx += 1\n",
    "    \n",
    "    # Convert to numeric type\n",
    "    similarities = similarities.apply(pd.to_numeric)\n",
    "    print(\"similarity shape: \", similarities.shape)\n",
    "    \n",
    "    # cosine -1\n",
    "    print(\"create cosine -1...\")\n",
    "    min_indices = similarities.idxmin(axis=1)\n",
    "    df_predicate.loc[:, \"neg_{}_{}\".format(metric, vector_type)] = df_content.loc[min_indices]['word'].values\n",
    "    df_predicate.loc[:, \"neg_value_{}_{}\".format(metric, vector_type)] = similarities.min(axis=1).values\n",
    "    del min_indices\n",
    "    \n",
    "    # \n",
    "    # cosine 0\n",
    "    print(\"create cosine 0...\")\n",
    "    pos_cos_sum_indices = np.abs(similarities).idxmin(axis=1)\n",
    "    df_predicate.loc[:, \"pos_{}_{}\".format(metric, vector_type)] = df_content.loc[pos_cos_sum_indices]['word'].values\n",
    "    df_predicate.loc[:, \"pos_value_{}_{}\".format(metric, vector_type)] = np.abs(similarities).min(axis=1).values  # absolute value\n",
    "    del pos_cos_sum_indices\n",
    "    \n",
    "    # separate neg_value_cosine_sum and pos_value_cosine_value into dataframe with 2 column\n",
    "    val_df = df_predicate[['neg_value_{}_{}'.format(metric, vector_type), 'neg_{}_{}'.format(metric, vector_type), 'pos_value_{}_{}'.format(metric, vector_type), 'pos_{}_{}'.format(metric, vector_type)]]\n",
    "    \n",
    "    \n",
    "    # drop 2 columns from df_predicate\n",
    "    df_predicate.drop(['neg_value_{}_{}'.format(metric, vector_type), 'pos_value_{}_{}'.format(metric, vector_type)], axis=1, inplace=True)\n",
    "    return df_predicate, val_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_noun_word(df_predicate, vector_type, dfs, metric = 'cosine'):\n",
    "    \n",
    "    pd.options.mode.copy_on_write = True\n",
    "    \n",
    "    val_dfs = []\n",
    "    for df in dfs:\n",
    "        val_df = compute_cosine_similarities(df_predicate, vector_type, df, metric)[1]\n",
    "        val_dfs.append(val_df)\n",
    "    \n",
    "    # merge 3 val_df into one with axis 1 and get the min value of each row\n",
    "    concat_df = pd.concat(val_dfs, axis=1)\n",
    "    del val_dfs\n",
    "    \n",
    "    concat_df_neg  = concat_df.filter(like='neg_value_{type}')\n",
    "    concat_df_neg.columns = ['neg_value_{type}_1', 'neg_value_{type}_2', 'neg_value_{type}_3']\n",
    "    \n",
    "    \n",
    "    word_cols = concat_df.filter(like='neg_{type}')\n",
    "    word_cols.columns = ['neg_{type}_1', 'neg_{type}_2', 'neg_{type}_3']\n",
    "    \n",
    "    \n",
    "    min_val_indices = concat_df_neg.columns.get_indexer(concat_df_neg.idxmin(axis=1))\n",
    "    df_predicate.loc[:, \"neg_{}\".format(metric)] =  word_cols.apply(lambda row: row.iloc[min_val_indices[row.name]], axis=1)\n",
    "    \n",
    "    return df_predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file... mlm_abolish_full.json\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "file_paths = {\n",
    "        \"noun\": \"./data_mlm/process_folder/list_content_word_v3/NOUN.json\",\n",
    "        \"verb\": \"./data_mlm/process_folder/list_content_word_v3/VERB.json\",\n",
    "        \"adj\": \"./data_mlm/process_folder/list_content_word_v3/ADJ.json\",\n",
    "        \"adv\": \"./data_mlm/process_folder/list_content_word_v3/ADV.json\",\n",
    "        \"predicate_dir\": \"./data_mlm/process_folder/word_present_each_file_v3/\",\n",
    "        \"wri_dir\": \"./data_mlm/pertured_data/masked_data_parquet/\",\n",
    "        \"df_dir\": \"/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/data_mlm/split_data/\"\n",
    "    }\n",
    "\n",
    "df_verb = read_data_dask(file_paths[\"verb\"])\n",
    "df_adj = read_data_dask(file_paths[\"adj\"])\n",
    "df_adv = read_data_dask(file_paths[\"adv\"])   \n",
    "df_noun = read_data_dask(file_paths[\"noun\"])\n",
    "files = get_files(file_paths[\"predicate_dir\"]) \n",
    "# dfs = load_df(file_paths[\"df_dir\"])\n",
    "for file in tqdm(files):\n",
    "    print(\"Processing file...\", file)\n",
    "    df_predicate = read_data_dask(file_paths[\"predicate_dir\"] + file)\n",
    "    df_predicate = preprocess_df_predicate(df_predicate)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(4):\n",
    "        if i == 1:\n",
    "            separated_df = df_predicate[df_predicate['tag_id'] == i]\n",
    "            pertured_df = compute_cosine_similarities(separated_df, 'sum', df_noun, metric = 'cosine')\n",
    "            del df_noun\n",
    "            results.append(pertured_df)\n",
    "        elif i == 2:\n",
    "            separated_df = df_predicate[df_predicate['tag_id'] == i]\n",
    "            pertured_df = compute_cosine_similarities(separated_df, 'sum', df_verb, metric = 'cosine')\n",
    "            del df_verb\n",
    "            results.append(pertured_df)\n",
    "        elif i == 3:\n",
    "            separated_df = df_predicate[df_predicate['tag_id'] == i]\n",
    "            pertured_df = compute_cosine_similarities(separated_df, 'sum', df_adj, metric = 'cosine')\n",
    "            del df_adj\n",
    "            results.append(pertured_df)\n",
    "        elif i == 4:\n",
    "            separated_df = df_predicate[df_predicate['tag_id'] == i]\n",
    "            pertured_df = compute_cosine_similarities(separated_df, 'sum', df_adv, metric = 'cosine')\n",
    "            del df_adv\n",
    "            results.append(pertured_df)\n",
    "    res_df = pd.concat(results, axis=0).sort_index()        \n",
    "    res_df.to_parquet(file_paths['wri_dir'] + file.replace(\"mlm_\", \"\").split(\".\")[0] + \".parquet\")\n",
    "    del res_df\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
