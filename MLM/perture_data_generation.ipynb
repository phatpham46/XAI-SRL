{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-05-20 18:44:26.989614: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-20 18:44:28.907654: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-20 18:44:31.972636: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-20 18:44:40.367773: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "sys.path.append('../')\n",
    "from MLM.mlm_utils.model_utils import TOKENIZER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(readPath):\n",
    "    '''read csv file and return list of json objects.'''\n",
    "    with open(readPath, 'r', encoding = 'utf-8') as file:\n",
    "        taskData = list(map(lambda x: json.loads(x), file))\n",
    "       \n",
    "    return taskData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cosine_module_2_tensors(tensor1, tensor2, cosine_sim):\n",
    "    \n",
    "#     norm_tensor1 = torch.norm(tensor1)\n",
    "#     norm_tensor2 = torch.norm(tensor2)\n",
    "    \n",
    "#     module_similarity = 1 - (torch.abs(norm_tensor1 - norm_tensor2) / (norm_tensor1 + norm_tensor2))\n",
    "    \n",
    "#     return module_similarity * cosine_sim\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "def cosine_sim(a, b):\n",
    "    \n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "def cosine_module_2_numpy(arr1, arr2, cosine_sim):\n",
    "        \n",
    "    norm_arr1 = np.linalg.norm(arr1)\n",
    "    norm_arr2 = np.linalg.norm(arr2)\n",
    "    \n",
    "    module_similarity = 1 - (np.abs(norm_arr1 - norm_arr2) / (norm_arr1 + norm_arr2))\n",
    "    \n",
    "    return module_similarity * cosine_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sen_content_word(vector_tensor, type, content_word_dict):\n",
    "    '''\n",
    "    Calculate cosine similarity between the sum and avg of the word and the content word vector.\n",
    "    Input:\n",
    "        sum_tensor: torch.Tensor, sum of the word vector.\n",
    "        avg_tensor: torch.Tensor, average of the word vector.\n",
    "        content_word_dict: dict (word_vector, sum_vector, avg_vector), dictionary containing the content word vector.\n",
    "    Output:\n",
    "        cosine: float, cosine similarity between the word vector and the content word vector.\n",
    "        cosine_module: float, cosine module similarity between the word vector and the content word vector.\n",
    "        content_word_dict['word_vector']: torch.Tensor, masked word vector.\n",
    "    '''\n",
    "    \n",
    "    # content_tensor = torch.tensor(content_word_dict['sum_vector']).clone().detach()\n",
    "    if type == 'sum':\n",
    "        content_tensor = np.array(content_word_dict['sum_vector'])\n",
    "    elif type == 'avg':\n",
    "        content_tensor = np.array(content_word_dict['avg_vector'])\n",
    "    # cosine_func = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    # cosine = cosine_func(vector_tensor/torch.norm(vector_tensor), content_tensor/torch.norm(content_tensor)).item()\n",
    "    cosine = round(cosine_sim(vector_tensor, content_tensor), 7)\n",
    "    cosine_module = round(cosine_module_2_numpy(vector_tensor, content_tensor, cosine), 7)\n",
    "    \n",
    "    del content_tensor\n",
    "    return cosine, cosine_module, content_word_dict['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_sen_list_word(vector_tensor, type, list_dict_content_word):\n",
    "    '''\n",
    "    Calculate cosine similarity between the sum and avg of the word and list the content word vector, then return the word with cosine approximate -1 and 0.\n",
    "    Input:\n",
    "        sum_tensor: torch.Tensor, sum of the word vector.\n",
    "        avg_tensor: torch.Tensor, average of the word vector.\n",
    "        list_dict_content_word: list of dict (word_vector, sum_vector, avg_vector), list of dictionary containing the content word vector.\n",
    "    Output:\n",
    "        replace_word_neg_cos: torch.Tensor, replaced word vector with cosine approximate -1.\n",
    "        replace_word_neg_cos_module: torch.Tensor, replaced word vector with cosine module approximate -1.\n",
    "        replace_word_pos_cos: torch.Tensor, replaced word vector with cosine approximate 0.\n",
    "        replace_word_pos_cos_module: torch.Tensor, replaced word vector with cosine module approximate 0.\n",
    "    '''\n",
    "   \n",
    "    list_result = list(map(lambda x: cosine_sen_content_word(vector_tensor, type, x), list_dict_content_word))\n",
    "    sorted_cos = sorted(list_result, key = lambda x: x[0]) # tang dan\n",
    "    sorted_cos_module = sorted(list_result, key = lambda x: x[1]) # tang dan\n",
    "    \n",
    "    # cosine approximate -1\n",
    "    neg_cos, _, replace_word_neg_cos = sorted_cos[0]\n",
    "    _, neg_cos_module, replace_word_neg_cos_module = sorted_cos_module[0]\n",
    "    \n",
    "    # cosine approximate 0\n",
    "    sorted_cos.sort(key=lambda x: abs(x[0]))\n",
    "    pos_cos, _, replace_word_pos_cos = sorted_cos[0]\n",
    "   \n",
    "    sorted_cos_module.sort(key=lambda x: abs(x[1]))\n",
    "    _, pos_cos_module, replace_word_pos_cos_module = sorted_cos_module[0]\n",
    "    \n",
    "    del list_result, sorted_cos, sorted_cos_module\n",
    "    # return pair of word and cosine similarity\n",
    "    return {'neg_cos': (neg_cos, replace_word_neg_cos), \n",
    "            'neg_cos_module': (neg_cos_module, replace_word_neg_cos_module), \n",
    "            'pos_cos': (pos_cos, replace_word_pos_cos), \n",
    "            'pos_cos_module': (pos_cos_module, replace_word_pos_cos_module)}\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find new word with lowest cosine sim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_new_word(predicate_data_chunk, chunkNumber, tempList, content_word_data):\n",
    "    ''' \n",
    "    Generate pertured sentence for each predicate data in the chunk. \n",
    "    '''\n",
    "    name = 'new_data_{}.json'.format(str(chunkNumber))\n",
    "    cosine_val_file = 'cosine_res_{}.csv'.format(str(chunkNumber))\n",
    "    def replace_new_word(origin_id, masked_word, list_new_word):\n",
    "        list_new_word = [item for sublist in list_new_word for item in sublist]\n",
    "        origin_sen = TOKENIZER.decode(torch.tensor(origin_id), skip_special_tokens = True)\n",
    "        list_new_sen = list(map(lambda x: origin_sen.replace(TOKENIZER.decode(masked_word), TOKENIZER.decode(x), 1), list_new_word))\n",
    "        \n",
    "        return list_new_sen\n",
    "        \n",
    "    def generate_new_word_in_sen(predicate_data):\n",
    "        \n",
    "        # find masked index in sentence\n",
    "        \n",
    "        masked_index = torch.where(torch.tensor(predicate_data['pos_tag_id']).clone().detach() != 0)\n",
    "        \n",
    "        # convert input id to numpy array, easy to replace new word at masked index\n",
    "        predicate_np = np.array(predicate_data['origin_id'])\n",
    "        \n",
    "        # convert sum and avg vector to tensor\n",
    "      \n",
    "        sum_tensor = np.array(predicate_data['sum_vector'])\n",
    "        avg_tensor = np.array(predicate_data['avg_vector'])\n",
    "        \n",
    "        # find cosine similarity between the sum and avg of the word and its corresponding type of content word vector\n",
    "        if predicate_data['pos_tag_id'][masked_index[0][0].item()] == 1:\n",
    "            content_word = content_word_data['noun']\n",
    "            \n",
    "        elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 2:\n",
    "            content_word =  content_word_data['verb']\n",
    "            \n",
    "        elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 3:\n",
    "            content_word =  content_word_data['adj']\n",
    "            \n",
    "        elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 4:\n",
    "            content_word =  content_word_data['adv']\n",
    "        else: \n",
    "            KeyError('pos_tag_id not in [1, 2, 3, 4]')\n",
    "        \n",
    "        lisst = [cosine_sim_sen_list_word(vector_np, type, content_word) for type, vector_np in zip(['sum', 'avg'], [sum_tensor, avg_tensor])]\n",
    "             \n",
    "        del content_word, sum_tensor, avg_tensor\n",
    "         \n",
    "        list_new_sens = replace_new_word(predicate_data['origin_id'], predicate_np[masked_index[0]], list(map(lambda x: [i[1] for i in x.values()], lisst)))\n",
    "        feature =  {\n",
    "                \"origin_uid\": predicate_data['origin_uid'], \n",
    "                \"sum_neg_cos\": list_new_sens[0], \n",
    "                \"sum_neg_cos_module\": list_new_sens[1],\n",
    "                \"sum_pos_cos\": list_new_sens[2], \n",
    "                \"sum_pos_cos_module\": list_new_sens[3],\n",
    "                \"avg_neg_cos\":list_new_sens[4],\n",
    "                \"avg_neg_cos_module\":list_new_sens[5],\n",
    "                \"avg_pos_cos\":list_new_sens[6],\n",
    "                \"avg_pos_cos_module\": list_new_sens[7],\n",
    "                \"pos_tag_id\": predicate_data['pos_tag_id']}\n",
    "        \n",
    "        return feature, lisst        \n",
    "    \n",
    "    # apply function to each sentence in the chunk          \n",
    "    list_feature = map(lambda x: generate_new_word_in_sen(x), tqdm(predicate_data_chunk))\n",
    "    \n",
    "    # write to json file           \n",
    "    with open(name, 'w') as wf, open(cosine_val_file, 'w', newline='') as csv_file:\n",
    "        for feature in list_feature:\n",
    "            wf.write('{}\\n'.format(json.dumps(feature[0]))) \n",
    "            \n",
    "            # Write data rows\n",
    "            for item in feature[1]:\n",
    "                csv_file.write(\"%s\\n\" % item)\n",
    "        tempList.append(name)\n",
    "        tempList.append(cosine_val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_process_gen_data(predicate_file, dict_content_word, wriDir):\n",
    "    '''\n",
    "    Create pertured data using multiple processes and save pertured data to a file.\n",
    "    \n",
    "    '''\n",
    "    print(\"Preprocessing file... \", predicate_file)\n",
    "    predicates_data = read_data(predicate_file)\n",
    "    \n",
    "    # MULTI PROCESSING\n",
    "    man = mp.Manager()\n",
    "\n",
    "    # shared list to store all temp files written by processes\n",
    "    tempFilesList = man.list()\n",
    "    numProcess = mp.cpu_count() - 1\n",
    "    # numProcess = 1\n",
    "    chunkSize = int(len(predicates_data) / (numProcess))\n",
    "    \n",
    "    print('Data Size: ', len(predicates_data))\n",
    "    print('number of threads: ', numProcess)\n",
    "\n",
    "    processes = []\n",
    "    for i in range(numProcess):\n",
    "        dataChunk = predicates_data[chunkSize*i : chunkSize*(i+1)]\n",
    "\n",
    "        p = mp.Process(target = find_new_word, args = (dataChunk, i, tempFilesList, dict_content_word))\n",
    "        \n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        \n",
    "    for pr in processes:\n",
    "        pr.join()\n",
    "    \n",
    "    wrtPath = wriDir + 'masked_data_text/' + '{}'.format(predicate_file.split('/')[-1].replace('mlm_', ''))\n",
    "    wrtCosine = wriDir + 'cosine_val/' + 'cosine_val_{}'.format(predicate_file.split('/')[-1].replace('mlm_', '').replace('.json', '.csv'))\n",
    "    # wrtPath = wriDir  + '{}'.format(predicate_file.split('/')[-1].replace('mlm_', ''))\n",
    "    # wrtCosine = wriDir  + 'cosine_val_{}'.format(predicate_file.split('/')[-1].replace('mlm_', '').replace('.json', '.csv'))\n",
    "    # combining the files written by multiple processes into a single final file\n",
    "    with open(wrtPath, 'w') as f:\n",
    "        for file in tempFilesList:\n",
    "            if file.endswith('.json'):\n",
    "                with open(file, 'r') as r:\n",
    "                    for line in r:\n",
    "                        sample =  json.loads(line)\n",
    "                        f.write('{}\\n'.format(json.dumps(sample)))\n",
    "                os.remove(file)\n",
    "            else:\n",
    "                # read csv and write to file\n",
    "                with open(file, 'r') as read_file, open(wrtCosine, 'w', newline='') as write_file:\n",
    "                    read_dataaa = read_file.readlines()\n",
    "                    for line in read_dataaa:\n",
    "                        write_file.write(\"%s\\n\" % line)\n",
    "                   \n",
    "                os.remove(file)\n",
    "        \n",
    "    print(\"Done file\", predicate_file)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 4 lists of content word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_and_convert_to_dict(file_path):\n",
    "    '''Convert DataFrame to list of dictionaries with keys: uid, word_vector, sum_vector, avg_vector.'''\n",
    "    \n",
    "    data=pd.read_csv(file_path)\n",
    "    \n",
    "    eval_locals = {'tensor': torch.tensor, 'torch': torch}\n",
    "    \n",
    "    list_data_dict = [{\"uid\": row[0],\n",
    "                        \"word\": eval(row[1], eval_locals),\n",
    "                        \"sum_vector\": eval(row[2], eval_locals),\n",
    "                        \"avg_vector\": eval(row[3], eval_locals),\n",
    "                        }  for row in data.itertuples(index=False)]\n",
    "    \n",
    "    return list_data_dict\n",
    "file_paths = {\n",
    "    \"noun\": \"../content_word_csv/NOUN.csv\",\n",
    "    \"verb\": \"../content_word_csv/VERB.csv\",\n",
    "    \"adj\": \"../content_word_csv/ADJ.csv\",\n",
    "    \"adv\": \"../content_word_csv/ADV.csv\"\n",
    "}\n",
    "\n",
    "dict_content_word = {key: read_and_convert_to_dict(file_path) for key, file_path in file_paths.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlm_utils.transform_func import get_files\n",
    "\n",
    "\n",
    "wriDir = './data_mlm/perturbed_data/'\n",
    "dataDir = './data_mlm/process_folder/word_present_each_file/'\n",
    "# files = get_files(dataDir)\n",
    "files = ['mlm_lead_full.json', 'mlm_inhibit_full.json', 'mlm_express_full.json']\n",
    " \n",
    "for file in files:\n",
    "    multi_process_gen_data(dataDir + file, dict_content_word, wriDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert pertured text data to json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read masked_data_text\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from mlm_utils.transform_func import encode_text\n",
    "\n",
    "def create_data_perturbation(dataDir, labelDir, type_file):\n",
    "    def encode_sentence(sentence, type_file):\n",
    "        encode_data = encode_text(sentence[type_file])\n",
    "        feature = {'uid': sentence['origin_uid'], \n",
    "                    'token_id': encode_data['input_ids'][0].tolist(),\n",
    "                    'type_id': encode_data['token_type_ids'][0].tolist(),\n",
    "                    'mask': encode_data['attention_mask'][0].tolist(),\n",
    "                    'pos_tag_id': sentence['pos_tag_id'],\n",
    "                    'label': sentence['label']\n",
    "                    }\n",
    "        return feature\n",
    "    \n",
    "    \n",
    "    dataMaskedDir = dataDir / Path('masked_data_text')\n",
    "    \n",
    "    wrtDir = dataDir / Path(type_file + '_old_label')\n",
    "    # create directory if not exist\n",
    "    if not os.path.exists(wrtDir):\n",
    "        os.makedirs(wrtDir)\n",
    "        \n",
    "        \n",
    "    for data_file, label_file in zip(os.listdir(dataMaskedDir), os.listdir(labelDir)):\n",
    "       \n",
    "        print(data_file, label_file)\n",
    "        with open(os.path.join(dataMaskedDir, data_file), 'r') as f, open(os.path.join(labelDir, label_file), 'r') as f1:\n",
    "            data = [json.loads(line) for line in f]  \n",
    "            data_label = [json.loads(line) for line in f1]\n",
    "            \n",
    "            # Create a dictionary from data_label for quick lookup\n",
    "            label_dict = {int(item['uid']): item['label'] for item in data_label}\n",
    "\n",
    "            # assign label from data_label if uid matches\n",
    "            for item in data:\n",
    "               \n",
    "                uid = item['origin_uid']\n",
    "                if uid in label_dict:\n",
    "                    item['label'] = label_dict[uid]\n",
    "                \n",
    "               \n",
    "            data = list(map(lambda x: encode_sentence(x, type_file), data))\n",
    "           \n",
    "             \n",
    "            # save data to json file\n",
    "            with open(os.path.join(wrtDir, '{}'.format(data_file).replace('_full', '')), 'w') as f:\n",
    "                for item in data:\n",
    "                    f.write('{}\\n'.format(json.dumps(item)))\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing type file:  avg_neg_cos\n",
      "abolish_full.json ner_conll_format_abolish_full.json\n",
      "alter_full.json ner_conll_format_alter_full.json\n",
      "begin_1_full.json ner_conll_format_begin_1_full.json\n",
      "begin_2_full.json ner_conll_format_begin_2_full.json\n",
      "block_full.json ner_conll_format_block_full.json\n",
      "catalyse_full.json ner_conll_format_catalyse_full.json\n",
      "confer_full.json ner_conll_format_confer_full.json\n",
      "decrease_1_full.json ner_conll_format_decrease_1_full.json\n",
      "decrease_2_full.json ner_conll_format_decrease_2_full.json\n",
      "delete_full.json ner_conll_format_delete_full.json\n",
      "develop_full.json ner_conll_format_develop_full.json\n",
      "disrupt_full.json ner_conll_format_disrupt_full.json\n",
      "eliminate_full.json ner_conll_format_eliminate_full.json\n",
      "encode_full.json ner_conll_format_encode_full.json\n",
      "express_full.json ner_conll_format_express_full.json\n",
      "generate_full.json ner_conll_format_generate_full.json\n",
      "inhibit_full.json ner_conll_format_inhibit_full.json\n",
      "initiate_full.json ner_conll_format_initiate_full.json\n",
      "lead_full.json ner_conll_format_lead_full.json\n",
      "lose_full.json ner_conll_format_lose_full.json\n",
      "modify_full.json ner_conll_format_modify_full.json\n",
      "mutate_full.json ner_conll_format_mutate_full.json\n",
      "proliferate_full.json ner_conll_format_proliferate_full.json\n",
      "recognize_full.json ner_conll_format_recognize_full.json\n",
      "result_full.json ner_conll_format_result_full.json\n",
      "skip_full.json ner_conll_format_skip_full.json\n",
      "splice_1_full.json ner_conll_format_splice_1_full.json\n",
      "splice_2_full.json ner_conll_format_splice_2_full.json\n",
      "transcribe_full.json ner_conll_format_transcribe_full.json\n",
      "transform_1_full.json ner_conll_format_transform_1_full.json\n",
      "transform_2_full.json ner_conll_format_transform_2_full.json\n",
      "translate_1_full.json ner_conll_format_translate_1_full.json\n",
      "translate_2_full.json ner_conll_format_translate_2_full.json\n",
      "translate_3_full.json ner_conll_format_translate_3_full.json\n",
      "truncate_full.json ner_conll_format_truncate_full.json\n"
     ]
    }
   ],
   "source": [
    "dataDir = Path('./data_mlm/perturbed_data')\n",
    "labelDir = Path('./data_mlm/process_folder/coNLL_tsv_json/label_cu')\n",
    "\n",
    "type_files = ['sum_neg_cos', 'avg_neg_cos', 'sum_pos_cos', 'avg_pos_cos', 'sum_neg_cos_module', 'avg_neg_cos_module', 'sum_pos_cos_module', 'avg_pos_cos_module']\n",
    "for type_file in type_files:\n",
    "    if type_file == 'avg_neg_cos':\n",
    "        print('Processing type file: ', type_file)\n",
    "        create_data_perturbation(dataDir, labelDir, type_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
