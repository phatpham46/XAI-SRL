{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "import torch\n",
    "import copy\n",
    "model = BertForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "MAX_SEQ_LEN = 85\n",
    "from mlm_utils.model_utils import TOKENIZER\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from mlm_utils.preprocess_functions import get_key, compare_tensors, get_pos_tag_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test mask content wword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[38;5;66;03m# count_masked_words.append(count_masked_key)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m masked_sentences, labels\n\u001b[0;32m---> 42\u001b[0m origin_input_id \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor([  \u001b[38;5;241m101\u001b[39m,   \u001b[38;5;241m189\u001b[39m, \u001b[38;5;241m27226\u001b[39m,  \u001b[38;5;241m1116\u001b[39m,  \u001b[38;5;241m1114\u001b[39m,  \u001b[38;5;241m2549\u001b[39m, \u001b[38;5;241m21521\u001b[39m,  \u001b[38;5;241m1198\u001b[39m,   \u001b[38;5;241m126\u001b[39m,  \u001b[38;5;241m1106\u001b[39m,\n\u001b[1;32m     43\u001b[0m          \u001b[38;5;241m4252\u001b[39m,  \u001b[38;5;241m1320\u001b[39m,   \u001b[38;5;241m122\u001b[39m,   \u001b[38;5;241m117\u001b[39m,  \u001b[38;5;241m1137\u001b[39m,  \u001b[38;5;241m6531\u001b[39m,  \u001b[38;5;241m1121\u001b[39m,   \u001b[38;5;241m172\u001b[39m,   \u001b[38;5;241m118\u001b[39m,  \u001b[38;5;241m1139\u001b[39m,\n\u001b[1;32m     44\u001b[0m          \u001b[38;5;241m1665\u001b[39m,   \u001b[38;5;241m117\u001b[39m,  \u001b[38;5;241m1138\u001b[39m,  \u001b[38;5;241m2999\u001b[39m,   \u001b[38;5;241m172\u001b[39m,   \u001b[38;5;241m118\u001b[39m,  \u001b[38;5;241m1139\u001b[39m,  \u001b[38;5;241m1665\u001b[39m,   \u001b[38;5;241m182\u001b[39m, \u001b[38;5;241m11782\u001b[39m,\n\u001b[1;32m     45\u001b[0m           \u001b[38;5;241m1116\u001b[39m,  \u001b[38;5;241m1104\u001b[39m,   \u001b[38;5;241m123\u001b[39m,   \u001b[38;5;241m119\u001b[39m,  \u001b[38;5;241m1512\u001b[39m,  \u001b[38;5;241m1105\u001b[39m,   \u001b[38;5;241m123\u001b[39m,   \u001b[38;5;241m119\u001b[39m,   \u001b[38;5;241m125\u001b[39m,   \u001b[38;5;241m180\u001b[39m,\n\u001b[1;32m     46\u001b[0m          \u001b[38;5;241m1830\u001b[39m,   \u001b[38;5;241m117\u001b[39m,  \u001b[38;5;241m1134\u001b[39m, \u001b[38;5;241m11271\u001b[39m,  \u001b[38;5;241m1120\u001b[39m,  \u001b[38;5;241m1147\u001b[39m,   \u001b[38;5;241m126\u001b[39m,  \u001b[38;5;241m3769\u001b[39m,   \u001b[38;5;241m117\u001b[39m,  \u001b[38;5;241m1229\u001b[39m,\n\u001b[1;32m     47\u001b[0m          \u001b[38;5;241m189\u001b[39m, \u001b[38;5;241m27226\u001b[39m,  \u001b[38;5;241m1116\u001b[39m,  \u001b[38;5;241m1114\u001b[39m,  \u001b[38;5;241m2549\u001b[39m, \u001b[38;5;241m21521\u001b[39m,  \u001b[38;5;241m1439\u001b[39m,  \u001b[38;5;241m4252\u001b[39m,  \u001b[38;5;241m1320\u001b[39m,   \u001b[38;5;241m122\u001b[39m,\n\u001b[1;32m     48\u001b[0m          \u001b[38;5;241m1137\u001b[39m, \u001b[38;5;241m27553\u001b[39m,  \u001b[38;5;241m1179\u001b[39m,   \u001b[38;5;241m122\u001b[39m, \u001b[38;5;241m13000\u001b[39m,   \u001b[38;5;241m172\u001b[39m,   \u001b[38;5;241m118\u001b[39m,  \u001b[38;5;241m1139\u001b[39m,  \u001b[38;5;241m1665\u001b[39m,   \u001b[38;5;241m182\u001b[39m,\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;241m11782\u001b[39m,   \u001b[38;5;241m1116\u001b[39m,   \u001b[38;5;241m113\u001b[39m,   \u001b[38;5;241m123\u001b[39m,   \u001b[38;5;241m119\u001b[39m,   \u001b[38;5;241m122\u001b[39m,   \u001b[38;5;241m118\u001b[39m,   \u001b[38;5;241m123\u001b[39m,   \u001b[38;5;241m119\u001b[39m,   \u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     50\u001b[0m           \u001b[38;5;241m180\u001b[39m,  \u001b[38;5;241m1830\u001b[39m,  \u001b[38;5;241m1107\u001b[39m,   \u001b[38;5;241m171\u001b[39m,   \u001b[38;5;241m102\u001b[39m])\n\u001b[1;32m     52\u001b[0m origin_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(origin_input_id, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(origin_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from prepared_for_mlm import decode_token, get_tokens_for_words, mask_content_words\n",
    "def mask_content_words(ids, word_dict):\n",
    "    '''\n",
    "    input:\n",
    "        ids: sample_id \n",
    "        word_dict: {'The': [tensor(170)], 'capital': [tensor(1109), tensor(3007)], ....}\n",
    "    output:\n",
    "        masked_sentences: [tensor([  101,  1103, 175, 10555,  103,   103,   103,   102])\n",
    "        label_ids: [tensor(6468)], [tensor(1568), tensor(13892)]\n",
    "    '''\n",
    "    labels = []\n",
    "    masked_sentences = []\n",
    "    except_tokens = [TOKENIZER.cls_token_id, TOKENIZER.sep_token_id, TOKENIZER.pad_token_id, TOKENIZER.unk_token_id]\n",
    "    count_masked_words = []\n",
    "    for (key, value) in word_dict.items():\n",
    "        masked_ids = ids.clone() # torch.Size([1, 85])\n",
    "        origin_sample = decode_token(masked_ids[0])\n",
    "        if get_pos_tag_word(key, origin_sample).values in ['NOUN', 'VERB', 'ADJ', 'ADV']:\n",
    "            print(\"KEY: \", key)\n",
    "            for i in range(len(masked_ids[0]) - len(value) + 1):\n",
    "                masked_id = masked_ids.clone()\n",
    "                label = torch.full_like(masked_id, fill_value=-100)\n",
    "                masked_indice = torch.full_like(masked_id, fill_value=0)\n",
    "            \n",
    "                if torch.equal(torch.as_tensor(masked_id[0][i:i+len(value)]).clone().detach(), torch.as_tensor(value).clone().detach()):\n",
    "                    masked_id[0][i:i+len(value)]= TOKENIZER.mask_token_id\n",
    "                    #print(\"MASKED ID: \", masked_id)\n",
    "                    masked_indice[masked_id == TOKENIZER.mask_token_id] = 1\n",
    "                    #print(\"MASKED INDICE: \", masked_indice)\n",
    "                    \n",
    "                    \n",
    "                    for idx, mask in enumerate(masked_indice[0]):\n",
    "                        if mask == 1:\n",
    "                            label[0][idx] = ids[0][idx]\n",
    "                            \n",
    "                    masked_sentences.append(masked_id)\n",
    "                    labels.append(label)\n",
    "                # count_masked_words.append(count_masked_key)\n",
    "            \n",
    "    return masked_sentences, labels\n",
    "\n",
    "origin_input_id = torch.tensor([  101,   189, 27226,  1116,  1114,  2549, 21521,  1198,   126,  1106,\n",
    "         4252,  1320,   122,   117,  1137,  6531,  1121,   172,   118,  1139,\n",
    "         1665,   117,  1138,  2999,   172,   118,  1139,  1665,   182, 11782,\n",
    "          1116,  1104,   123,   119,  1512,  1105,   123,   119,   125,   180,\n",
    "         1830,   117,  1134, 11271,  1120,  1147,   126,  3769,   117,  1229,\n",
    "         189, 27226,  1116,  1114,  2549, 21521,  1439,  4252,  1320,   122,\n",
    "         1137, 27553,  1179,   122, 13000,   172,   118,  1139,  1665,   182,\n",
    "        11782,   1116,   113,   123,   119,   122,   118,   123,   119,   128,\n",
    "          180,  1830,  1107,   171,   102])\n",
    "\n",
    "origin_text = tokenizer.decode(origin_input_id, skip_special_tokens=True) \n",
    "print(origin_text)\n",
    "doc = nlp(origin_text)\n",
    "words_str = [word.text for word in [token for token in doc]]\n",
    "\n",
    "token_test = tokenizer.encode_plus(\n",
    "                ' '.join(words_str),\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                padding='max_length', \n",
    "                truncation=True,  \n",
    "                add_special_tokens = True,\n",
    "                return_tensors=\"pt\",  \n",
    "                return_attention_mask = True,\n",
    "                return_offsets_mapping=True  \n",
    "            )\n",
    "\n",
    "word_dict = get_tokens_for_words(\n",
    "    words_str, \n",
    "    token_test['input_ids'], \n",
    "    token_test['offset_mapping'][0]\n",
    "    )\n",
    "\n",
    "masked_sen_2d = origin_input_id.view(-1, 85)\n",
    "\n",
    "masked_sentences, label_ids = mask_content_words(masked_sen_2d, word_dict)\n",
    "print(len(masked_sentences), len(label_ids  ))\n",
    "# for i in range(len(masked_sentences)):\n",
    "#     print(label_ids[i], masked_sentences[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test is POs match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag_word(word, text):\n",
    "    doc = nlp(text)\n",
    "    word_split = word.split()\n",
    "    print(\"================\")\n",
    "    for token in doc:\n",
    "        if token.text in word_split:\n",
    "            print(token.text, token.pos_)\n",
    "        #     return token.pos_\n",
    "    return None\n",
    "\n",
    "def get_key(dictionary, value, count_word):\n",
    "    unique_set = torch.chunk(value, count_word)[0]\n",
    "    for key, val in dictionary.items():\n",
    "        if compare_tensors(val, unique_set):\n",
    "            return key\n",
    "    return None \n",
    "def is_POS_match(input_ids, lm_label_ids, b_count_word):\n",
    "    '''\n",
    "    Function to check if the POS tag of the masked token in the logits is the same as the POS tag of the masked token in the original text.\n",
    "    Note: This function assumes that the logits are of shape # ([85, 28996]) \n",
    "    lm_label_ids: shape (batch_size, sequence_length)\n",
    "    '''\n",
    "    \n",
    "    origin_input_id = input_ids.clone() # Origin input id:  torch.Size([85])\n",
    "    result_ids = input_ids.clone() # torch.Size([85])\n",
    "    \n",
    "    # Find the index of the masked token from lm_label_ids\n",
    "    masked_idx = torch.where(lm_label_ids != -100)[0]\n",
    "    masked_idx_input = torch.where(input_ids == tokenizer.mask_token_id)[0]\n",
    "    origin_input_id[masked_idx_input] = lm_label_ids[masked_idx] \n",
    "    \n",
    "    \n",
    "    # make sure masked_idx_input and masked_idx are the same using assert\n",
    "    assert torch.equal(masked_idx, masked_idx_input), \"Masked index and label index are not the same.\"\n",
    "    print(\"lm_label_ids[masked_idx]  \", lm_label_ids )\n",
    "    # get pos tag of origin text\n",
    "    origin_text = tokenizer.decode(origin_input_id, skip_special_tokens=True) \n",
    "    \n",
    "    doc = nlp(origin_text)\n",
    "    words_str = [word.text for word in [token for token in doc]]\n",
    "    \n",
    "    token_test = tokenizer.encode_plus(\n",
    "                    ' '.join(words_str),\n",
    "                    max_length=MAX_SEQ_LEN,\n",
    "                    padding='max_length', \n",
    "                    truncation=True,  \n",
    "                    add_special_tokens = True,\n",
    "                    return_tensors=\"pt\",  \n",
    "                    return_attention_mask = True,\n",
    "                    return_offsets_mapping=True  \n",
    "                )\n",
    "    masked_sens_2d = torch.unsqueeze(input_ids, 0)\n",
    "    label_ids_2d = torch.unsqueeze(lm_label_ids, 0)\n",
    "    output = model(input_ids = masked_sens_2d, attention_mask = token_test['attention_mask'], token_type_ids=token_test['token_type_ids'], labels=label_ids_2d)\n",
    "    print(\"shape logits: \", output.logits.shape)  ##shape logits:  torch.Size([1, 85, 28996])\n",
    "    \n",
    "    word_dict = get_tokens_for_words(\n",
    "        words_str, \n",
    "        token_test['input_ids'], \n",
    "        token_test['offset_mapping'][0]\n",
    "        )\n",
    "      \n",
    "    # Get key for the masked token\n",
    "    \n",
    "    masked_word = get_key(word_dict, origin_input_id[masked_idx],b_count_word )\n",
    "    print(\"masked word: \", masked_word)\n",
    "    pos_tag_origin = get_pos_tag_word(masked_word, origin_text)\n",
    "    print(\"pos tag origin: \", masked_word, pos_tag_origin)         \n",
    "    \n",
    "    # # Extract the logits for the masked position\n",
    "    masked_logits = output.logits[0][masked_idx]\n",
    "    # print(\"MASKED LOGITS: \", masked_logits) # torch.Size([28996])\n",
    "    pred = [torch.argmax(output.logits[0][i]).item() for i in masked_idx]\n",
    "    # Print top 10 masked tokens\n",
    "        # print(tokenizer.convert_ids_to_tokens(torch.topk(outputs.logits[0, idx, :], 10).indices))\n",
    "        \n",
    "    # Replace the index of the masked token with the list of predicted tokens\n",
    "    for i in range(len(masked_idx)):\n",
    "        result_ids[masked_idx[i]] = pred[i]\n",
    "        \n",
    "    # print(\"result sentence: \", tokenizer.decode(result_ids, skip_special_tokens=True))\n",
    "    for i in pred:\n",
    "        print(\"PRED WORD: \",i,  tokenizer.decode(i))\n",
    "    print(\"PRED WORD: \",pred,  tokenizer.decode(pred))\n",
    "    logits_tag = get_pos_tag_word(tokenizer.decode(pred), tokenizer.decode(result_ids, skip_special_tokens=True) )\n",
    "    print(\"POS TAG PRED WORD: \", logits_tag)\n",
    "    \n",
    "    \n",
    "    # Cross-entropy term\n",
    "    \n",
    "    cross_entropy_term = F.cross_entropy(output.logits.view(-1, tokenizer.vocab_size), lm_label_ids.view(-1))\n",
    "    print(\"Cross entropy term shape: \", cross_entropy_term.shape)      ##Cross entropy term shape:  torch.Size([2720])\n",
    "  \n",
    "    # Custom matching term\n",
    "    matching_term_batch = (pos_tag_origin == logits_tag)\n",
    "\n",
    "    # Combine terms to get the loss for 1 batch\n",
    "    matching_term = torch.where(matching_term_batch == torch.tensor(True), torch.tensor(1.0), torch.tensor(0.0))\n",
    "    print(\"Matching term shape: \", matching_term)  ##Matching term shape:  torch.Size([2720])\n",
    "    print(\"cross entropy term: \", cross_entropy_term)\n",
    "    \n",
    "    loss = 0.5 * cross_entropy_term + (1 - matching_term)\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_label_ids[masked_idx]   tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, 5190, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, 5190, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100])\n",
      "shape logits:  torch.Size([1, 85, 28996])\n",
      "masked word:  acid\n",
      "================\n",
      "acid NOUN\n",
      "acid NOUN\n",
      "pos tag origin:  acid None\n",
      "PRED WORD:  26825 insulin\n",
      "PRED WORD:  3850 drug\n",
      "PRED WORD:  [26825, 3850] insulin drug\n",
      "================\n",
      "insulin NOUN\n",
      "drug NOUN\n",
      "POS TAG PRED WORD:  None\n",
      "Cross entropy term shape:  torch.Size([])\n",
      "Matching term shape:  tensor(1.)\n",
      "cross entropy term:  tensor(4.9894, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4947, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "label_id = torch.tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5190, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5190, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])\n",
    "\n",
    "input_id= torch.tensor([101, 1748, 4982, 117, 1103, 3687, 26883, 1320, 1104, 194, 3031, 1162, 2423, 170, 15792, 19033, 103, 4789, 1107, 19255, 118, 4065, 3652, 117, 8783, 1115, 194, 3031, 1162, 2399, 170, 3607, 1648, 1107, 19255, 118, 4065, 103, 4789, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "print(is_POS_match(input_id, label_id, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tumours is in the given tensor.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample word dictionary and tensor\n",
    "word_dict = {'tumours': [torch.tensor(189)], 'with': [torch.tensor(1114)]}\n",
    "given_tensor = torch.tensor([189, 27226, 1116, 189, 27226, 1116])\n",
    "\n",
    "# Function to check if any tensor in the dictionary values is in the given tensor\n",
    "def is_in_tensor(word_dict, tensor):\n",
    "    for word, tensors in word_dict.items():\n",
    "        for tensor_val in tensors:\n",
    "            if (tensor == tensor_val).any().item():\n",
    "                print(f\"{word} is in the given tensor.\")\n",
    "                break\n",
    "\n",
    "# Check if any word in the dictionary is in the given tensor\n",
    "is_in_tensor(word_dict, given_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7282,  0.6782, -0.3622,  0.9572,  0.9279,  1.0818, -0.4334,\n",
      "          -0.8105, -0.1358,  0.5705],\n",
      "         [ 1.3758, -2.1371, -0.0726, -1.8334, -1.3297, -0.1785,  1.5264,\n",
      "           1.5781,  0.3917, -2.0859],\n",
      "         [ 0.6384,  0.3851, -1.4952,  1.9068, -0.4026, -0.4253, -0.2667,\n",
      "          -0.0509,  0.0526,  0.5777],\n",
      "         [ 0.4248, -0.0755, -0.2062,  0.8857, -0.0867,  0.0920, -1.0359,\n",
      "           0.9809,  2.1640,  0.6484],\n",
      "         [ 0.3108, -0.1413,  0.3952, -0.2666,  1.1596, -0.7127, -0.0294,\n",
      "           0.6728,  0.4344, -0.8212]]])\n",
      "tensor([[[ 1.7282,  0.3218,  1.3622,  0.0428,  0.0721, -0.0818,  1.4334,\n",
      "           1.8105,  1.1358,  0.4295],\n",
      "         [-0.3758,  3.1371,  1.0726,  2.8334,  2.3297,  1.1785, -0.5264,\n",
      "          -0.5781,  0.6083,  3.0859],\n",
      "         [ 0.3616,  0.6149,  2.4952, -0.9068,  1.4026,  1.4253,  1.2667,\n",
      "           1.0509,  0.9474,  0.4223],\n",
      "         [ 0.5752,  1.0755,  1.2062,  0.1143,  1.0867,  0.9080,  2.0359,\n",
      "           0.0191, -1.1640,  0.3516],\n",
      "         [ 0.6892,  1.1413,  0.6048,  1.2666, -0.1596,  1.7127,  1.0294,\n",
      "           0.3272,  0.5656,  1.8212]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming logits is your tensor of shape (batch_size, max_seq_length, vocab_size)\n",
    "logits = torch.randn((1, 5, 10))  # Example tensor, replace it with your actual logits\n",
    "\n",
    "# Compute the inverse of the logits\n",
    "inverse_logits = 1 - logits\n",
    "\n",
    "# You can also use torch.sub() function\n",
    "# inverse_logits = torch.sub(1, logits)\n",
    "print(logits)\n",
    "print(inverse_logits)  # Should output the same shape as logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
