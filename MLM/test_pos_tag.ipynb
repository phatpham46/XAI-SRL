{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-04-02 10:57:41.606717: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-02 10:57:42.102198: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 10:57:44.526981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-02 10:57:47.821555: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "import torch\n",
    "import copy\n",
    "model = BertForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "MAX_SEQ_LEN = 85\n",
    "from mlm_utils.model_utils import TOKENIZER\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from mlm_utils.preprocess_functions import get_key, compare_tensors, get_pos_tag_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test mask content wword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizerFast' object has no attribute 'decode_plus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 52\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m masked_sentences, labels\n\u001b[1;32m     42\u001b[0m origin_input_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([  \u001b[38;5;241m101\u001b[39m,   \u001b[38;5;241m189\u001b[39m, \u001b[38;5;241m27226\u001b[39m,  \u001b[38;5;241m1116\u001b[39m,  \u001b[38;5;241m1114\u001b[39m,  \u001b[38;5;241m2549\u001b[39m, \u001b[38;5;241m21521\u001b[39m,  \u001b[38;5;241m1198\u001b[39m,   \u001b[38;5;241m126\u001b[39m,  \u001b[38;5;241m1106\u001b[39m,\n\u001b[1;32m     43\u001b[0m          \u001b[38;5;241m4252\u001b[39m,  \u001b[38;5;241m1320\u001b[39m,   \u001b[38;5;241m122\u001b[39m,   \u001b[38;5;241m117\u001b[39m,  \u001b[38;5;241m1137\u001b[39m,  \u001b[38;5;241m6531\u001b[39m,  \u001b[38;5;241m1121\u001b[39m,   \u001b[38;5;241m172\u001b[39m,   \u001b[38;5;241m118\u001b[39m,  \u001b[38;5;241m1139\u001b[39m,\n\u001b[1;32m     44\u001b[0m          \u001b[38;5;241m1665\u001b[39m,   \u001b[38;5;241m117\u001b[39m,  \u001b[38;5;241m1138\u001b[39m,  \u001b[38;5;241m2999\u001b[39m,   \u001b[38;5;241m172\u001b[39m,   \u001b[38;5;241m118\u001b[39m,  \u001b[38;5;241m1139\u001b[39m,  \u001b[38;5;241m1665\u001b[39m,   \u001b[38;5;241m182\u001b[39m, \u001b[38;5;241m11782\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;241m11782\u001b[39m,   \u001b[38;5;241m1116\u001b[39m,   \u001b[38;5;241m113\u001b[39m,   \u001b[38;5;241m123\u001b[39m,   \u001b[38;5;241m119\u001b[39m,   \u001b[38;5;241m122\u001b[39m,   \u001b[38;5;241m118\u001b[39m,   \u001b[38;5;241m123\u001b[39m,   \u001b[38;5;241m119\u001b[39m,   \u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     50\u001b[0m           \u001b[38;5;241m180\u001b[39m,  \u001b[38;5;241m1830\u001b[39m,  \u001b[38;5;241m1107\u001b[39m,   \u001b[38;5;241m171\u001b[39m,   \u001b[38;5;241m102\u001b[39m])\n\u001b[0;32m---> 52\u001b[0m origin_text \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_plus\u001b[49m(origin_input_id, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_offsets_mapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(origin_text)\n\u001b[1;32m     54\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(origin_text)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertTokenizerFast' object has no attribute 'decode_plus'"
     ]
    }
   ],
   "source": [
    "from prepared_for_mlm import decode_token, get_tokens_for_words, mask_content_words\n",
    "def mask_content_words(ids, word_dict):\n",
    "    '''\n",
    "    input:\n",
    "        ids: sample_id \n",
    "        word_dict: {'The': [tensor(170)], 'capital': [tensor(1109), tensor(3007)], ....}\n",
    "    output:\n",
    "        masked_sentences: [tensor([  101,  1103, 175, 10555,  103,   103,   103,   102])\n",
    "        label_ids: [tensor(6468)], [tensor(1568), tensor(13892)]\n",
    "    '''\n",
    "    labels = []\n",
    "    masked_sentences = []\n",
    "    except_tokens = [TOKENIZER.cls_token_id, TOKENIZER.sep_token_id, TOKENIZER.pad_token_id, TOKENIZER.unk_token_id]\n",
    "    count_masked_words = []\n",
    "    for (key, value) in word_dict.items():\n",
    "        masked_ids = ids.clone() # torch.Size([1, 85])\n",
    "        origin_sample = decode_token(masked_ids[0])\n",
    "        if get_pos_tag_word(key, origin_sample).values in ['NOUN', 'VERB', 'ADJ', 'ADV']:\n",
    "            print(\"KEY: \", key)\n",
    "            for i in range(len(masked_ids[0]) - len(value) + 1):\n",
    "                masked_id = masked_ids.clone()\n",
    "                label = torch.full_like(masked_id, fill_value=-100)\n",
    "                masked_indice = torch.full_like(masked_id, fill_value=0)\n",
    "            \n",
    "                if torch.equal(torch.as_tensor(masked_id[0][i:i+len(value)]).clone().detach(), torch.as_tensor(value).clone().detach()):\n",
    "                    masked_id[0][i:i+len(value)]= TOKENIZER.mask_token_id\n",
    "                    #print(\"MASKED ID: \", masked_id)\n",
    "                    masked_indice[masked_id == TOKENIZER.mask_token_id] = 1\n",
    "                    #print(\"MASKED INDICE: \", masked_indice)\n",
    "                    \n",
    "                    \n",
    "                    for idx, mask in enumerate(masked_indice[0]):\n",
    "                        if mask == 1:\n",
    "                            label[0][idx] = ids[0][idx]\n",
    "                            \n",
    "                    masked_sentences.append(masked_id)\n",
    "                    labels.append(label)\n",
    "                # count_masked_words.append(count_masked_key)\n",
    "            \n",
    "    return masked_sentences, labels\n",
    "\n",
    "origin_input_id = torch.tensor([  101,   189, 27226,  1116,  1114,  2549, 21521,  1198,   126,  1106,\n",
    "         4252,  1320,   122,   117,  1137,  6531,  1121,   172,   118,  1139,\n",
    "         1665,   117,  1138,  2999,   172,   118,  1139,  1665,   182, 11782,\n",
    "          1116,  1104,   123,   119,  1512,  1105,   123,   119,   125,   180,\n",
    "         1830,   117,  1134, 11271,  1120,  1147,   126,  3769,   117,  1229,\n",
    "         189, 27226,  1116,  1114,  2549, 21521,  1439,  4252,  1320,   122,\n",
    "         1137, 27553,  1179,   122, 13000,   172,   118,  1139,  1665,   182,\n",
    "        11782,   1116,   113,   123,   119,   122,   118,   123,   119,   128,\n",
    "          180,  1830,  1107,   171,   102])\n",
    "\n",
    "origin_text = tokenizer.decode_plus(origin_input_id, skip_special_tokens=True, return_offsets_mapping=True) \n",
    "print(origin_text)\n",
    "doc = nlp(origin_text)\n",
    "words_str = [word.text for word in [token for token in doc]]\n",
    "\n",
    "token_test = tokenizer.encode_plus(\n",
    "                ' '.join(words_str),\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                padding='max_length', \n",
    "                truncation=True,  \n",
    "                add_special_tokens = True,\n",
    "                return_tensors=\"pt\",  \n",
    "                return_attention_mask = True,\n",
    "                return_offsets_mapping=True  \n",
    "            )\n",
    "\n",
    "word_dict = get_tokens_for_words(\n",
    "    words_str, \n",
    "    token_test['input_ids'], \n",
    "    token_test['offset_mapping'][0]\n",
    "    )\n",
    "\n",
    "masked_sen_2d = origin_input_id.view(-1, 85)\n",
    "\n",
    "masked_sentences, label_ids = mask_content_words(masked_sen_2d, word_dict)\n",
    "# print(len(masked_sentences), len(label_ids  ))\n",
    "# for i in range(len(masked_sentences)):\n",
    "#     print(label_ids[i], masked_sentences[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test is POs match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag_word(word, text):\n",
    "    doc = nlp(text)\n",
    "    word_split = word.split()\n",
    "    print(\"================\")\n",
    "    for token in doc:\n",
    "        if token.text in word_split:\n",
    "            print(token.text, token.pos_)\n",
    "        #     return token.pos_\n",
    "    return None\n",
    "\n",
    "def get_key(dictionary, value, count_word):\n",
    "    unique_set = torch.chunk(value, count_word)[0]\n",
    "    for key, val in dictionary.items():\n",
    "        if compare_tensors(val, unique_set):\n",
    "            return key\n",
    "    return None \n",
    "def is_POS_match(input_ids, lm_label_ids, b_count_word):\n",
    "    '''\n",
    "    Function to check if the POS tag of the masked token in the logits is the same as the POS tag of the masked token in the original text.\n",
    "    Note: This function assumes that the logits are of shape # ([85, 28996]) \n",
    "    lm_label_ids: shape (batch_size, sequence_length)\n",
    "    '''\n",
    "    \n",
    "    origin_input_id = input_ids.clone() # Origin input id:  torch.Size([85])\n",
    "    result_ids = input_ids.clone() # torch.Size([85])\n",
    "    \n",
    "    # Find the index of the masked token from lm_label_ids\n",
    "    masked_idx = torch.where(lm_label_ids != -100)[0]\n",
    "    masked_idx_input = torch.where(input_ids == tokenizer.mask_token_id)[0]\n",
    "    origin_input_id[masked_idx_input] = lm_label_ids[masked_idx] \n",
    "    \n",
    "    \n",
    "    # make sure masked_idx_input and masked_idx are the same using assert\n",
    "    assert torch.equal(masked_idx, masked_idx_input), \"Masked index and label index are not the same.\"\n",
    "    print(\"lm_label_ids[masked_idx]  \", lm_label_ids )\n",
    "    # get pos tag of origin text\n",
    "    origin_text = tokenizer.decode(origin_input_id, skip_special_tokens=True) \n",
    "    \n",
    "    doc = nlp(origin_text)\n",
    "    words_str = [word.text for word in [token for token in doc]]\n",
    "    \n",
    "    token_test = tokenizer.encode_plus(\n",
    "                    ' '.join(words_str),\n",
    "                    max_length=MAX_SEQ_LEN,\n",
    "                    padding='max_length', \n",
    "                    truncation=True,  \n",
    "                    add_special_tokens = True,\n",
    "                    return_tensors=\"pt\",  \n",
    "                    return_attention_mask = True,\n",
    "                    return_offsets_mapping=True  \n",
    "                )\n",
    "    masked_sens_2d = torch.unsqueeze(input_ids, 0)\n",
    "    label_ids_2d = torch.unsqueeze(lm_label_ids, 0)\n",
    "    output = model(input_ids = masked_sens_2d, attention_mask = token_test['attention_mask'], token_type_ids=token_test['token_type_ids'], labels=label_ids_2d)\n",
    "    print(\"shape logits: \", output.logits.shape)  ##shape logits:  torch.Size([1, 85, 28996])\n",
    "    \n",
    "    word_dict = get_tokens_for_words(\n",
    "        words_str, \n",
    "        token_test['input_ids'], \n",
    "        token_test['offset_mapping'][0]\n",
    "        )\n",
    "      \n",
    "    # Get key for the masked token\n",
    "    \n",
    "    masked_word = get_key(word_dict, origin_input_id[masked_idx],b_count_word )\n",
    "    print(\"masked word: \", masked_word)\n",
    "    pos_tag_origin = get_pos_tag_word(masked_word, origin_text)\n",
    "    print(\"pos tag origin: \", masked_word, pos_tag_origin)         \n",
    "    \n",
    "    # # Extract the logits for the masked position\n",
    "    masked_logits = output.logits[0][masked_idx]\n",
    "    # print(\"MASKED LOGITS: \", masked_logits) # torch.Size([28996])\n",
    "    pred = [torch.argmax(output.logits[0][i]).item() for i in masked_idx]\n",
    "    # Print top 10 masked tokens\n",
    "        # print(tokenizer.convert_ids_to_tokens(torch.topk(outputs.logits[0, idx, :], 10).indices))\n",
    "        \n",
    "    # Replace the index of the masked token with the list of predicted tokens\n",
    "    for i in range(len(masked_idx)):\n",
    "        result_ids[masked_idx[i]] = pred[i]\n",
    "        \n",
    "    # print(\"result sentence: \", tokenizer.decode(result_ids, skip_special_tokens=True))\n",
    "    for i in pred:\n",
    "        print(\"PRED WORD: \",i,  tokenizer.decode(i))\n",
    "    print(\"PRED WORD: \",pred,  tokenizer.decode(pred))\n",
    "    logits_tag = get_pos_tag_word(tokenizer.decode(pred), tokenizer.decode(result_ids, skip_special_tokens=True) )\n",
    "    print(\"POS TAG PRED WORD: \", logits_tag)\n",
    "    \n",
    "    \n",
    "    # Cross-entropy term\n",
    "    \n",
    "    cross_entropy_term = F.cross_entropy(output.logits.view(-1, tokenizer.vocab_size), lm_label_ids.view(-1))\n",
    "    print(\"Cross entropy term shape: \", cross_entropy_term.shape)      ##Cross entropy term shape:  torch.Size([2720])\n",
    "  \n",
    "    # Custom matching term\n",
    "    matching_term_batch = (pos_tag_origin == logits_tag)\n",
    "\n",
    "    # Combine terms to get the loss for 1 batch\n",
    "    matching_term = torch.where(matching_term_batch == torch.tensor(True), torch.tensor(1.0), torch.tensor(0.0))\n",
    "    print(\"Matching term shape: \", matching_term)  ##Matching term shape:  torch.Size([2720])\n",
    "    print(\"cross entropy term: \", cross_entropy_term)\n",
    "    \n",
    "    loss = 0.5 * cross_entropy_term + (1 - matching_term)\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_label_ids[masked_idx]   tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, 5190, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, 5190, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100])\n",
      "shape logits:  torch.Size([1, 85, 28996])\n",
      "masked word:  acid\n",
      "================\n",
      "acid NOUN\n",
      "acid NOUN\n",
      "pos tag origin:  acid None\n",
      "PRED WORD:  26825 insulin\n",
      "PRED WORD:  3850 drug\n",
      "PRED WORD:  [26825, 3850] insulin drug\n",
      "================\n",
      "insulin NOUN\n",
      "drug NOUN\n",
      "POS TAG PRED WORD:  None\n",
      "Cross entropy term shape:  torch.Size([])\n",
      "Matching term shape:  tensor(1.)\n",
      "cross entropy term:  tensor(4.9894, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4947, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "label_id = torch.tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "         -100,  -100,  -100,  -100,   188,  1643, 22548,  -100,  -100,  -100,\n",
    "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "         -100,  -100,  -100,  -100,  -100])\n",
    "\n",
    "input_id= torch.tensor([  101,  1142,  2848,  3792,  2217,   118,   171,  1665,  1918,   187,\n",
    "         1605,  1108, 23448, 11549,  1193, 14715, 13098,  1121,  1103,  1269,\n",
    "        25338,  6697,  1112,   171,  1665,  1918,   117,  1105, 10877,   182,\n",
    "        11782,  7987,  1956,   117,   174,   119,   176,   119,   185, 23415,\n",
    "         6397,  3382,  6840,  1105,   103,   103,   103,   119,   102,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "            0,     0,     0,     0,     0])     \n",
    "\n",
    "print(is_POS_match(input_id, label_id, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tumours is in the given tensor.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample word dictionary and tensor\n",
    "word_dict = {'tumours': [torch.tensor(189)], 'with': [torch.tensor(1114)]}\n",
    "given_tensor = torch.tensor([189, 27226, 1116, 189, 27226, 1116])\n",
    "\n",
    "# Function to check if any tensor in the dictionary values is in the given tensor\n",
    "def is_in_tensor(word_dict, tensor):\n",
    "    for word, tensors in word_dict.items():\n",
    "        for tensor_val in tensors:\n",
    "            if (tensor == tensor_val).any().item():\n",
    "                print(f\"{word} is in the given tensor.\")\n",
    "                break\n",
    "\n",
    "# Check if any word in the dictionary is in the given tensor\n",
    "is_in_tensor(word_dict, given_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7282,  0.6782, -0.3622,  0.9572,  0.9279,  1.0818, -0.4334,\n",
      "          -0.8105, -0.1358,  0.5705],\n",
      "         [ 1.3758, -2.1371, -0.0726, -1.8334, -1.3297, -0.1785,  1.5264,\n",
      "           1.5781,  0.3917, -2.0859],\n",
      "         [ 0.6384,  0.3851, -1.4952,  1.9068, -0.4026, -0.4253, -0.2667,\n",
      "          -0.0509,  0.0526,  0.5777],\n",
      "         [ 0.4248, -0.0755, -0.2062,  0.8857, -0.0867,  0.0920, -1.0359,\n",
      "           0.9809,  2.1640,  0.6484],\n",
      "         [ 0.3108, -0.1413,  0.3952, -0.2666,  1.1596, -0.7127, -0.0294,\n",
      "           0.6728,  0.4344, -0.8212]]])\n",
      "tensor([[[ 1.7282,  0.3218,  1.3622,  0.0428,  0.0721, -0.0818,  1.4334,\n",
      "           1.8105,  1.1358,  0.4295],\n",
      "         [-0.3758,  3.1371,  1.0726,  2.8334,  2.3297,  1.1785, -0.5264,\n",
      "          -0.5781,  0.6083,  3.0859],\n",
      "         [ 0.3616,  0.6149,  2.4952, -0.9068,  1.4026,  1.4253,  1.2667,\n",
      "           1.0509,  0.9474,  0.4223],\n",
      "         [ 0.5752,  1.0755,  1.2062,  0.1143,  1.0867,  0.9080,  2.0359,\n",
      "           0.0191, -1.1640,  0.3516],\n",
      "         [ 0.6892,  1.1413,  0.6048,  1.2666, -0.1596,  1.7127,  1.0294,\n",
      "           0.3272,  0.5656,  1.8212]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming logits is your tensor of shape (batch_size, max_seq_length, vocab_size)\n",
    "logits = torch.randn((1, 5, 10))  # Example tensor, replace it with your actual logits\n",
    "\n",
    "# Compute the inverse of the logits\n",
    "inverse_logits = 1 - logits\n",
    "\n",
    "# You can also use torch.sub() function\n",
    "# inverse_logits = torch.sub(1, logits)\n",
    "print(logits)\n",
    "print(inverse_logits)  # Should output the same shape as logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 1, 1, 1, 3, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def pos_tag_mapping(pos_tag):\n",
    "    if pos_tag ==\"NOUN\":\n",
    "        return 1\n",
    "    elif pos_tag ==\"VERB\":\n",
    "        return 2\n",
    "    elif pos_tag ==\"ADJ\":\n",
    "        return 3\n",
    "    elif pos_tag == \"ADV\":\n",
    "        return 4\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def get_pos_tag_id(word_dict, pos_tag_dict, label_id):\n",
    "    pos_tag_id = []\n",
    "    for label in label_id:\n",
    "        word = None\n",
    "        for key, value in word_dict.items():\n",
    "            if label in value:\n",
    "                word = key\n",
    "                break\n",
    "        if word is not None:\n",
    "            pos_tag = pos_tag_dict.get(word)\n",
    "            pos_tag_id.append(pos_tag_mapping(pos_tag))\n",
    "        else:\n",
    "            pos_tag_id.append(-1)\n",
    "    return pos_tag_id\n",
    "\n",
    "# Example usage\n",
    "import torch\n",
    "\n",
    "word_dict = {\"Ba\": torch.tensor([104, 4394]), \"masture\": torch.tensor([424]), \"ofs\": torch.tensor([434]), \"a\": torch.tensor([32])}\n",
    "pos_tag_dict = {\"Ba\": \"NOUN\", \"masture\": \"NOUN\", \"ofs\":\"ADJ\", \"a\":\"PUNCT\"}\n",
    "label_id = torch.tensor([102, 104, 4394, 424, 434, 32, -100])\n",
    "\n",
    "token_ids = get_pos_tag_id(word_dict, pos_tag_dict, label_id)\n",
    "print(token_ids)  # Output: [-1, 1, 1, 1, 3, -1, -1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Term Tensor: tensor([1, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_matching_terms(pred, original, mask_index, pos_tag_id_pred, pos_tag_id_origin):\n",
    "    matching_term_tensor = torch.where(pos_tag_id_pred[mask_index] == pos_tag_id_origin[mask_index], \n",
    "                                       torch.tensor(1), \n",
    "                                       torch.tensor(0))\n",
    "    return matching_term_tensor\n",
    "\n",
    "# Khởi tạo các giá trị mẫu\n",
    "mask_index = [0, 1, 2]  # Ví dụ danh sách các index của mask\n",
    "pos_tag_id_pred = torch.tensor([1, 2, 3, 4, 5])  # Ví dụ tensor pos_tag_id_pred\n",
    "pos_tag_id_origin = torch.tensor([1, 2, 3, 7, 8])  # Ví dụ tensor pos_tag_id_origin\n",
    "\n",
    "# Kiểm tra matching terms\n",
    "matching_term_tensor = torch.zeros_like(pos_tag_id_pred)\n",
    "matching_term_tensor[mask_index] = check_matching_terms(pred=None, original=None, mask_index=mask_index, \n",
    "                                             pos_tag_id_pred=pos_tag_id_pred, \n",
    "                                             pos_tag_id_origin=pos_tag_id_origin)\n",
    "\n",
    "print(\"Matching Term Tensor:\", matching_term_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag_dict = {'probed': 'VERB'}\n",
    "pred_id = torch.tensor([  101,  1122,  1180,  3510, 10936,  1116,  1107,   189,  4832,  1204, 113,   189,  4889,   118, 20232,  1174, 21718,  2568,   117,   121, 119,  4991,   110,   189,  7921,  1424,   118,  406,   114,  4051, 126,   110,   171,  3292,  2042, 23651,  1312,  1394,   113,  1111, 2848,   118,   185, 15342,  7880,  4649,  5864,  2042,   171,  7841,  1116,   114,  1137, 10458, 12140,  1181,  6831,  1105, 17357, 1181,\n",
    "1114, 26491,   119,   102,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,     0,   0,     0,     0,     0,     0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_mapping(pos_tag):\n",
    "    if pos_tag ==\"NOUN\":\n",
    "        return 1\n",
    "    elif pos_tag ==\"VERB\":\n",
    "        return 2\n",
    "    elif pos_tag ==\"ADJ\":\n",
    "        return 3\n",
    "    elif pos_tag == \"ADV\":\n",
    "        return 4\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "def get_pos_tag_id(word_dict, pos_tag_dict, label_id):\n",
    "    pos_tag_id = torch.full_like(label_id, fill_value=-1)\n",
    "    \n",
    "    for key in pos_tag_dict.keys():\n",
    "        tokens = word_dict.get(key)\n",
    "        \n",
    "        for i in range(len(label_id) - len(tokens) + 1):\n",
    "            if torch.equal(torch.as_tensor(label_id[i:i+len(tokens)]).clone().detach(), torch.as_tensor(tokens).clone().detach()):\n",
    "               \n",
    "                pos_tag_id[i:i+len(tokens)] = pos_tag_mapping(pos_tag_dict.get(key))\n",
    " \n",
    "    return pos_tag_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17357,  1181]) tensor([17357,  1181])\n",
      "58 tensor([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\\n        -1, -1, -1, -1,  2,  2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_id = get_pos_tag_id(word_dict, pos_tag_dict, pred_id)\n",
    "# convert list to string\n",
    "str(list_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17357,  1181])\n"
     ]
    }
   ],
   "source": [
    "print(word_dict.get('probed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
