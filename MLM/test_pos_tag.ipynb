{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "import torch\n",
    "import copy\n",
    "model = BertForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "MAX_SEQ_LEN = 85\n",
    "from mlm_utils.model_utils import TOKENIZER\n",
    "\n",
    "from prepared_for_mlm import masking_sentence_word, get_tokens_for_words, mask_content_words, get_pos_tag\n",
    "import torch.nn.functional as F\n",
    "from mlm_utils.preprocess_functions import get_key, compare_tensors, get_pos_tag_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test mask content wword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "origin_input_id = torch.tensor([  101,   189, 27226,  1116,  1114,  2549, 21521,  1198,   126,  1106,\n",
    "         4252,  1320,   122,   117,  1137,  6531,  1121,   172,   118,  1139,\n",
    "         1665,   117,  1138,  2999,   172,   118,  1139,  1665,   182, 11782,\n",
    "          1116,  1104,   123,   119,  1512,  1105,   123,   119,   125,   180,\n",
    "         1830,   117,  1134, 11271,  1120,  1147,   126,  3769,   117,  1229,\n",
    "         189, 27226,  1116,  1114,  2549, 21521,  1439,  4252,  1320,   122,\n",
    "         1137, 27553,  1179,   122, 13000,   172,   118,  1139,  1665,   182,\n",
    "        11782,   1116,   113,   123,   119,   122,   118,   123,   119,   128,\n",
    "          180,  1830,  1107,   171,   102])\n",
    "\n",
    "origin_text = tokenizer.decode(origin_input_id, skip_special_tokens=True) \n",
    "    \n",
    "doc = nlp(origin_text)\n",
    "words_str = [word.text for word in [token for token in doc]]\n",
    "\n",
    "token_test = tokenizer.encode_plus(\n",
    "                ' '.join(words_str),\n",
    "                max_length=MAX_SEQ_LEN,\n",
    "                padding='max_length', \n",
    "                truncation=True,  \n",
    "                add_special_tokens = True,\n",
    "                return_tensors=\"pt\",  \n",
    "                return_attention_mask = True,\n",
    "                return_offsets_mapping=True  \n",
    "            )\n",
    "\n",
    "word_dict = get_tokens_for_words(\n",
    "    words_str, \n",
    "    token_test['input_ids'], \n",
    "    token_test['offset_mapping'][0]\n",
    "    )\n",
    "\n",
    "masked_sentences, label_ids = mask_content_words(origin_input_id, word_dict)\n",
    "for i in range(len(masked_sentences)):\n",
    "    print(label_ids[i], masked_sentences[i], tokenizer.decode(masked_sentences[i], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test is POs match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag_word(word, text):\n",
    "    doc = nlp(text)\n",
    "    word_split = word.split()\n",
    "    print(\"================\")\n",
    "    for token in doc:\n",
    "        if token.text in word_split:\n",
    "            print(token.text, token.pos_)\n",
    "        #     return token.pos_\n",
    "    return None\n",
    "\n",
    "def get_key(dictionary, value, count_word):\n",
    "    unique_set = torch.chunk(value, count_word)[0]\n",
    "    for key, val in dictionary.items():\n",
    "        if compare_tensors(val, unique_set):\n",
    "            return key\n",
    "    return None \n",
    "def is_POS_match(input_ids, lm_label_ids, b_count_word):\n",
    "    '''\n",
    "    Function to check if the POS tag of the masked token in the logits is the same as the POS tag of the masked token in the original text.\n",
    "    Note: This function assumes that the logits are of shape # ([85, 28996]) \n",
    "    lm_label_ids: shape (batch_size, sequence_length)\n",
    "    '''\n",
    "    \n",
    "    origin_input_id = input_ids.clone() # Origin input id:  torch.Size([85])\n",
    "    result_ids = input_ids.clone() # torch.Size([85])\n",
    "    \n",
    "    # Find the index of the masked token from lm_label_ids\n",
    "    masked_idx = torch.where(lm_label_ids != -100)[0]\n",
    "    masked_idx_input = torch.where(input_ids == tokenizer.mask_token_id)[0]\n",
    "    origin_input_id[masked_idx_input] = lm_label_ids[masked_idx] \n",
    "    \n",
    "    \n",
    "    # make sure masked_idx_input and masked_idx are the same using assert\n",
    "    assert torch.equal(masked_idx, masked_idx_input), \"Masked index and label index are not the same.\"\n",
    "    print(\"lm_label_ids[masked_idx]  \", lm_label_ids )\n",
    "    # get pos tag of origin text\n",
    "    origin_text = tokenizer.decode(origin_input_id, skip_special_tokens=True) \n",
    "    \n",
    "    doc = nlp(origin_text)\n",
    "    words_str = [word.text for word in [token for token in doc]]\n",
    "    \n",
    "    token_test = tokenizer.encode_plus(\n",
    "                    ' '.join(words_str),\n",
    "                    max_length=MAX_SEQ_LEN,\n",
    "                    padding='max_length', \n",
    "                    truncation=True,  \n",
    "                    add_special_tokens = True,\n",
    "                    return_tensors=\"pt\",  \n",
    "                    return_attention_mask = True,\n",
    "                    return_offsets_mapping=True  \n",
    "                )\n",
    "    masked_sens_2d = torch.unsqueeze(input_ids, 0)\n",
    "    label_ids_2d = torch.unsqueeze(lm_label_ids, 0)\n",
    "    output = model(input_ids = masked_sens_2d, attention_mask = token_test['attention_mask'], token_type_ids=token_test['token_type_ids'], labels=label_ids_2d)\n",
    "    \n",
    "    \n",
    "    word_dict = get_tokens_for_words(\n",
    "        words_str, \n",
    "        token_test['input_ids'], \n",
    "        token_test['offset_mapping'][0]\n",
    "        )\n",
    "      \n",
    "    # Get key for the masked token\n",
    "    \n",
    "    masked_word = get_key(word_dict, origin_input_id[masked_idx],b_count_word )\n",
    "    print(\"masked word: \", masked_word)\n",
    "    pos_tag_origin = get_pos_tag_word(masked_word, origin_text)\n",
    "    print(\"pos tag origin: \", masked_word, pos_tag_origin)         \n",
    "    \n",
    "    # # Extract the logits for the masked position\n",
    "    masked_logits = output.logits[0][masked_idx]\n",
    "    # print(\"MASKED LOGITS: \", masked_logits) # torch.Size([28996])\n",
    "    pred = [torch.argmax(output.logits[0][i]).item() for i in masked_idx]\n",
    "    # Print top 10 masked tokens\n",
    "        # print(tokenizer.convert_ids_to_tokens(torch.topk(outputs.logits[0, idx, :], 10).indices))\n",
    "        \n",
    "    # Replace the index of the masked token with the list of predicted tokens\n",
    "    for i in range(len(masked_idx)):\n",
    "        result_ids[masked_idx[i]] = pred[i]\n",
    "        \n",
    "    # print(\"result sentence: \", tokenizer.decode(result_ids, skip_special_tokens=True))\n",
    "    for i in pred:\n",
    "        print(\"PRED WORD: \",i,  tokenizer.decode(i))\n",
    "    print(\"PRED WORD: \",pred,  tokenizer.decode(pred))\n",
    "    logits_tag = get_pos_tag_word(tokenizer.decode(pred), tokenizer.decode(result_ids, skip_special_tokens=True) )\n",
    "    print(\"POS TAG PRED WORD: \", logits_tag)\n",
    "    \n",
    "    \n",
    "    # Cross-entropy term\n",
    "    \n",
    "    cross_entropy_term = F.cross_entropy(output.logits.view(-1, tokenizer.vocab_size), lm_label_ids.view(-1))\n",
    "    print(\"Cross entropy term shape: \", cross_entropy_term.shape)      ##Cross entropy term shape:  torch.Size([2720])\n",
    "  \n",
    "    # Custom matching term\n",
    "    matching_term_batch = (pos_tag_origin == logits_tag)\n",
    "\n",
    "    # Combine terms to get the loss for 1 batch\n",
    "    matching_term = torch.where(matching_term_batch == torch.tensor(True), torch.tensor(1.0), torch.tensor(0.0))\n",
    "    print(\"Matching term shape: \", matching_term)  ##Matching term shape:  torch.Size([2720])\n",
    "    print(\"cross entropy term: \", cross_entropy_term)\n",
    "    \n",
    "    loss = 0.5 * cross_entropy_term + (1 - matching_term)\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_label_ids[masked_idx]   tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, 5190, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, 5190, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100])\n",
      "masked word:  acid\n",
      "================\n",
      "acid NOUN\n",
      "acid NOUN\n",
      "pos tag origin:  acid None\n",
      "PRED WORD:  26825 insulin\n",
      "PRED WORD:  3850 drug\n",
      "PRED WORD:  [26825, 3850] insulin drug\n",
      "================\n",
      "insulin NOUN\n",
      "drug NOUN\n",
      "POS TAG PRED WORD:  None\n",
      "Cross entropy term shape:  torch.Size([])\n",
      "Matching term shape:  tensor(1.)\n",
      "cross entropy term:  tensor(4.9894, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4947, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "label_id = torch.tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5190, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5190, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])\n",
    "\n",
    "input_id= torch.tensor([101, 1748, 4982, 117, 1103, 3687, 26883, 1320, 1104, 194, 3031, 1162, 2423, 170, 15792, 19033, 103, 4789, 1107, 19255, 118, 4065, 3652, 117, 8783, 1115, 194, 3031, 1162, 2399, 170, 3607, 1648, 1107, 19255, 118, 4065, 103, 4789, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "print(is_POS_match(input_id, label_id, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tumours is in the given tensor.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Sample word dictionary and tensor\n",
    "word_dict = {'tumours': [torch.tensor(189)], 'with': [torch.tensor(1114)]}\n",
    "given_tensor = torch.tensor([189, 27226, 1116, 189, 27226, 1116])\n",
    "\n",
    "# Function to check if any tensor in the dictionary values is in the given tensor\n",
    "def is_in_tensor(word_dict, tensor):\n",
    "    for word, tensors in word_dict.items():\n",
    "        for tensor_val in tensors:\n",
    "            if (tensor == tensor_val).any().item():\n",
    "                print(f\"{word} is in the given tensor.\")\n",
    "                break\n",
    "\n",
    "# Check if any word in the dictionary is in the given tensor\n",
    "is_in_tensor(word_dict, given_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
