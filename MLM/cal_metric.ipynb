{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "\n",
    "def read_csv(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "    return data\n",
    "\n",
    "\n",
    "def apply_eval(data):\n",
    "    \"\"\"\n",
    "    Apply eval function to all elements in a list.\n",
    "    \n",
    "    Args:\n",
    "    - data (list): List of strings\n",
    "    \n",
    "    Returns:\n",
    "    - list: List with elements evaluated\n",
    "    \"\"\"\n",
    "    # Define required functions and modules\n",
    "    eval_locals = {'tensor': torch.tensor, 'torch': torch}\n",
    "    \n",
    "    return [eval(item, eval_locals) for item in data]\n",
    "\n",
    "\n",
    "def convert_to_dict(data):\n",
    "    list_items = map(lambda x: apply_eval(x), data[1:])\n",
    "    \n",
    "    list_data_dict = [{\"uid\": items[0], \n",
    "                        \"word_vector\": items[1].clone().detach(), \n",
    "                        \"sum_vector\": items[2].clone().detach(), \n",
    "                        \"avg_vector\": items[3].clone().detach()} \n",
    "                      for items in list_items]\n",
    "    \n",
    "    return list_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file and convert to dict\n",
    "import json\n",
    "\n",
    "def read_data(readPath):\n",
    "\n",
    "    with open(readPath, 'r', encoding = 'utf-8') as file:\n",
    "        taskData = []\n",
    "        for i, line in enumerate(file):\n",
    "            sample = json.loads(line)\n",
    "            taskData.append(sample)\n",
    "\n",
    "    return taskData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate cosine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sys.path.append('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM')\n",
    "from mlm_utils.model_utils import TOKENIZER\n",
    "\n",
    "def cosine_sen_content_word(sum_vector_tensor, content_word_dict):\n",
    "    '''một từ với 1 content word\n",
    "    trả ra consine similarity giữa sum_vector_tensor và sum_vector của content word\n",
    "    '''\n",
    "    # SUM VECTOR \n",
    "    sum_content_tensor = content_word_dict['sum_vector'].clone().detach()\n",
    "    \n",
    "    # cosine_val_sum = cosine_similarity(sum_vector_tensor.unsqueeze(0), sum_content_tensor.unsqueeze(0))[0][0] \n",
    "    second_cosine = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    cosine_val_sum = second_cosine(sum_vector_tensor, sum_content_tensor).item()\n",
    "   \n",
    "    return cosine_val_sum, content_word_dict['word_vector']\n",
    "\n",
    " \n",
    "def cosine_sim_sen_list_word(sum_vector_tensor, list_dict_content_word):\n",
    "    '''tinh cosine trong 1 cau voi tat ca cac content word. trả ra từ có cosine gần -1 và từ có cosine gần 0'''\n",
    "    list_result = map(lambda x: cosine_sen_content_word(sum_vector_tensor, x), list_dict_content_word)\n",
    "    list_result = sorted(list_result, key = lambda x: x[0], reverse = True) # giam dan\n",
    "    \n",
    "    # cosine gan -1\n",
    "    min_cosine_neg, replace_word_cosine_neg = list_result[-1]\n",
    "              \n",
    "    # consine gan 0\n",
    "    sort_positive_cosine = list(filter(lambda x: x[0] > 0, list_result))\n",
    "    min_cosine_0, replace_word_cosine_0 = sort_positive_cosine[-1]\n",
    "     \n",
    "    return replace_word_cosine_neg, replace_word_cosine_0\n",
    "   \n",
    "def replace_word_in_sentence(predicate_data, content_word_data):\n",
    "    ''' 1 câu gốc thành 2 câu mới: cosine = -1 và cosine = 0\n",
    "    '''\n",
    "    masked_index = torch.where(torch.tensor(predicate_data['pos_tag_id']).clone().detach() != 0)\n",
    "\n",
    "    predicate_np = np.array(predicate_data['input_id'])\n",
    "    replace_word_cosine_neg = None\n",
    "    replace_word_cosine_0 = None\n",
    "    \n",
    "    sum_vector_tensor = torch.tensor(predicate_data['sum_vector']).clone().detach()\n",
    "    if predicate_data['pos_tag_id'][masked_index[0][0].item()] == 1:\n",
    "        replace_word_cosine_neg, replace_word_cosine_0 = cosine_sim_sen_list_word(sum_vector_tensor, content_word_data['noun'])\n",
    "    elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 2:\n",
    "        replace_word_cosine_neg, replace_word_cosine_0 = cosine_sim_sen_list_word(sum_vector_tensor, content_word_data['verb'])\n",
    "    elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 3:\n",
    "        replace_word_cosine_neg, replace_word_cosine_0 = cosine_sim_sen_list_word(sum_vector_tensor, content_word_data['adj'])\n",
    "    elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 4:\n",
    "        replace_word_cosine_neg, replace_word_cosine_0 = cosine_sim_sen_list_word(sum_vector_tensor, content_word_data['adv'])\n",
    "    else: \n",
    "        KeyError('pos_tag_id not in [1, 2, 3, 4]')\n",
    "    \n",
    "    print(\"masked index: \", masked_index[0])   \n",
    "    print(\"type predicate\", type(predicate_data['input_id']), type(predicate_data['input_id'][0]))\n",
    "    print(\"predicate data\", predicate_np[masked_index[0]]) \n",
    "    \n",
    "   \n",
    "    predicate_np[masked_index[0]] = replace_word_cosine_neg\n",
    "    \n",
    "    new_data_cosine_neg = predicate_np.tolist()\n",
    "    \n",
    "    predicate_np[masked_index[0]] = replace_word_cosine_0\n",
    "    new_data_cosine_pos = predicate_np.tolist()\n",
    "    \n",
    "    return {\"origin_id\": predicate_data['input_id'], \n",
    "            \"cosine_neg_id\": new_data_cosine_neg, \n",
    "            \"cosine_0_id\": new_data_cosine_pos, \n",
    "            \"pos_tag_id\":predicate_data['pos_tag_id']}\n",
    "\n",
    "def create_new_data_neg_sum(predicate_file, dict_content_word, wriDir):\n",
    "    '''\n",
    "    return 4 data cho moi file:\n",
    "        sum_vector va cosine = -1: uid, origin_input_id, sum_neg_input_id, pos_tag_id\n",
    "        sum_vector va cosine = 0: uid, origin_input_id, sum_0_input_id, pos_tag_id\n",
    "        avg_vector va cosine = -1: uid, origin_input_id, avg_neg_input_id, pos_tag_id\n",
    "        avg_vector va cosine = 0: uid, origin_input_id, avg_0_input_id, pos_tag_id\n",
    "    \n",
    "    '''\n",
    "    predicates_data = read_data(predicate_file)\n",
    "    predicates_data =  [item for sublist in predicates_data for item in sublist]\n",
    "    \n",
    "    new_data = map(lambda x: replace_word_in_sentence(x, dict_content_word), predicates_data[:50])\n",
    "    \n",
    "    # write to csv file with header uid, origin_sentence, cosine_neg_sentence, cosine_0_sentence, pos_tag_id\n",
    "    with open(wriDir + 'sum_vector_{}'.format(predicate_file.split('/')[-1].replace('mlm_', '').replace('.json', '.csv')), 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for item in new_data:\n",
    "            writer.writerow(item)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = './word_present_each_file/mlm_abolish_full.json'\n",
    "wriDir = './pertured_data/'\n",
    "\n",
    "noun_file = read_csv('./list_content_word_v2/NOUN.csv')\n",
    "verb_file = read_csv('./list_content_word_v2/VERB.csv')\n",
    "adj_file = read_csv('./list_content_word_v2/ADJ.csv')\n",
    "adv_file = read_csv('./list_content_word_v2/ADV.csv')\n",
    "\n",
    "dict_content_word = {\"noun\": convert_to_dict(noun_file),\n",
    "                        \"verb\": convert_to_dict(verb_file),\n",
    "                        \"adj\": convert_to_dict(adj_file),\n",
    "                        \"adv\": convert_to_dict(adv_file)}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked index:  tensor([7])\n",
      "type predicate <class 'list'> <class 'int'>\n",
      "predicate data 6468\n",
      "masked index:  tensor([10])\n",
      "type predicate <class 'list'> <class 'int'>\n",
      "predicate data 1148\n",
      "masked index:  tensor([11, 12, 13, 14])\n",
      "type predicate <class 'list'> <class 'int'>\n",
      "predicate data [  183 21977 26918 23767]\n",
      "masked index:  tensor([16, 17])\n",
      "type predicate <class 'list'> <class 'int'>\n",
      "predicate data [27553  1179]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (4,) could not be broadcast to indexing result of shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_new_data_neg_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_content_word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriDir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [26], line 91\u001b[0m, in \u001b[0;36mcreate_new_data_neg_sum\u001b[0;34m(predicate_file, dict_content_word, wriDir)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(wriDir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum_vector_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(predicate_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlm_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     90\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(file)\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m new_data:\n\u001b[1;32m     92\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwriterow(item)\n",
      "Cell \u001b[0;32mIn [26], line 86\u001b[0m, in \u001b[0;36mcreate_new_data_neg_sum.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     83\u001b[0m predicates_data \u001b[38;5;241m=\u001b[39m read_data(predicate_file)\n\u001b[1;32m     84\u001b[0m predicates_data \u001b[38;5;241m=\u001b[39m  [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m predicates_data \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[0;32m---> 86\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mreplace_word_in_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_content_word\u001b[49m\u001b[43m)\u001b[49m, predicates_data[:\u001b[38;5;241m50\u001b[39m])\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# write to csv file with header uid, origin_sentence, cosine_neg_sentence, cosine_0_sentence, pos_tag_id\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(wriDir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum_vector_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(predicate_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlm_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "Cell \u001b[0;32mIn [26], line 66\u001b[0m, in \u001b[0;36mreplace_word_in_sentence\u001b[0;34m(predicate_data, content_word_data)\u001b[0m\n\u001b[1;32m     62\u001b[0m predicate_np[masked_index[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m replace_word_cosine_neg\n\u001b[1;32m     64\u001b[0m new_data_cosine_neg \u001b[38;5;241m=\u001b[39m predicate_np\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 66\u001b[0m \u001b[43mpredicate_np\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmasked_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m replace_word_cosine_0\n\u001b[1;32m     67\u001b[0m new_data_cosine_pos \u001b[38;5;241m=\u001b[39m predicate_np\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morigin_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: predicate_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_id\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine_neg_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: new_data_cosine_neg, \n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine_0_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: new_data_cosine_pos, \n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_tag_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:predicate_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_tag_id\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (4,) could not be broadcast to indexing result of shape (2,)"
     ]
    }
   ],
   "source": [
    "create_new_data_neg_sum(file_data, dict_content_word, wriDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
