{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate cosine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from transformers import BertTokenizerFast\n",
    "# sys.path.append('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM')\n",
    "BERT_PRETRAIN_MODEL = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "TOKENIZER = BertTokenizerFast.from_pretrained(BERT_PRETRAIN_MODEL, do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(readPath):\n",
    "    df = pd.read_json(readPath, lines=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def cosine_module(a, b, cosine_sum):\n",
    "    norm_array1 = np.linalg.norm(a)\n",
    "    norm_array2 = np.linalg.norm(b)\n",
    "    \n",
    "    module_similarity = 1 - (np.abs(norm_array1 - norm_array2) / (norm_array1 + norm_array2))\n",
    "    \n",
    "    return module_similarity * cosine_sum\n",
    " \n",
    "\n",
    "def cosine_module_2_tensors(tensor1, tensor2, cosine_sum_m):\n",
    "\n",
    "    norm_tensor1 = torch.norm(tensor1)\n",
    "    norm_tensor2 = torch.norm(tensor2)\n",
    "    \n",
    "    # cosine_sim_v2 = cosine_func(tensor1 / norm_tensor1, tensor2 / norm_tensor2)\n",
    "    module_similarity = 1 - (torch.abs(norm_tensor1 - norm_tensor2) / (norm_tensor1 + norm_tensor2))\n",
    "    \n",
    "    return module_similarity * cosine_sum_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def cosine_sen_content_word(sum_tensor, avg_tensor, content_word_dict):\n",
    "    '''một từ với 1 content word\n",
    "    trả ra consine similarity giữa sum_vector_tensor và sum_vector của content word\n",
    "    '''\n",
    "   \n",
    "    content_tensor_sum = content_word_dict['sum_vector']\n",
    "    # content_tensor_avg = torch.tensor(content_word_dict['avg_vector']).clone().detach()\n",
    "    t0 = time.time()\n",
    "    # cosine_sum = cosine_func(sum_tensor/torch.norm(sum_tensor), content_tensor_sum/torch.norm(content_tensor_sum)).item()\n",
    "    cosine_sum = cosine_sim(sum_tensor, content_tensor_sum)\n",
    "    cosine_module_sum = cosine_module(sum_tensor, content_tensor_sum, cosine_sum)\n",
    "    print(\"time cu\", time.time() - t0)\n",
    "    print(\"cosine cu\", cosine_sum, cosine_module_sum)\n",
    "    print(\"=======\")\n",
    "    \n",
    "    sum_vector_tensor = torch.tensor(sum_tensor).clone().detach()\n",
    "    # avg_vector_tensor = torch.tensor(avg_tensor).clone().detach()\n",
    "    content_tensor_sum_tensor = torch.tensor(content_tensor_sum).clone().detach()\n",
    "    t1 = time.time()\n",
    "    cosine_func = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    cosine_sum_m = cosine_func(sum_vector_tensor/torch.norm(sum_vector_tensor), content_tensor_sum_tensor/torch.norm(content_tensor_sum_tensor)).item()\n",
    "    cosine_module_sum_m = cosine_module_2_tensors(sum_vector_tensor, content_tensor_sum_tensor, cosine_sum_m)\n",
    "    print(\"time moi\", time.time() - t1)\n",
    "    print(\"cosine moi\", cosine_sum_m, cosine_module_sum_m)\n",
    "    # cosine_avg = cosine_func(avg_tensor/torch.norm(avg_tensor), content_tensor_avg/torch.norm(content_tensor_avg)).item()\n",
    "    # cosine_module_avg = cosine_module_2_tensors(avg_tensor, content_tensor_avg, cosine_func)\n",
    "    \n",
    "    return cosine_sum, cosine_module_sum, content_word_dict['word'] # , cosine_avg, cosine_module_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_sim_sen_list_word(sum_tensor, avg_tensor, list_dict_content_word):\n",
    "    '''tinh cosine trong 1 cau voi tat ca cac content word. trả ra từ có cosine gần -1 và từ có cosine gần 0'''\n",
    "    list_result = list(map(lambda x: cosine_sen_content_word(sum_tensor, avg_tensor, x), list_dict_content_word))\n",
    "    \n",
    "    sorted_cos_sum = sorted(list_result, key = lambda x: x[0]) # tang dan\n",
    "   \n",
    "    sorted_cos_module_sum = sorted(list_result, key = lambda x: x[1]) # tang dan\n",
    "    \n",
    "    # sorted_cos_avg = sorted(list_result, key = lambda x: x[3]) # tang dan\n",
    "    \n",
    "    # sorted_cos_module_avg = sorted(list_result, key = lambda x: x[4]) # tang dan\n",
    "    \n",
    "    # cosine gần -1\n",
    "    cos_neg_sum, _, replace_word_neg_cos_sum = sorted_cos_sum[0]\n",
    "    _, cos_module_neg_sum, replace_word_neg_cos_module_sum = sorted_cos_module_sum[0]\n",
    "    # _, _, replace_word_neg_cos_avg, cos_avg, _ = sorted_cos_avg[-1]\n",
    "    # _, _, replace_word_neg_cos_module_avg, _, cos_module_avg = sorted_cos_module_avg[-1]\n",
    "    \n",
    "    # cosine gần 0\n",
    "    sorted_cos_sum.sort(key=lambda x: abs(x[0]))\n",
    "    cos_pos_sum, _, replace_word_0_cos_sum = sorted_cos_sum[0]\n",
    "   \n",
    "    sorted_cos_module_sum.sort(key=lambda x: abs(x[1]))\n",
    "    _, cos_module_pos_sum, replace_word_0_cos_module_sum = sorted_cos_module_sum[0]\n",
    "   \n",
    "    \n",
    "    # abs_list_cos_avg = list(map(lambda x: abs(x[3] - 0), sorted_cos_avg))\n",
    "    # min_abs_cos_avg = min(abs_list_cos_avg)\n",
    "    # _, _, replace_word_0_cos_avg, cos_pos_avg, _ = sorted_cos_avg[abs_list_cos_avg.index(min_abs_cos_avg)]\n",
    "    \n",
    "    # abs_list_cos_module_avg = list(map(lambda x: abs(x[4] - 0), sorted_cos_module_avg))\n",
    "    # min_abs_cos_module_avg = min(abs_list_cos_module_avg)\n",
    "    # _, _, replace_word_0_cos_module_avg, _, cos_module_pos_avg = sorted_cos_module_avg[abs_list_cos_module_avg.index(min_abs_cos_module_avg)]\n",
    "    \n",
    "    return replace_word_neg_cos_sum, replace_word_neg_cos_module_sum, replace_word_0_cos_sum, replace_word_0_cos_module_sum #, replace_word_neg_cos_avg, replace_word_neg_cos_module_avg, replace_word_0_cos_avg, replace_word_0_cos_module_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_word_in_sentence(predicate_data_chunk, chunkNumber, tempList, content_word_data):\n",
    "    ''' 1 câu gốc thành 2 câu mới: cosine = -1 và cosine = 0\n",
    "    '''\n",
    "    name = 'new_data_{}.json'.format(str(chunkNumber))\n",
    "\n",
    "    # cosine_func = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    def new_word_in_sen(predicate_data):\n",
    "       \n",
    "        # masked_index = torch.where(torch.tensor(predicate_data['pos_tag_id']).clone().detach() != 0)\n",
    "        masked_index = np.where(np.array(predicate_data['pos_tag_id']) != 0)\n",
    "\n",
    "        predicate_np = np.array(predicate_data['origin_id'])\n",
    "        \n",
    "        avg_tensor = predicate_data['avg_vector']\n",
    "        sum_tensor = predicate_data['sum_vector']\n",
    "        \n",
    "        content_word = None\n",
    "        pos_tag = None\n",
    "        if predicate_data['pos_tag_id'][masked_index[0][0].item()] == 1:\n",
    "            content_word = content_word_data['noun']\n",
    "            pos_tag = 'noun'\n",
    "            \n",
    "        elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 2:\n",
    "            content_word = content_word_data['verb']\n",
    "            pos_tag = 'verb'\n",
    "        \n",
    "        elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 3:\n",
    "            content_word = content_word_data['adj']\n",
    "            pos_tag = 'adj'\n",
    "        \n",
    "        elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 4:\n",
    "            content_word = content_word_data['adv']\n",
    "            pos_tag = 'adv'\n",
    "        else: \n",
    "            KeyError('pos_tag_id not in [1, 2, 3, 4]')\n",
    "        \n",
    "        replace_word_neg_cos, replace_word_neg_cos_module, replace_word_0_cos, replace_word_0_cos_module = cosine_sim_sen_list_word(sum_tensor, avg_tensor, content_word)\n",
    "\n",
    "        # cosine = -1\n",
    "        origin_word = TOKENIZER.decode(predicate_np[masked_index[0]])\n",
    "        # origin_sen = TOKENIZER.decode(predicate_np, skip_special_tokens = True)\n",
    "        # cos_neg_sen_sum = origin_sen.replace(origin_word, TOKENIZER.decode(replace_word_cosine_neg_sum), 1) \n",
    "        cos_neg = TOKENIZER.decode(replace_word_neg_cos)\n",
    "        cos_module_neg = TOKENIZER.decode(replace_word_neg_cos_module)\n",
    "        \n",
    "        # cosine = 0\n",
    "        cos_0 = TOKENIZER.decode(replace_word_0_cos)\n",
    "        cos_module_0 = TOKENIZER.decode(replace_word_0_cos_module)\n",
    "        \n",
    "        feature =  {\n",
    "                \"origin_uid\": predicate_data['origin_uid'],\n",
    "                \"origin_id\": predicate_data['origin_id'].tolist(), \n",
    "                \"masked_index\": masked_index[0].tolist(),\n",
    "                \"masked_word\": origin_word,\n",
    "                \"cos_neg\": cos_neg, \n",
    "                \"cos_0\": cos_0, \n",
    "                \"cos_module_neg\": cos_module_neg,\n",
    "                \"cos_module_0\": cos_module_0,\n",
    "                \"pos_tag\": pos_tag}\n",
    "        return feature         \n",
    "                \n",
    "    # list_feature = map(lambda x: new_word_in_sen(x), tqdm(predicate_data_chunk))\n",
    "    list_feature =  new_word_in_sen(predicate_data_chunk[0])          \n",
    "    with open(name, 'w') as wf:\n",
    "        for feature in list_feature:\n",
    "            wf.write('{}\\n'.format(json.dumps(feature))) \n",
    "        tempList.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_data(predicate_file, dict_content_word, wriDir):\n",
    "    '''\n",
    "    return 4 data cho moi file:\n",
    "        sum_vector va cosine = -1: uid, origin_input_id, sum_neg_input_id, pos_tag_id\n",
    "        sum_vector va cosine = 0: uid, origin_input_id, sum_0_input_id, pos_tag_id\n",
    "        avg_vector va cosine = -1: uid, origin_input_id, avg_neg_input_id, pos_tag_id\n",
    "        avg_vector va cosine = 0: uid, origin_input_id, avg_0_input_id, pos_tag_id\n",
    "    \n",
    "    '''\n",
    "    print(\"Preprocessing file... \", predicate_file)\n",
    "    predicates_data = read_data(predicate_file)[:10]\n",
    "    # predicates_data = [item for sublist in predicates_data for item in sublist][:10]\n",
    "    \n",
    "    # MULTI PROCESSING\n",
    "    man = mp.Manager()\n",
    "\n",
    "    # shared list to store all temp files written by processes\n",
    "    tempFilesList = man.list()\n",
    "    \n",
    "    # numProcess = mp.cpu_count() - 1\n",
    "    numProcess = 1\n",
    "    \n",
    "    \n",
    "    chunkSize = int(len(predicates_data) / (numProcess))\n",
    "    print('Data Size: ', len(predicates_data))\n",
    "    print('number of threads: ', numProcess)\n",
    "  \n",
    "    processes = []\n",
    "    for i in range(numProcess):\n",
    "        dataChunk = predicates_data[chunkSize*i : chunkSize*(i+1)]\n",
    "\n",
    "        p = mp.Process(target = replace_word_in_sentence, args = (dataChunk, i, tempFilesList, dict_content_word))\n",
    "        \n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        \n",
    "    for pr in processes:\n",
    "        pr.join()\n",
    "    \n",
    "    wrtPath = wriDir + '{}'.format(predicate_file.split('/')[-1].replace('mlm_', ''))\n",
    "    \n",
    "    # combining the files written by multiple processes into a single final file\n",
    "    with open(wrtPath, 'w') as f:\n",
    "        for file in tempFilesList:\n",
    "            with open(file, 'r') as r:\n",
    "                for line in r:\n",
    "                    sample =  json.loads(line)\n",
    "                    f.write('{}\\n'.format(json.dumps(sample)))\n",
    "            os.remove(file)\n",
    "        \n",
    "    print(\"Done file\", predicate_file)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## khong chay nua\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_paths = {\n",
    "    \"noun\": \"./data_mlm/process_folder/list_content_word_v2/NOUN.json\",\n",
    "    \"verb\": \"./data_mlm/process_folder/list_content_word_v2/VERB.json\",\n",
    "    \"adj\": \"./data_mlm/process_folder/list_content_word_v2/ADJ.json\",\n",
    "    \"adv\": \"./data_mlm/process_folder/list_content_word_v2/ADV.json\"\n",
    "}\n",
    "\n",
    "\n",
    "dict_content_word = {key: read_data(file_path) for key, file_path in file_paths.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42549, 22676, 13854, 4537)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_content_word['noun']), len(dict_content_word['verb']), len(dict_content_word['adj']), len(dict_content_word['adv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chay tiep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-05-14 15:11:12.600124: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-14 15:11:13.237953: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-14 15:11:15.725031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-14 15:11:18.315892: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mlm_utils.transform_func import get_files\n",
    "# wriDir = './pertured_data/'\n",
    "wriDir = './data_mlm/process_folder'\n",
    "dataDir = './data_mlm/process_folder/word_present_each_file/'\n",
    "files = get_files(dataDir)\n",
    "\n",
    "# for file in files:\n",
    "    \n",
    "# create_new_data(dataDir + files[0], dict_content_word, wriDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_word_in_sen(predicate_data, content_word_data):\n",
    "       \n",
    "    # masked_index = torch.where(torch.tensor(predicate_data['pos_tag_id']).clone().detach() != 0)\n",
    "    masked_index = np.where(np.array(predicate_data['pos_tag_id']) != 0)\n",
    "    print(\"masked index\", masked_index)\n",
    "    predicate_np = np.array(predicate_data['origin_id'])\n",
    "    \n",
    "    avg_tensor = predicate_data['avg_vector']\n",
    "    sum_tensor = predicate_data['sum_vector']\n",
    "    \n",
    "    content_word = None\n",
    "    pos_tag = None\n",
    "    if predicate_data['pos_tag_id'][masked_index[0][0].item()] == 1:\n",
    "        content_word = content_word_data['noun']\n",
    "        pos_tag = 'noun'\n",
    "        \n",
    "    elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 2:\n",
    "        content_word = content_word_data['verb']\n",
    "        pos_tag = 'verb'\n",
    "    \n",
    "    elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 3:\n",
    "        content_word = content_word_data['adj']\n",
    "        pos_tag = 'adj'\n",
    "    \n",
    "    elif predicate_data['pos_tag_id'][masked_index[0][0].item()] == 4:\n",
    "        content_word = content_word_data['adv']\n",
    "        pos_tag = 'adv'\n",
    "    else: \n",
    "        KeyError('pos_tag_id not in [1, 2, 3, 4]')\n",
    "    \n",
    "    replace_word_neg_cos, replace_word_neg_cos_module, replace_word_0_cos, replace_word_0_cos_module = cosine_sim_sen_list_word(sum_tensor, avg_tensor, content_word)\n",
    "\n",
    "    # cosine = -1\n",
    "    origin_word = TOKENIZER.decode(predicate_np[masked_index[0]])\n",
    "    # origin_sen = TOKENIZER.decode(predicate_np, skip_special_tokens = True)\n",
    "    # cos_neg_sen_sum = origin_sen.replace(origin_word, TOKENIZER.decode(replace_word_cosine_neg_sum), 1) \n",
    "    cos_neg = TOKENIZER.decode(replace_word_neg_cos)\n",
    "    cos_module_neg = TOKENIZER.decode(replace_word_neg_cos_module)\n",
    "    \n",
    "    # cosine = 0\n",
    "    cos_0 = TOKENIZER.decode(replace_word_0_cos)\n",
    "    cos_module_0 = TOKENIZER.decode(replace_word_0_cos_module)\n",
    "    \n",
    "    feature =  {\n",
    "            \"origin_uid\": predicate_data['origin_uid'],\n",
    "            \"origin_id\": predicate_data['origin_id'], \n",
    "            \"masked_index\": masked_index[0].tolist(),\n",
    "            \"masked_word\": origin_word,\n",
    "            \"cos_neg\": cos_neg, \n",
    "            \"cos_0\": cos_0, \n",
    "            \"cos_module_neg\": cos_module_neg,\n",
    "            \"cos_module_0\": cos_module_0,\n",
    "            \"pos_tag\": pos_tag}\n",
    "    return feature    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_uid</th>\n",
       "      <th>origin_id</th>\n",
       "      <th>pos_tag_id</th>\n",
       "      <th>word</th>\n",
       "      <th>sum_vector</th>\n",
       "      <th>avg_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 170, 176, 118, 1106, 118, 170, 6468, 112...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[6468]</td>\n",
       "      <td>[-0.582668125629425, 0.7431492805480957, -0.67...</td>\n",
       "      <td>[-0.582668125629425, 0.7431492805480957, -0.67...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 170, 176, 118, 1106, 118, 170, 6468, 112...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1148]</td>\n",
       "      <td>[-0.5867326855659485, 1.362990140914917, -0.44...</td>\n",
       "      <td>[-0.5867326855659485, 1.362990140914917, -0.44...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 170, 176, 118, 1106, 118, 170, 6468, 112...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>[183, 21977, 26918, 23767]</td>\n",
       "      <td>[-2.1963400840759277, 4.488198757171631, -4.05...</td>\n",
       "      <td>[-0.5490850210189819, 1.1220496892929077, -1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 170, 176, 118, 1106, 118, 170, 6468, 112...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[27553, 1179]</td>\n",
       "      <td>[-0.9821252822875977, 1.8156334161758423, -2.4...</td>\n",
       "      <td>[-0.49106264114379883, 0.9078167080879211, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 170, 176, 118, 1106, 118, 170, 6468, 112...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5351]</td>\n",
       "      <td>[-0.13667286932468414, 0.5582464933395386, -0....</td>\n",
       "      <td>[-0.13667286932468414, 0.5582464933395386, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin_uid                                          origin_id  \\\n",
       "0           0  [101, 170, 176, 118, 1106, 118, 170, 6468, 112...   \n",
       "1           0  [101, 170, 176, 118, 1106, 118, 170, 6468, 112...   \n",
       "2           0  [101, 170, 176, 118, 1106, 118, 170, 6468, 112...   \n",
       "3           0  [101, 170, 176, 118, 1106, 118, 170, 6468, 112...   \n",
       "4           0  [101, 170, 176, 118, 1106, 118, 170, 6468, 112...   \n",
       "\n",
       "                                          pos_tag_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                         word  \\\n",
       "0                      [6468]   \n",
       "1                      [1148]   \n",
       "2  [183, 21977, 26918, 23767]   \n",
       "3               [27553, 1179]   \n",
       "4                      [5351]   \n",
       "\n",
       "                                          sum_vector  \\\n",
       "0  [-0.582668125629425, 0.7431492805480957, -0.67...   \n",
       "1  [-0.5867326855659485, 1.362990140914917, -0.44...   \n",
       "2  [-2.1963400840759277, 4.488198757171631, -4.05...   \n",
       "3  [-0.9821252822875977, 1.8156334161758423, -2.4...   \n",
       "4  [-0.13667286932468414, 0.5582464933395386, -0....   \n",
       "\n",
       "                                          avg_vector  \n",
       "0  [-0.582668125629425, 0.7431492805480957, -0.67...  \n",
       "1  [-0.5867326855659485, 1.362990140914917, -0.44...  \n",
       "2  [-0.5490850210189819, 1.1220496892929077, -1.0...  \n",
       "3  [-0.49106264114379883, 0.9078167080879211, -1....  \n",
       "4  [-0.13667286932468414, 0.5582464933395386, -0....  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates_data = read_data(dataDir + files[0])[:10]\n",
    "# convert to dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(predicates_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_uid</th>\n",
       "      <th>origin_id</th>\n",
       "      <th>pos_tag_id</th>\n",
       "      <th>word</th>\n",
       "      <th>sum_vector</th>\n",
       "      <th>avg_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 170, 176, 118, 1106, 118, 170, 6468, 112...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[6468]</td>\n",
       "      <td>[-0.582668125629425, 0.7431492805480957, -0.67...</td>\n",
       "      <td>[-0.582668125629425, 0.7431492805480957, -0.67...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 170, 176, 118, 1106, 118, 170, 6468, 112...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1148]</td>\n",
       "      <td>[-0.5867326855659485, 1.362990140914917, -0.44...</td>\n",
       "      <td>[-0.5867326855659485, 1.362990140914917, -0.44...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 170, 176, 118, 1106, 118, 170, 6468, 112...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>[183, 21977, 26918, 23767]</td>\n",
       "      <td>[-2.1963400840759277, 4.488198757171631, -4.05...</td>\n",
       "      <td>[-0.5490850210189819, 1.1220496892929077, -1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 170, 176, 118, 1106, 118, 170, 6468, 112...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[27553, 1179]</td>\n",
       "      <td>[-0.9821252822875977, 1.8156334161758423, -2.4...</td>\n",
       "      <td>[-0.49106264114379883, 0.9078167080879211, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[101, 170, 176, 118, 1106, 118, 170, 6468, 112...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5351]</td>\n",
       "      <td>[-0.13667286932468414, 0.5582464933395386, -0....</td>\n",
       "      <td>[-0.13667286932468414, 0.5582464933395386, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin_uid                                          origin_id  \\\n",
       "0           0  [101, 170, 176, 118, 1106, 118, 170, 6468, 112...   \n",
       "1           0  [101, 170, 176, 118, 1106, 118, 170, 6468, 112...   \n",
       "2           0  [101, 170, 176, 118, 1106, 118, 170, 6468, 112...   \n",
       "3           0  [101, 170, 176, 118, 1106, 118, 170, 6468, 112...   \n",
       "4           0  [101, 170, 176, 118, 1106, 118, 170, 6468, 112...   \n",
       "\n",
       "                                          pos_tag_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                         word  \\\n",
       "0                      [6468]   \n",
       "1                      [1148]   \n",
       "2  [183, 21977, 26918, 23767]   \n",
       "3               [27553, 1179]   \n",
       "4                      [5351]   \n",
       "\n",
       "                                          sum_vector  \\\n",
       "0  [-0.582668125629425, 0.7431492805480957, -0.67...   \n",
       "1  [-0.5867326855659485, 1.362990140914917, -0.44...   \n",
       "2  [-2.1963400840759277, 4.488198757171631, -4.05...   \n",
       "3  [-0.9821252822875977, 1.8156334161758423, -2.4...   \n",
       "4  [-0.13667286932468414, 0.5582464933395386, -0....   \n",
       "\n",
       "                                          avg_vector  \n",
       "0  [-0.582668125629425, 0.7431492805480957, -0.67...  \n",
       "1  [-0.5867326855659485, 1.362990140914917, -0.44...  \n",
       "2  [-0.5490850210189819, 1.1220496892929077, -1.0...  \n",
       "3  [-0.49106264114379883, 0.9078167080879211, -1....  \n",
       "4  [-0.13667286932468414, 0.5582464933395386, -0....  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noun = read_data(\"/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/data_mlm/process_folder/word_present_each_file/mlm_abolish_full.json\")\n",
    "\n",
    "df_noun.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
