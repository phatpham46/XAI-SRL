{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_csv(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/list_content_word_v2/ADV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def apply_eval(data):\n",
    "    \"\"\"\n",
    "    Apply eval function to all elements in a list.\n",
    "    \n",
    "    Args:\n",
    "    - data (list): List of strings\n",
    "    \n",
    "    Returns:\n",
    "    - list: List with elements evaluated\n",
    "    \"\"\"\n",
    "    # Define required functions and modules\n",
    "    eval_locals = {'tensor': torch.tensor, 'torch': torch}\n",
    "    \n",
    "    return [eval(item, eval_locals) for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_dict(data):\n",
    "    list_data_dict = []\n",
    "    \n",
    "    for i in range(1, len(data)):\n",
    "        items = apply_eval(data[i])\n",
    "        f = {\"uid\": items[0], \"word_vector\": items[1].clone().detach(), \"sum_vector\": items[2].clone().detach(), \"avg_vector\": items[3].clone().detach()}\n",
    "        list_data_dict.append(f)\n",
    "    return list_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = convert_to_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4489"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read json file and convert to dict\n",
    "import json\n",
    "\n",
    "import json\n",
    "import torch\n",
    "\n",
    "def read_data(readPath):\n",
    "\n",
    "    with open(readPath, 'r', encoding = 'utf-8') as file:\n",
    "        taskData = []\n",
    "        for i, line in enumerate(file):\n",
    "            sample = json.loads(line)\n",
    "            taskData.append(sample)\n",
    "\n",
    "    return taskData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = read_data('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/word_present_each_file/mlm_abolish_full.json')\n",
    "\n",
    "# convert 2-d list to 1-d list\n",
    "file_data =  [item for sublist in file_data for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(file_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_id', 'pos_tag_id', 'word', 'sum_vector', 'avg_vector'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate cosine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "def cosine_sim_sen_list_word(word_dict, list_dict_content_word):\n",
    "    # tinh cosine trong 1 cau voi tat ca cac content word\n",
    "    # word_vector: vector cua 1 tu trong file dong tu\n",
    "    # list_content_word: list cac tu content word\n",
    "   \n",
    "    result_dict = {\"sum_cosine_neg\": 0, \"word_neg_sum\": torch.tensor([]), \n",
    "                   \"sum_cosine_0\": 1, \"word_0_sum\": torch.tensor([]), \n",
    "                   \"avg_cosine_neg\": 0, \"word_neg_avg\":  torch.tensor([]), \n",
    "                   \"avg_cosine_0\": 1, \"word_0_avg\":  torch.tensor([])}\n",
    "   \n",
    "    min_cosine_neg_sum = 0\n",
    "    min_cosine_0_sum = 1\n",
    "    min_cosine_neg_avg = 0\n",
    "    min_cosine_0_avg = 1\n",
    "    \n",
    "    for content_word in list_dict_content_word:\n",
    "       \n",
    "        # SUM VECTOR \n",
    "        sum_tensor = torch.tensor(word_dict['sum_vector']).clone().detach().unsqueeze(0)\n",
    "        sum_content_tensor = torch.tensor(content_word['sum_vector']).clone().detach().unsqueeze(0)\n",
    "        cosine_val_sum = cosine_similarity(sum_tensor, sum_content_tensor)\n",
    "        if cosine_val_sum <= min_cosine_neg_sum:\n",
    "            min_cosine_neg_sum = cosine_val_sum\n",
    "            result_dict['word_neg_sum'] = content_word\n",
    "            \n",
    "        elif cosine_val_sum <= min_cosine_0_sum:\n",
    "            min_cosine_0_sum = cosine_val_sum\n",
    "            result_dict['word_0_sum'] =  content_word\n",
    "         \n",
    "         \n",
    "        # AVG VECTOR\n",
    "        avg_tensor = torch.tensor(word_dict['avg_vector']).clone().detach().unsqueeze(0)\n",
    "        content_word_tensor = torch.tensor(content_word['avg_vector']).clone().detach().unsqueeze(0)\n",
    "        cosine_val_avg = cosine_similarity(avg_tensor, content_word_tensor)\n",
    "        \n",
    "        if cosine_val_avg <= min_cosine_neg_avg:\n",
    "            min_cosine_neg_avg = cosine_val_avg\n",
    "            result_dict['word_neg_avg'] = content_word\n",
    "        elif cosine_val_avg <= min_cosine_0_avg:\n",
    "            min_cosine_0_avg = cosine_val_avg\n",
    "            result_dict['word_0_avg'] =  content_word\n",
    "         \n",
    "    result_dict['sum_cosine_neg'] = min_cosine_neg_sum  \n",
    "    result_dict['sum_cosine_0'] = min_cosine_0_sum\n",
    "    return result_dict\n",
    "            \n",
    "def create_new_data_neg_sum(predicate_file, content_word_data, wriDir):\n",
    "    '''\n",
    "    return 4 data cho moi file:\n",
    "        sum_vector va cosine = -1: uid, origin_input_id, sum_neg_input_id, pos_tag_id\n",
    "        sum_vector va cosine = 0: uid, origin_input_id, sum_0_input_id, pos_tag_id\n",
    "        avg_vector va cosine = -1: uid, origin_input_id, avg_neg_input_id, pos_tag_id\n",
    "        avg_vector va cosine = 0: uid, origin_input_id, avg_0_input_id, pos_tag_id\n",
    "    \n",
    "    '''\n",
    "    predicate_data = read_data(predicate_file)\n",
    "    predicate_data =  [item for sublist in predicate_data for item in sublist]\n",
    "    result_dict = {}\n",
    "    for i in range(len(predicate_data)):\n",
    "        \n",
    "        # chua xu li cai pos tag de truyen vao content_word_data cho dung tung loai tu \n",
    "        \n",
    "        masked_index = torch.where(torch.tensor(predicate_data[i]['pos_tag_id']).clone().detach() != 0)\n",
    "     \n",
    "        if predicate_data[i]['pos_tag_id'][masked_index[0][0].item()] == 4:\n",
    "            result_dict = cosine_sim_sen_list_word(predicate_data[i], content_word_data)\n",
    "           \n",
    "        # sentence (cau trong moi file) : dict_keys(['input_id', 'pos_tag_id', 'word', 'sum_vector', 'avg_vector'])\n",
    "        # list content: uid,word,sum_vector,avg_vector\n",
    "        \n",
    "        # test trang tu thoi, mot if day du cho nay \n",
    "        # sum_vector va cosine = -1\n",
    "            if type(result_dict['word_neg_sum']) != torch.Tensor:\n",
    "                \n",
    "                new_word_neg_sum = result_dict['word_neg_sum']['word_vector']\n",
    "                predicate_data[i]['input_id'][masked_index[0].item()] = new_word_neg_sum\n",
    "            \n",
    "        \n",
    "    new_data =  [{key: value for key, value in d.items() if key not in ['word_vector', 'sum_vector', 'avg_vector']} for d in predicate_data]\n",
    "    # write to json file\n",
    "    with open(wriDir + 'neg_sum_{}'.format(predicate_file.replace('mlm_', '')), 'w') as file:\n",
    "        json.dump(new_data, file)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = '/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/word_present_each_file/mlm_abolish_full.json'\n",
    "wriDir = '/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/pertured_data'\n",
    "create_new_data_neg_sum(file_data, a, wriDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tuple = (1, 2, 3, 4, 5)\n",
    "iterator = iter(tuple)\n",
    "for i in iterator:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
