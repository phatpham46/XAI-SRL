{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tạo pertured data và lưu giống data cũ (cal_metric.py)\n",
    "    1. Tạo ra câu pertured va convert json và ghép label (pertured_folder/masked_data_json)\n",
    "### 2. Lấy ra vector logit \n",
    "    1. convert interim -> txt \n",
    "    2. transform, prepare -> json \n",
    "    3. đưa json (masked data 0.1) vào model lấy logit, prediction -> cần tạo class để lấy cho cả origin data và masked data\n",
    "### 3. convert label gold theo từ thành label gold theo token\n",
    "### 4. Tính inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from mlm_utils.model_utils import TOKENIZER\n",
    "from mlm_utils.transform_func import get_files, encode_text\n",
    "from mlm_utils.pertured_dataset import PerturedDataset\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-05-18 00:44:03.393636: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-18 00:44:03.488413: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-18 00:44:05.390236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-18 00:44:06.879489: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from mlm_utils.model_utils import TOKENIZER\n",
    "from mlm_utils.transform_func import get_files, encode_text\n",
    "from mlm_utils.pertured_dataset import PerturedDataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_params(model_file):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Load finetuned model\n",
    "    loadedDict = torch.load(model_file, map_location=torch.device(device))\n",
    "\n",
    "    taskParams = loadedDict['task_params']\n",
    "   \n",
    "    allParams = {}\n",
    "    allParams['task_params'] = taskParams\n",
    "    allParams['gpu'] = torch.cuda.is_available()\n",
    "\n",
    "    # dummy values\n",
    "    allParams['num_train_steps'] = 10\n",
    "    allParams['warmup_steps'] = 0\n",
    "    allParams['learning_rate'] = 2e-5\n",
    "    allParams['epsilon'] = 1e-8\n",
    "\n",
    "    return allParams, loadedDict      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "sys.path.append('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/')\n",
    "from SRL.model import multiTaskModel\n",
    "model_file = Path('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/output/multi_task_model_9_13050.pt')\n",
    "allParams, loadedDict = load_params(model_file)\n",
    "model = multiTaskModel(allParams)\n",
    "model.load_multi_task_model(loadedDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_pred_score(dataloader, model):\n",
    "    allPreds = []\n",
    "    allScores = []\n",
    "    allLogitsSoftmax = []\n",
    "    allLogitsRaw = []\n",
    "    allLabels = []\n",
    "    model.network.eval()\n",
    "    for batch in tqdm(dataloader, total = len(dataloader)):\n",
    "        batch = tuple(t.to(device) if isinstance(t, torch.Tensor) else t for t in batch)\n",
    "      \n",
    "        origin_uid, origin_id, label, token_id, type_id, mask = batch\n",
    "        with torch.no_grad():\n",
    "            outputs_model, logits = model.network(token_id, type_id, mask, 0, 'conllsrl')\n",
    "           \n",
    "            outLogitsSoftmax = nn.functional.softmax(logits, dim = 2).data.cpu().numpy()\n",
    "            \n",
    "            outLogitsSigmoid = nn.functional.sigmoid(logits).data.cpu().numpy()\n",
    "            \n",
    "            predicted_sm = np.argmax(outLogitsSoftmax, axis = 2)\n",
    "            \n",
    "            # here in score, we only want to give out the score of the class of tag, which is maximum\n",
    "            predScore = np.max(outLogitsSigmoid, axis = 2).tolist() \n",
    "            \n",
    "            predicted_sm = predicted_sm.tolist()\n",
    "        \n",
    "            # get the attention masks, we need to discard the predictions made for extra padding\n",
    "            predictedTags = []\n",
    "            predScoreTags = []\n",
    "            \n",
    "            # if mask is not None:\n",
    "            #     #shape of attention Masks (batchSize, maxSeqLen)\n",
    "            #     actualLengths = mask.cpu().numpy().sum(axis = 1).tolist()\n",
    "            \n",
    "            #     for i, (pred, sc) in enumerate(zip(predicted_sm, predScore)):\n",
    "            #         predictedTags.append( pred[:actualLengths[i]] )\n",
    "            #         predScoreTags.append( sc[:actualLengths[i]])\n",
    "    \n",
    "            # else:\n",
    "            #     predictedTags = predicted_sm\n",
    "            #     predScoreTags = predScore\n",
    "            \n",
    "            # để nguyên kh cần lấy theo attention mask\n",
    "            allPreds.append(predicted_sm)  \n",
    "            allScores.append(predScore)  \n",
    "            allLogitsSoftmax.append(outLogitsSoftmax)\n",
    "            allLabels.append(label.tolist())\n",
    "            allLogitsRaw.append(logits.data.cpu().numpy())\n",
    "        break\n",
    "    # flatten allPreds, allScores\n",
    "    allPreds = [item for sublist in allPreds for item in sublist]\n",
    "    allScores = [item for sublist in allScores for item in sublist]\n",
    "    allLabels = [item for sublist in allLabels for item in sublist]\n",
    "    allLogitsSoftmax = [item for sublist in allLogitsSoftmax for item in sublist]\n",
    "    allLogitsRaw = [item for sublist in allLogitsRaw for item in sublist]\n",
    "    return allPreds, allScores, allLabels, allLogitsSoftmax, allLogitsRaw      \n",
    "\n",
    "      \n",
    "    \n",
    "def write_pred_label(allPreds, allScores, allLabels, allParams, wrtFile):\n",
    "    \n",
    "    allLabels = []\n",
    "    allIds = []\n",
    "    \n",
    "    labMap = allParams['task_params'].labelMap['conllsrl']\n",
    "    \n",
    "    labMapRevN = {v:k for k,v in labMap.items()}\n",
    "   \n",
    "    for j, (p, l) in enumerate(zip(allPreds, allLabels)):\n",
    "        allLabels[j] = l[:len(p)]\n",
    "        allPreds[j] = [labMapRevN[int(ele)] for ele in p]\n",
    "        allLabels[j] = [labMapRevN[int(ele)] for ele in allLabels[j]]\n",
    "    # allPreds[i] = [ [ labMapRev[int(p)] for p in pp ] for pp in allPreds[i] ]\n",
    "    # allLabels[i] = [ [labMapRev[int(l)] for l in ll] for ll in allLabels[i] ]\n",
    "\n",
    "    newPreds = []\n",
    "    newLabels = []\n",
    "    newScores = []\n",
    "    for m, samp in enumerate(allLabels):\n",
    "        Preds = []\n",
    "        Labels = []\n",
    "        Scores = []\n",
    "        for n, ele in enumerate(samp):\n",
    "            #print(ele)\n",
    "            if ele != '[CLS]' and ele != '[SEP]' and ele != 'X':\n",
    "                #print('inside')\n",
    "                Preds.append(allPreds[m][n])\n",
    "                Labels.append(ele)\n",
    "                Scores.append(allScores[m][n])\n",
    "                \n",
    "        newPreds.append(Preds)\n",
    "        newLabels.append(Labels)\n",
    "        newScores.append(Scores)\n",
    "    \n",
    "    allLabels = newLabels\n",
    "    allPreds = newPreds\n",
    "    allScores = newScores\n",
    "    df = pd.DataFrame({\"uid\" : allIds, \"prediction\" : allPreds, \"label\" : allLabels})\n",
    "    \n",
    "    df.to_csv(wrtFile, sep = \"\\t\", index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/49 [00:06<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# nếu số kh cao thì lấy folder masked_data_json_v2\n",
    "writeDir = Path('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/data_mlm/pertured_data/masked_data_json_v2')\n",
    "dataset = PerturedDataset(\n",
    "        file_name= os.path.join(writeDir,'pertured_data_alter_full.json'),\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "dataloader = dataset.generate_batches(\n",
    "            dataset= dataset,\n",
    "            batch_size=32)\n",
    "allPreds, allScores,allLabels, allLogitsSoftmax, allLogitsRaw  = get_pred_score(dataloader, model)\n",
    "wrtFile = \"/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/pertured_data_prediction.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "print(len(allPreds[0]))\n",
    "print(len(allLabels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'B-A1': 0,\n",
    " 'I-A1': 1,\n",
    " 'O': 2,\n",
    " 'B-V': 3,\n",
    " 'B-A0': 4,\n",
    " 'I-A0': 5,\n",
    " 'B-A4': 6,\n",
    " 'I-A4': 7,\n",
    " 'I-A3': 8,\n",
    " 'B-A2': 9,\n",
    " 'I-A2': 10,\n",
    " 'B-A3': 11,\n",
    " '[CLS]': 12,\n",
    " '[SEP]': 13,\n",
    " 'X': 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_arg_preds(preds_origin, preds_masked, label_origin=None): # label_origin: nhãn gold\n",
    "    list_idx_arg_change = []\n",
    "    assert len(preds_origin) == len(preds_masked), 'Length of preds_origin and preds_masked must be the same'\n",
    "    for i in range(len(preds_origin)):\n",
    "        test = preds_origin[i] not in [2, 3, 12, 13, 14] and preds_masked[i] not in [2, 3, 12, 13, 14]\n",
    "        if label_origin:\n",
    "            assert len(preds_origin) == len(label_origin), 'Length of preds_origin and label_origin must be the same'\n",
    "            if test or label_origin[i] not in [2, 3, 12, 13, 14]:\n",
    "                list_idx_arg_change.append(i)\n",
    "        else:\n",
    "            if test:\n",
    "                list_idx_arg_change.append(i)\n",
    "    return list_idx_arg_change\n",
    "\n",
    "def calculateInfluenceScore(outLogitsSigmoid_original, outLogitsSigmoid_meddle, list_arg_change):\n",
    "    influence_score = []\n",
    "    weight = []\n",
    "    assert len(outLogitsSigmoid_original) == len(outLogitsSigmoid_meddle) \n",
    "    for i in range(len(outLogitsSigmoid_original)):\n",
    "        if i not in list_arg_change:\n",
    "            continue\n",
    "        max_index_original = torch.argmax(outLogitsSigmoid_original[i])\n",
    "        max_index_meddle = torch.argmax(outLogitsSigmoid_meddle[i])\n",
    "        if max_index_original == max_index_meddle:\n",
    "            influence_score.append((outLogitsSigmoid_original[i][max_index_original] - outLogitsSigmoid_meddle[i][max_index_meddle]) / max(outLogitsSigmoid_original[i][max_index_original], outLogitsSigmoid_meddle[i][max_index_meddle]))\n",
    "            weight.append(1)\n",
    "        else:\n",
    "            influ_old_label = (outLogitsSigmoid_original[i][max_index_original] - outLogitsSigmoid_meddle[i][max_index_original]) / max(outLogitsSigmoid_original[i][max_index_original], outLogitsSigmoid_meddle[i][max_index_original])\n",
    "            influ_new_label = (outLogitsSigmoid_meddle[i][max_index_meddle] - outLogitsSigmoid_original[i][max_index_meddle]) / max(outLogitsSigmoid_original[i][max_index_meddle], outLogitsSigmoid_meddle[i][max_index_meddle])\n",
    "            influence_score.append(influ_old_label + influ_new_label)\n",
    "            weight.append(2)\n",
    "    return influence_score, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for i in range(len(resultwordOrigin['logits'])):\n",
    "    resultwordOrigin['logits'][i] = torch.tensor(resultwordOrigin['logits'][i])\n",
    "for i in range(len(resultwordMasked['logits'])):\n",
    "    resultwordMasked['logits'][i] = torch.tensor(resultwordMasked['logits'][i])\n",
    "    \n",
    "influenceScore, weight = calculateInfluenceScore(resultwordOrigin['logits'][2], resultwordMasked['logits'][3], get_idx_arg_preds(resultwordOrigin['preds'][2], resultwordMasked['preds'][3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Done\n",
    "# convert csv to coNLL format and write to txt file\n",
    "!python ../data_transformations.py --transform_file '/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/SRL/transform_file_mlm.yml'\n",
    "# output file MLM/coNLL_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Done\n",
    "!python ../data_transformations.py --transform_file '/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/SRL/transform_file_conll.yml'\n",
    "# nhớ chỉnh lại đường dẫn trong file transform_file_conll.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Done\n",
    "# !python ../data_preparation.py --task_file tasks_file_SRL.yml --data_dir ../data/coNLL_tsv --max_seq_len 50\n",
    "\n",
    "!python ../data_preparation.py \\\n",
    "        --task_file \"/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/SRL/tasks_file_SRL.yml\" \\\n",
    "        --data_dir ../MLM/coNLL_tsv_json \\\n",
    "        --max_seq_len 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
