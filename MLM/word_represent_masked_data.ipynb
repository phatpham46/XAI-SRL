{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tạo pertured data và lưu giống data cũ (cal_metric.py)\n",
    "    1. Tạo ra câu pertured va convert json (pertured_folder/masked_data_json)\n",
    "### 2. Lấy ra vector logit \n",
    "    1. convert interim -> txt \n",
    "    2. transform, prepare -> json \n",
    "    3. ghép label (từ 2.2) của câu gốc với câu mask\n",
    "    4. đưa json (masked data 0.1) vào model lấy logit, prediction \n",
    "### 3. lấy được index của chỗ arg bị thay đổi của dự đoán sau khi masked data \n",
    "### 4. Tính inf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from mlm_utils.model_utils import TOKENIZER\n",
    "from mlm_utils.transform_func import get_files, encode_text\n",
    "from mlm_utils.pertured_dataset import PerturedDataset\n",
    "from pathlib import Path\n",
    "# sys.path.append('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/')\n",
    "# from SRL.model import multiTaskModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(readPath):\n",
    "    \n",
    "    with open(readPath, 'r', encoding = 'utf-8') as file:\n",
    "        taskData = []\n",
    "        for i, line in enumerate(file):\n",
    "            sample = json.loads(line)\n",
    "            taskData.append(sample)\n",
    "            \n",
    "    return taskData\n",
    "\n",
    "def generate_data_and_write_json(dataDir, wriDir, lableDir):\n",
    "    \n",
    "    def gen_data_line(line):\n",
    "        \n",
    "        origin_sen = TOKENIZER.decode(torch.tensor(line['origin_id']), skip_special_tokens = True)\n",
    "\n",
    "        new_word_list_set = set([line['cos_neg'], line['cos_0'], line['cos_module_neg'], line['cos_module_0']])\n",
    "\n",
    "        masked_data_set = list(map(lambda x: origin_sen.replace(line['masked_word'], x, 1), new_word_list_set))\n",
    "        return line['origin_id'], str(masked_data_set)\n",
    "    \n",
    "    files_data = get_files(dataDir)\n",
    "    files_label = get_files(lableDir)\n",
    "    \n",
    "    \n",
    "    for data_f, label_f in tqdm(zip(files_data, files_label)):\n",
    "        if data_f == 'catalyse_full.json':\n",
    "            data = read_data(dataDir + data_f)\n",
    "            label_data = read_data(lableDir + label_f)\n",
    "        \n",
    "            masked_data_file = list(map(lambda x: gen_data_line(x), data))\n",
    "\n",
    "            tmp_list = []\n",
    "            # lay label giua pertured data voi coNLL_tsv_json      \n",
    "            print(\"Process file\", data_f)\n",
    "            for idx, (origin_id, pertured_text) in enumerate(masked_data_file):\n",
    "                flat = 0\n",
    "                for line in label_data:\n",
    "                    if list(filter(None, origin_id)) == list(filter(None, line['token_id'])):\n",
    "                        flat = 1\n",
    "                        tmp_list.append((origin_id, pertured_text, line['label']))\n",
    "                    else: \n",
    "                        continue\n",
    "                if flat == 0:\n",
    "                    print(\"origin id\", origin_id)\n",
    "                    print(\"TOKENIZER\", TOKENIZER.decode(torch.tensor(origin_id), skip_special_tokens = True))\n",
    "                    break\n",
    "                \n",
    "            write_file = \"pertured_data_{}\".format(data_f)\n",
    "\n",
    "            with open(os.path.join(wriDir, write_file) , 'w') as f:\n",
    "                for idx, (origin_id, pertured_text, label) in enumerate(tmp_list):\n",
    "                    encoded_text = encode_text(pertured_text)\n",
    "                    sample = {\n",
    "                        'uid': idx,\n",
    "                        'origin_id': origin_id,\n",
    "                        'label': label,\n",
    "                        'token_id': encoded_text['input_ids'][0].tolist(),\n",
    "                        'type_id': encoded_text['token_type_ids'][0].tolist(),\n",
    "                        'mask': encoded_text['attention_mask'][0].tolist()\n",
    "                    }\n",
    "                    f.write('{}\\n'.format(json.dumps(sample)))\n",
    "            print(\"Done\", data_f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file catalyse_full.json\n",
      "origin id [101, 173, 1605, 1119, 9538, 8830, 1132, 1682, 1106, 5855, 17449, 3171, 1103, 8362, 11129, 1158, 1104, 173, 1605, 1118, 26499, 1158, 1103, 9986, 22587, 1206, 13185, 7616, 1112, 1152, 5070, 1373, 1103, 173, 1605, 15947, 1107, 170, 15281, 4633, 113, 1719, 2588, 1137, 4389, 114, 117, 1105, 1138, 1151, 2602, 1106, 1129, 2017, 1107, 1242, 6818, 14391, 5669, 117, 1259, 25544, 117, 15416, 117, 173, 1605, 6949, 1105, 5179, 113, 1111, 3761, 1267, 1745, 117, 1820, 132, 5285, 3084, 2393, 119, 117, 1772, 102]\n",
      "TOKENIZER dna helicases are able to catalyze the unwinding of dna by disrupting the hydrogen bonding between paired bases as they progress along the dna strand in a polar fashion ( either 35 or 53 ), and have been shown to be involved in many essential cellular processes, including replication, transcription, dna repair and translation ( for reviews see west, 1996 ; bird et al., 1998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:00, 39.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done catalyse_full.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labelDir = './data_mlm/process_folder/coNLL_tsv_json/dmis-lab/biobert-base-cased-v1.2_prepared_data/'\n",
    "generate_data_and_write_json(dataDir = './data_mlm/process_folder/pertured_data/', wriDir = './data_mlm/pertured_folder/masked_data_json', lableDir=labelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "lisst = ['dna', 'helicases', 'are', 'able', 'to', 'catalyze', 'the', 'unwinding', 'of', 'dna', 'by', 'disrupting', 'the', 'hydrogen', 'bonding', 'between', 'paired', 'bases', 'as', 'they', 'progress', 'along', 'the', 'dna', 'strand', 'in', 'a', 'polar', 'fashion', '(', 'either', '35', 'or', '53', ')', ',', 'and', 'have', 'been', 'shown', 'to', 'be', 'involved', 'in', 'many', 'essential', 'cellular', 'processes', ',', 'including', 'replication', ',', 'transcription', ',', 'dna', 'repair', 'and', 'translation', '(', 'for', 'reviews', 'see', 'west', ',', '1996', ';', 'bird', 'et', 'al', '.', ',', '1998', ';', 'lohman', 'et', 'al', '.', ',', '1998', ';', 'hall', 'and', 'matson', ',', '1999', ')', '.']\n",
    "print(len(TOKENIZER.encode(\" \".join(lisst), add_special_tokens = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "print(len([101, 1649, 117, 1103, 17019, 3943, 1144, 1151, 5855, 17449, 5591, 1118, 1231, 1643, 1559, 1604, 1105, 1231, 1643, 1545, 1604, 1146, 1106, 6087, 118, 11203, 1750, 19723, 117, 1780, 1152, 185, 13159, 1279, 1103, 2072, 1231, 1643, 1571, 1477, 1105, 1231, 1643, 12882, 19350, 4001, 117, 3569, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "dna helicases are able to catalyze the unwinding of dna by disrupting the hydrogen bonding between paired bases as they progress along the dna strand in a polar fashion ( either 35 or 53 ), and have been shown to be involved in many essential cellular processes, including replication, transcription, dna repair and translation ( for reviews see west, 1996 ; bird et al., 1998\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([101, 173, 1605, 1119, 9538, 8830, 1132, 1682, 1106, 5855, 17449, 3171, 1103, 8362, 11129, 1158, 1104, 173, 1605, 1118, 26499, 1158, 1103, 9986, 22587, 1206, 13185, 7616, 1112, 1152, 5070, 1373, 1103, 173, 1605, 15947, 1107, 170, 15281, 4633, 113, 1719, 2588, 1137, 4389, 114, 117, 1105, 1138, 1151, 2602, 1106, 1129, 2017, 1107, 1242, 6818, 14391, 5669, 117, 1259, 25544, 117, 15416, 117, 173, 1605, 6949, 1105, 5179, 113, 1111, 3761, 1267, 1745, 117, 1820, 132, 5285, 3084, 2393, 119, 117, 1772, 102])\n",
    "print(len(t))\n",
    "print(TOKENIZER.decode(t, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writeDir = Path('./data_mlm/pertured_folder/masked_data_json')\n",
    "dataset = PerturedDataset(\n",
    "        data_path=writeDir,\n",
    "        file_name='pertured_data_abolish_full.json',\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "dataloader = dataset.generate_batches(\n",
    "            dataset= dataset,\n",
    "            batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_params(model_file):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Load finetuned model\n",
    "    loadedDict = torch.load(model_file, map_location=torch.device(device))\n",
    "\n",
    "    taskParams = loadedDict['task_params']\n",
    "    allParams = {}\n",
    "    allParams['task_params'] = taskParams\n",
    "    allParams['gpu'] = torch.cuda.is_available()\n",
    "\n",
    "    # dummy values\n",
    "    allParams['num_train_steps'] = 10\n",
    "    allParams['warmup_steps'] = 0\n",
    "    allParams['learning_rate'] = 2e-5\n",
    "    allParams['epsilon'] = 1e-8\n",
    "\n",
    "    return allParams, loadedDict      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "model_file = Path('/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/output/multi_task_model_9_13050.pt')\n",
    "allParams, loadedDict = load_params(model_file)\n",
    "model = multiTaskModel(allParams)\n",
    "model.load_multi_task_model(loadedDict)\n",
    "predicate_file = 'mlm_abolish_full.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_pred_score(dataloader, model):\n",
    "    allPreds = []\n",
    "    allScores = []\n",
    "    allLogits = []\n",
    "    for batch in tqdm(dataloader, total = len(dataloader)):\n",
    "        batch = tuple(t.to(device) if isinstance(t, torch.Tensor) else t for t in batch)\n",
    "\n",
    "        uid, origin_id, token_id, type_id, mask = batch\n",
    "        with torch.no_grad():\n",
    "            outputs_model, logits = model.network(token_id, type_id, mask, 0, 'conllsrl')\n",
    "            \n",
    "            outLogitsSoftmax = nn.functional.softmax(logits, dim = 2).data.cpu().numpy()\n",
    "            outLogitsSigmoid = nn.functional.sigmoid(logits).data.cpu().numpy()\n",
    "            #shape of logits now (batchSize, maxSeqLen, classNum: 15)\n",
    "            \n",
    "            predicted = np.argmax(outLogitsSoftmax, axis = 2)\n",
    "            # here in score, we only want to give out the score of the class of tag, which is maximum\n",
    "            predScore = np.max(outLogitsSigmoid, axis = 2).tolist()\n",
    "            \n",
    "            #shape of predicted now (batchSize, maxSeqLen)\n",
    "            # logger.debug(\"Final Predictions shape after argmx: {}\".format(predicted.shape))\n",
    "            predicted = predicted.tolist()\n",
    "        \n",
    "            # get the attention masks, we need to discard the predictions made for extra padding\n",
    "            \n",
    "            predictedTags = []\n",
    "            predScoreTags = []\n",
    "            \n",
    "            if mask is not None:\n",
    "                #shape of attention Masks (batchSize, maxSeqLen)\n",
    "                actualLengths = mask.cpu().numpy().sum(axis = 1).tolist()\n",
    "            \n",
    "                for i, (pred, sc) in enumerate(zip(predicted, predScore)):\n",
    "                    predictedTags.append( pred[:actualLengths[i]] )\n",
    "                    predScoreTags.append( sc[:actualLengths[i]])\n",
    "    \n",
    "            else:\n",
    "                predictedTags = predicted\n",
    "                predScoreTags = predScore\n",
    "            \n",
    "            allPreds.append(predictedTags)  \n",
    "            allScores.append(predScoreTags)  \n",
    "            allLogits.append(outLogitsSoftmax)\n",
    "    # flatten allPreds, allScores\n",
    "    allPreds = [item for sublist in allPreds for item in sublist]\n",
    "    allScores = [item for sublist in allScores for item in sublist]\n",
    "    return allPreds, allScores      \n",
    "\n",
    "\n",
    "# def get_origin_labels(labelDir, perturedDir):\n",
    "#     # lấy label thông qua origin_id của coNLL_tsv_json voi masked_data_json\n",
    "#     label_file = '/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/data_mlm/process_folder/coNLL_tsv_json/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_format_abolish_full.json'\n",
    "#     perture_file = '/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/data_mlm/pertured_folder/masked_data_json/pertured_data_abolish_full.json'\n",
    "    \n",
    "#     with open(label_file, 'w') as label_f:\n",
    "        \n",
    "    \n",
    "def write_pred_label(allPreds, allScores, labelDir, allParams, wrtFile):\n",
    "    predicate_file = 'mlm_abolish_full.json'\n",
    "    \n",
    "    allLabels = []\n",
    "    allIds = []\n",
    "    \n",
    "    \n",
    "    with open(labelDir + predicate_file, 'r') as label_f:\n",
    "        for i, line in enumerate(label_f):\n",
    "            sample = json.loads(line)   \n",
    "            allLabels.append(sample['label'])\n",
    "            allIds.append(sample['uid'])\n",
    "    labMap = allParams['task_params'].labelMap['conllsrl']\n",
    "    \n",
    "    labMapRevN = {v:k for k,v in labMap.items()}\n",
    "\n",
    "    for j, (p, l) in enumerate(zip(allPreds[i], allLabels[i])):\n",
    "        allLabels[i][j] = l[:len(p)]\n",
    "        allPreds[i][j] = [labMapRevN[int(ele)] for ele in p]\n",
    "        allLabels[i][j] = [labMapRevN[int(ele)] for ele in allLabels[i][j]]\n",
    "    #allPreds[i] = [ [ labMapRev[int(p)] for p in pp ] for pp in allPreds[i] ]\n",
    "    #allLabels[i] = [ [labMapRev[int(l)] for l in ll] for ll in allLabels[i] ]\n",
    "\n",
    "    newPreds = []\n",
    "    newLabels = []\n",
    "    newScores = []\n",
    "    for m, samp in enumerate(allLabels[i]):\n",
    "        Preds = []\n",
    "        Labels = []\n",
    "        Scores = []\n",
    "        for n, ele in enumerate(samp):\n",
    "            #print(ele)\n",
    "            if ele != '[CLS]' and ele != '[SEP]' and ele != 'X':\n",
    "                #print('inside')\n",
    "                Preds.append(allPreds[i][m][n])\n",
    "                Labels.append(ele)\n",
    "                Scores.append(allScores[i][m][n])\n",
    "                \n",
    "        newPreds.append(Preds)\n",
    "        newLabels.append(Labels)\n",
    "        newScores.append(Scores)\n",
    "    \n",
    "    allLabels[i] = newLabels\n",
    "    allPreds[i] = newPreds\n",
    "    allScores[i] = newScores\n",
    "    for i in range(len(allPreds)):\n",
    "        if allPreds[i] == []:\n",
    "            continue\n",
    "       \n",
    "        df = pd.DataFrame({\"uid\" : allIds[i], \"prediction\" : allPreds[i], \"label\" : allLabels[i]})\n",
    "        \n",
    "        df.to_csv(wrtFile, sep = \"\\t\", index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:01<00:00,  3.79s/it]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m allPreds, allScores \u001b[38;5;241m=\u001b[39m get_pred_score(dataloader, model)\n\u001b[1;32m      3\u001b[0m wrtFile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/pertured_data_prediction.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mwrite_pred_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallPreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallScores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabelDir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrtFile\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [10], line 57\u001b[0m, in \u001b[0;36mwrite_pred_label\u001b[0;34m(allPreds, allScores, labelDir, allParams, wrtFile)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(label_f):\n\u001b[1;32m     56\u001b[0m         sample \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)   \n\u001b[0;32m---> 57\u001b[0m         allLabels\u001b[38;5;241m.\u001b[39mappend(\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     58\u001b[0m         allIds\u001b[38;5;241m.\u001b[39mappend(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     59\u001b[0m labMap \u001b[38;5;241m=\u001b[39m allParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask_params\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlabelMap[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconllsrl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "# labelDir ='../MLM/data_mlm/process_folder/mlm_output/'\n",
    "labelDir = '../MLM/data_mlm/process_folder/coNLL_tsv_json/dmis-lab/biobert-base-cased-v1.2_prepared_data/ner_conll_format_abolish_full.json'\n",
    "allPreds, allScores = get_pred_score(dataloader, model)\n",
    "wrtFile = \"/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/pertured_data_prediction.json\"\n",
    "write_pred_label(allPreds, allScores, labelDir, allParams, wrtFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Done\n",
    "# convert csv to coNLL format and write to txt file\n",
    "!python ../data_transformations.py --transform_file '/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/SRL/transform_file_mlm.yml'\n",
    "# output file MLM/coNLL_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Done\n",
    "!python ../data_transformations.py --transform_file '/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/SRL/transform_file_conll.yml'\n",
    "# nhớ chỉnh lại đường dẫn trong file transform_file_conll.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Done\n",
    "# !python ../data_preparation.py --task_file tasks_file_SRL.yml --data_dir ../data/coNLL_tsv --max_seq_len 50\n",
    "\n",
    "!python ../data_preparation.py \\\n",
    "        --task_file \"/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/SRL/tasks_file_SRL.yml\" \\\n",
    "        --data_dir ../MLM/coNLL_tsv_json \\\n",
    "        --max_seq_len 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
