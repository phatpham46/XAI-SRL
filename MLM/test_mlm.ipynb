{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(line):\n",
    "    line = re.sub(r'-+',' ',line)\n",
    "    #line = re.sub(r'[^a-zA-Z, ]+',\" \",line)\n",
    "    line = re.sub(r'[ ]+',\" \",line)\n",
    "    line += \".\"\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = 'In contrast, further deletion of the C-terminal transactivation domain in the Pax5 mutants B8 and B9 can abolish transcriptional stimulation, whereas internal deletion of the conserved octapeptide motif (OP) or the partial homeodomain (HD) of Pax5 did not have any effect (Figure 3B).'\n",
    "print(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "print(\"inputs\", inputs)\n",
    "print(\"len(inputs.input_ids[0])\", len(inputs.input_ids[0]))\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "print(\"labels\", labels)\n",
    "print(\"len(labels[0])\", len(labels[0]))\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "def get_random_indices(sentence):\n",
    "    # Get the length of the sentence\n",
    "    sentence_length = len(sentence.split())\n",
    "  \n",
    "    masked_idx = random.sample(range(sentence_length), 10)\n",
    "    print(\"masked_idx\", masked_idx)\n",
    "    # Shuffle the indices of the sentence\n",
    "    indices = np.arange(0, sentence_length-1, 1)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Select the first 10 unique indices\n",
    "    random_indices = indices[:10]\n",
    "\n",
    "    # Ensure that the selected indices are distinct\n",
    "    while len(np.unique(random_indices)) < 10:\n",
    "        np.random.shuffle(indices)\n",
    "        random_indices = indices[:10]\n",
    "\n",
    "    return random_indices\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"This is a sample sentence This is a sample sentence This is a sample sentence.\",\n",
    "    \"Another example sentence with more words This is a sample sentence This is a sample sentence.\",\n",
    "    \"A third sentence to demonstrate the process This is a sample sentence This is a sample sentence.\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    random_indices = get_random_indices(sentence)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Random Indices: {random_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentences = [[\"This is the first sentence.\"], ['This is the sencond sentence.']] \n",
    "encoded_inputs = tokenizer(sentences, padding=\"max_length\", truncation=True)\n",
    "\n",
    "print(encoded_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example shapes\u001b[39;00m\n\u001b[1;32m      5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m85\u001b[39m)  \u001b[38;5;66;03m# Assuming 32 batches, each with a sequence length of 85\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Example logits, shape [85, 28996]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m28996\u001b[39m, (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m85\u001b[39m))  \u001b[38;5;66;03m# Assuming labels for each position in the sequence\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Ensure that logits match the number of classes in your task\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example shapes\n",
    "input_ids = torch.randn(32, 85)  # Assuming 32 batches, each with a sequence length of 85\n",
    "logits = outputs.logits[0]  # Example logits, shape [85, 28996]\n",
    "labels = torch.randint(0, 28996, (32, 85))  # Assuming labels for each position in the sequence\n",
    "\n",
    "# Ensure that logits match the number of classes in your task\n",
    "num_classes = logits.size(1)\n",
    "\n",
    "# Transpose logits to be [sequence_length, batch_size, num_classes]\n",
    "logits = logits.transpose(0, 1)\n",
    "\n",
    "# Flatten the logits and labels for the loss calculation\n",
    "logits_flat = logits.contiguous().view(-1, num_classes)\n",
    "labels_flat = labels.view(-1)\n",
    "\n",
    "# Assuming a simple sequence classification task\n",
    "# You can use CrossEntropyLoss along the sequence dimension\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = loss_fn(logits_flat, labels_flat)\n",
    "\n",
    "# Your training/update step here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor with size [32, 85]\n",
    "original_tensor = torch.randn(32, 85)\n",
    "print(original_tensor)  \n",
    "# Sum along the second dimension (axis 1)\n",
    "summed_tensor = torch.sum(original_tensor, dim=1)\n",
    "print(summed_tensor)\n",
    "# Check the size of the resulting tensor\n",
    "print(summed_tensor.size())  # Should print torch.Size([32])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Input text with a [MASK] token\n",
    "text = \"The [MASK] is a large mammal native to North America.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get the model's output logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Assuming you have a [MASK] token in the input, find its position\n",
    "mask_position = tokens['input_ids'][0].tolist().index(tokenizer.mask_token_id)\n",
    "print(\"mask_position\", mask_position)\n",
    "# Extract the logits for the masked position\n",
    "masked_logits = logits[0, mask_position]\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
    "\n",
    "# Get the token with the highest probability (predicted token)\n",
    "predicted_token_id = torch.argmax(probabilities).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "# Replace the [MASK] token with the predicted token in the original text\n",
    "result_text = text.replace('[MASK]', predicted_token)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Result Text:\", result_text)\n",
    "print(\"Predicted Token:\", predicted_token)\n",
    "print(\"Probabilities:\", probabilities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "labels tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110, 14247,  1548,   119,\n",
      "           102]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.01"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = \"The capital of France is [MASK].\"\n",
    "# inputs = tokenizer.encode_plus(mask, add_special_tokens=True,\n",
    "#                                     truncation_strategy ='only_first',\n",
    "#                                     max_length = 11, padding='max_length', return_tensors=\"pt\") \n",
    "inputs = tokenizer(mask, return_tensors=\"pt\", max_length=11, padding='max_length')\n",
    "print(len(inputs.input_ids))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\", max_length=11)[\"input_ids\"]\n",
    "print(\"labels\", labels)\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)\n",
    "\n",
    "# [ 101, 1996, 3007, 1997, 2605, 2003,  103, 1012,  102]\n",
    "\n",
    "# [101, 1103, 2364, 1104, 175, 10555, 1110, 103, 119, 102] dmislab token của ngta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1056, 1044, 1041, 1039, 1037, 1052, 1045, 1056, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110,   103,   119,   102,\n",
      "             0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
      "unknown\n",
      "predicted token id:  tensor([3655])\n",
      "labels:  tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110, 14247,  1548,   119,\n",
      "           102]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.01"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\", max_length=11, padding='max_length')\n",
    "print(inputs)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "print(tokenizer.decode(predicted_token_id))\n",
    "print(\"predicted token id: \", predicted_token_id)\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\", max_length=11, padding='max_length')[\"input_ids\"]\n",
    "\n",
    "print(\"labels: \", labels)\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)\n",
    "\n",
    "#tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110,   103,   119,   102, 0]]\n",
    "#tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110, 14247,  1548,   119, 102]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-14 13:08:01.125343: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-14 13:08:03.440760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 13:08:06.716099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-03-14 13:08:18.571125: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "import math\n",
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple, defaultdict\n",
    "from tempfile import TemporaryDirectory\n",
    "from sklearn.metrics import accuracy_score\n",
    "from babel.dates import format_time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "# sys.path.insert(1, '/content/SRLPredictionEasel')\n",
    "\n",
    "# from bert_mlm_finetune import BertForMLMPreTraining \n",
    "from transformers import BertTokenizer, BertConfig, AdamW, get_linear_schedule_with_warmup, BertForMaskedLM \n",
    "from utils_mlm import count_num_cpu_gpu\n",
    "import spacy\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", do_lower_case=True)\n",
    "\n",
    "MLM_IGNORE_LABEL_IDX = -1\n",
    "VOCAB_SIZE = 28996 \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "MAX_SEQ_LEN = 85\n",
    "NUM_CPU = count_num_cpu_gpu()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define get pos tag func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(text, index):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Check if the index is within the bounds\n",
    "    if index < 0 or index >= len(doc):\n",
    "       return \"Index out of bounds\"\n",
    "\n",
    "    # Get the POS tag for the word at the specified index\n",
    "    pos_tag = doc[index].pos_\n",
    "    return pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is POS match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_POS_match(logits, input_ids, lm_label_ids):\n",
    "    '''\n",
    "    Function to check if the POS tag of the masked token in the logits is the same as the POS tag of the masked token in the original text.\n",
    "    Note: This function assumes that the logits are of shape # ([85, 28996]) \n",
    "    lm_label_ids: shape (batch_size, sequence_length)\n",
    "    '''\n",
    "    \n",
    "    origin_input_id = input_ids.clone() # Origin input id:  torch.Size([85])\n",
    "   \n",
    "    # Find the index of the masked token from lm_label_ids\n",
    "    masked_idx = torch.where(lm_label_ids != -100)[0]\n",
    "    masked_idx_input = torch.where(input_ids == tokenizer.mask_token_id)[0]\n",
    "   \n",
    "    origin_input_id[masked_idx_input] = lm_label_ids[masked_idx] \n",
    "    \n",
    "    # get pos tag of origin text\n",
    "    text = tokenizer.decode(input_ids)\n",
    "    origin_text = tokenizer.decode(origin_input_id) \n",
    "    print(\"ORIGIN TEXT: \", origin_text)\n",
    "    pos_tag_origin = get_pos_tag(origin_text, masked_idx_input)\n",
    "    print(\"POS TAG ORIGIN: \", pos_tag_origin)\n",
    "   \n",
    "    # Extract the logits for the masked position\n",
    "    masked_logits = logits[0, masked_idx]\n",
    "    print(\"MASKED LOGITS: \", masked_logits) # torch.Size([28996])\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
    "\n",
    "    # Get the token with the highest probability (predicted token)\n",
    "    predicted_token_id = torch.argmax(probabilities).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "    # Replace the [MASK] token with the predicted token in the original text\n",
    "    result_text = text.replace('[MASK]', predicted_token) \n",
    "    print(\"RESULT TEXT: \", result_text)\n",
    "    \n",
    "    # get pos tag of logits\n",
    "    logits_tag = get_pos_tag(result_text, masked_idx)\n",
    "    print(\"LOGITS TAGS: \", logits_tag)\n",
    "    return pos_tag_origin == logits_tag    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custome loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def custom_loss(input_ids, logits, labels):\n",
    "  \n",
    "    # Cross-entropy term\n",
    "    cross_entropy_term = F.cross_entropy(logits, labels, reduction='none')\n",
    "    print(\"Cross entropy term shape: \", cross_entropy_term.shape)      ##Cross entropy term shape:  torch.Size([2720])\n",
    "    # logits_shape = (32, 85, VOCAB_SIZE)\n",
    "    # logits_tensor = logits.view(*logits_shape)\n",
    "    \n",
    "    # labels_shape = (32, 85)\n",
    "    # labels_tensor = labels.view(*labels_shape)\n",
    "    \n",
    "    \n",
    "    # Custom matching term\n",
    "    matching_term_lst = []\n",
    "    # for logit, input_id, label in zip(logits_tensor, input_ids, labels_tensor):\n",
    "       \n",
    "    matching_term = is_POS_match(logits=logits, input_ids=input_ids, lm_label_ids=labels)\n",
    "    print(\"Matching term: \", matching_term) \n",
    "    matching_term_lst.append(matching_term) \n",
    "    # hay mình thử sửa cái POS cho cái batch luôn kh, tại cái logit với cái label truyền vô là cái batch á\n",
    "    # mà t sợ nhiều khi mình reshape sai nên nó tính sai \n",
    "    matching_term = torch.tensor(matching_term_lst)\n",
    "    # Combine terms\n",
    "    loss = 0.5 * cross_entropy_term + (1 - matching_term)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train with costom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted token: 119 ['.']\n",
      "tensor(3.5668, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### THEO CAI LOSS NAY NHA PHAT OIIIIIII\n",
    "# https://discuss.huggingface.co/t/bertformaskedlm-s-loss-and-scores-how-the-loss-is-computed/607/2\n",
    "import copy\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "import torch\n",
    "model = BertForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "\n",
    "text = \"Who was Jim Paterson ? Jim Paterson is a doctor\".lower()\n",
    "inputs  =  tokenizer.encode_plus(text,  return_tensors=\"pt\", add_special_tokens = True, truncation=True, pad_to_max_length = True,\n",
    "                                         return_attention_mask = True,  max_length=64)\n",
    "\n",
    "input_ids = inputs['input_ids']\n",
    "labels  = copy.deepcopy(input_ids) #this is the part I changed\n",
    "input_ids[0][7] = tokenizer.mask_token_id\n",
    "labels[input_ids != tokenizer.mask_token_id] = -100 \n",
    "\n",
    "output = model(input_ids = input_ids, attention_mask = inputs['attention_mask'] , token_type_ids=inputs['token_type_ids'] , labels=labels)\n",
    "\n",
    "pred = torch.argmax( output.logits[0][7]).item()\n",
    "print(\"predicted token:\", pred, tokenizer.convert_ids_to_tokens([pred])  )\n",
    "\n",
    "\n",
    "# logSoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "# NLLLos = torch.nn.NLLLoss()\n",
    "\n",
    "# print(NLLLos( logSoftmax(torch.unsqueeze(output.logits[0][7], 0)), torch.tensor([pred]))) #the same as F.cross_entropy(scores.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "# calculate loss manually\n",
    "import torch.nn.functional as F\n",
    "loss2 = F.cross_entropy(output.logits.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "print(loss2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 30522])\n"
     ]
    }
   ],
   "source": [
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test loss cross entropy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
