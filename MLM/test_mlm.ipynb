{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(line):\n",
    "    line = re.sub(r'-+',' ',line)\n",
    "    #line = re.sub(r'[^a-zA-Z, ]+',\" \",line)\n",
    "    line = re.sub(r'[ ]+',\" \",line)\n",
    "    line += \".\"\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = 'In contrast, further deletion of the C-terminal transactivation domain in the Pax5 mutants B8 and B9 can abolish transcriptional stimulation, whereas internal deletion of the conserved octapeptide motif (OP) or the partial homeodomain (HD) of Pax5 did not have any effect (Figure 3B).'\n",
    "print(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "print(\"inputs\", inputs)\n",
    "print(\"len(inputs.input_ids[0])\", len(inputs.input_ids[0]))\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "print(\"labels\", labels)\n",
    "print(\"len(labels[0])\", len(labels[0]))\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "def get_random_indices(sentence):\n",
    "    # Get the length of the sentence\n",
    "    sentence_length = len(sentence.split())\n",
    "  \n",
    "    masked_idx = random.sample(range(sentence_length), 10)\n",
    "    print(\"masked_idx\", masked_idx)\n",
    "    # Shuffle the indices of the sentence\n",
    "    indices = np.arange(0, sentence_length-1, 1)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Select the first 10 unique indices\n",
    "    random_indices = indices[:10]\n",
    "\n",
    "    # Ensure that the selected indices are distinct\n",
    "    while len(np.unique(random_indices)) < 10:\n",
    "        np.random.shuffle(indices)\n",
    "        random_indices = indices[:10]\n",
    "\n",
    "    return random_indices\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"This is a sample sentence This is a sample sentence This is a sample sentence.\",\n",
    "    \"Another example sentence with more words This is a sample sentence This is a sample sentence.\",\n",
    "    \"A third sentence to demonstrate the process This is a sample sentence This is a sample sentence.\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    random_indices = get_random_indices(sentence)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Random Indices: {random_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentences = [[\"This is the first sentence.\"], ['This is the sencond sentence.']] \n",
    "encoded_inputs = tokenizer(sentences, padding=\"max_length\", truncation=True)\n",
    "\n",
    "print(encoded_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example shapes\u001b[39;00m\n\u001b[1;32m      5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m85\u001b[39m)  \u001b[38;5;66;03m# Assuming 32 batches, each with a sequence length of 85\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Example logits, shape [85, 28996]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m28996\u001b[39m, (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m85\u001b[39m))  \u001b[38;5;66;03m# Assuming labels for each position in the sequence\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Ensure that logits match the number of classes in your task\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example shapes\n",
    "input_ids = torch.randn(32, 85)  # Assuming 32 batches, each with a sequence length of 85\n",
    "logits = outputs.logits[0]  # Example logits, shape [85, 28996]\n",
    "labels = torch.randint(0, 28996, (32, 85))  # Assuming labels for each position in the sequence\n",
    "\n",
    "# Ensure that logits match the number of classes in your task\n",
    "num_classes = logits.size(1)\n",
    "\n",
    "# Transpose logits to be [sequence_length, batch_size, num_classes]\n",
    "logits = logits.transpose(0, 1)\n",
    "\n",
    "# Flatten the logits and labels for the loss calculation\n",
    "logits_flat = logits.contiguous().view(-1, num_classes)\n",
    "labels_flat = labels.view(-1)\n",
    "\n",
    "# Assuming a simple sequence classification task\n",
    "# You can use CrossEntropyLoss along the sequence dimension\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = loss_fn(logits_flat, labels_flat)\n",
    "\n",
    "# Your training/update step here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor with size [32, 85]\n",
    "original_tensor = torch.randn(32, 85)\n",
    "print(original_tensor)  \n",
    "# Sum along the second dimension (axis 1)\n",
    "summed_tensor = torch.sum(original_tensor, dim=1)\n",
    "print(summed_tensor)\n",
    "# Check the size of the resulting tensor\n",
    "print(summed_tensor.size())  # Should print torch.Size([32])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Input text with a [MASK] token\n",
    "text = \"The [MASK] is a large mammal native to North America.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get the model's output logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Assuming you have a [MASK] token in the input, find its position\n",
    "mask_position = tokens['input_ids'][0].tolist().index(tokenizer.mask_token_id)\n",
    "print(\"mask_position\", mask_position)\n",
    "# Extract the logits for the masked position\n",
    "masked_logits = logits[0, mask_position]\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
    "\n",
    "# Get the token with the highest probability (predicted token)\n",
    "predicted_token_id = torch.argmax(probabilities).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "# Replace the [MASK] token with the predicted token in the original text\n",
    "result_text = text.replace('[MASK]', predicted_token)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Result Text:\", result_text)\n",
    "print(\"Predicted Token:\", predicted_token)\n",
    "print(\"Probabilities:\", probabilities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "labels tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110, 14247,  1548,   119,\n",
      "           102]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.01"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = \"The capital of France is [MASK].\"\n",
    "# inputs = tokenizer.encode_plus(mask, add_special_tokens=True,\n",
    "#                                     truncation_strategy ='only_first',\n",
    "#                                     max_length = 11, padding='max_length', return_tensors=\"pt\") \n",
    "inputs = tokenizer(mask, return_tensors=\"pt\", max_length=11, padding='max_length')\n",
    "print(len(inputs.input_ids))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\", max_length=11)[\"input_ids\"]\n",
    "print(\"labels\", labels)\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)\n",
    "\n",
    "# [ 101, 1996, 3007, 1997, 2605, 2003,  103, 1012,  102]\n",
    "\n",
    "# [101, 1103, 2364, 1104, 175, 10555, 1110, 103, 119, 102] dmislab token của ngta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1056, 1044, 1041, 1039, 1037, 1052, 1045, 1056, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110,   103,   119,   102,\n",
      "             0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
      "unknown\n",
      "predicted token id:  tensor([3655])\n",
      "labels:  tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110, 14247,  1548,   119,\n",
      "           102]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.01"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\", max_length=11, padding='max_length')\n",
    "print(inputs)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "print(tokenizer.decode(predicted_token_id))\n",
    "print(\"predicted token id: \", predicted_token_id)\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\", max_length=11, padding='max_length')[\"input_ids\"]\n",
    "\n",
    "print(\"labels: \", labels)\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)\n",
    "\n",
    "#tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110,   103,   119,   102, 0]]\n",
    "#tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110, 14247,  1548,   119, 102]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-14 13:08:01.125343: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-14 13:08:03.440760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 13:08:06.716099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-03-14 13:08:18.571125: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "import math\n",
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple, defaultdict\n",
    "from tempfile import TemporaryDirectory\n",
    "from sklearn.metrics import accuracy_score\n",
    "from babel.dates import format_time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "# sys.path.insert(1, '/content/SRLPredictionEasel')\n",
    "\n",
    "# from bert_mlm_finetune import BertForMLMPreTraining \n",
    "from transformers import BertTokenizer, BertConfig, AdamW, get_linear_schedule_with_warmup, BertForMaskedLM \n",
    "from utils_mlm import count_num_cpu_gpu\n",
    "import spacy\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", do_lower_case=True)\n",
    "\n",
    "MLM_IGNORE_LABEL_IDX = -1\n",
    "VOCAB_SIZE = 28996 \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "MAX_SEQ_LEN = 85\n",
    "NUM_CPU = count_num_cpu_gpu()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define get pos tag func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is POS match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_POS_match(logits, input_ids, lm_label_ids):\n",
    "    '''\n",
    "    Function to check if the POS tag of the masked token in the logits is the same as the POS tag of the masked token in the original text.\n",
    "    Note: This function assumes that the logits are of shape # ([85, 28996]) \n",
    "    lm_label_ids: shape (batch_size, sequence_length)\n",
    "    '''\n",
    "    \n",
    "    origin_input_id = input_ids.clone() # Origin input id:  torch.Size([85])\n",
    "   \n",
    "    # Find the index of the masked token from lm_label_ids\n",
    "    masked_idx = torch.where(lm_label_ids != -100)[0]\n",
    "    masked_idx_input = torch.where(input_ids == tokenizer.mask_token_id)[0]\n",
    "   \n",
    "    origin_input_id[masked_idx_input] = lm_label_ids[masked_idx] \n",
    "    \n",
    "    # get pos tag of origin text\n",
    "    text = tokenizer.decode(input_ids)\n",
    "    origin_text = tokenizer.decode(origin_input_id) \n",
    "    print(\"ORIGIN TEXT: \", origin_text)\n",
    "    pos_tag_origin = get_pos_tag(origin_text, masked_idx_input)\n",
    "    print(\"POS TAG ORIGIN: \", pos_tag_origin)\n",
    "   \n",
    "    # Extract the logits for the masked position\n",
    "    masked_logits = logits[0, masked_idx]\n",
    "    print(\"MASKED LOGITS: \", masked_logits) # torch.Size([28996])\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
    "\n",
    "    # Get the token with the highest probability (predicted token)\n",
    "    predicted_token_id = torch.argmax(probabilities).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "    # Replace the [MASK] token with the predicted token in the original text\n",
    "    result_text = text.replace('[MASK]', predicted_token) \n",
    "    print(\"RESULT TEXT: \", result_text)\n",
    "    \n",
    "    # get pos tag of logits\n",
    "    logits_tag = get_pos_tag(result_text, masked_idx)\n",
    "    print(\"LOGITS TAGS: \", logits_tag)\n",
    "    return pos_tag_origin == logits_tag    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custome loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def custom_loss(input_ids, logits, labels):\n",
    "  \n",
    "    # Cross-entropy term\n",
    "    cross_entropy_term = F.cross_entropy(logits, labels, reduction='none')\n",
    "    print(\"Cross entropy term shape: \", cross_entropy_term.shape)      ##Cross entropy term shape:  torch.Size([2720])\n",
    "    # logits_shape = (32, 85, VOCAB_SIZE)\n",
    "    # logits_tensor = logits.view(*logits_shape)\n",
    "    \n",
    "    # labels_shape = (32, 85)\n",
    "    # labels_tensor = labels.view(*labels_shape)\n",
    "    \n",
    "    \n",
    "    # Custom matching term\n",
    "    matching_term_lst = []\n",
    "    # for logit, input_id, label in zip(logits_tensor, input_ids, labels_tensor):\n",
    "       \n",
    "    matching_term = is_POS_match(logits=logits, input_ids=input_ids, lm_label_ids=labels)\n",
    "    print(\"Matching term: \", matching_term) \n",
    "    matching_term_lst.append(matching_term) \n",
    "    # hay mình thử sửa cái POS cho cái batch luôn kh, tại cái logit với cái label truyền vô là cái batch á\n",
    "    # mà t sợ nhiều khi mình reshape sai nên nó tính sai \n",
    "    matching_term = torch.tensor(matching_term_lst)\n",
    "    # Combine terms\n",
    "    loss = 0.5 * cross_entropy_term + (1 - matching_term)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train with costom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tokens_for_words(words, input_ids, offsets):\n",
    "    '''\n",
    "    Input:\n",
    "        words = ['The', 'capital', 'of', 'the', 'France', 'is', 'Paris', '.']\n",
    "        input_ids = [101, 1109, 3007, 1104, 2605, 1110, 3000, 119, 102]\n",
    "        offsets = [(None, None), (0, 3), (4, 11), (12, 14), (15, 17), (18, 23), (24, 25), (None, None)]\n",
    "    Output: \n",
    "        word_dict = {'The': [tensor(101), tensor(170), tensor(170)], 'capital': [tensor(1109), tensor(3007)], ....}\n",
    "    '''\n",
    "    word_dict = {}\n",
    "    current_word = ''\n",
    "    token_list = []\n",
    "    sentence = ' '.join(words)\n",
    "    print(\"input id shape: \", input_ids.shape)\n",
    "    for j, (token, offset) in enumerate(zip(input_ids, offsets)):\n",
    "        \n",
    "        if offset[0] is not None:  # If the token is associated with a word in the original text\n",
    "            start, end = offset\n",
    "            original_word = sentence[start:end]\n",
    "            # original_word = tokenizer.decode(input_ids[j], skip_special_tokens=True)\n",
    "            if j > 0 and offset[0] == offsets[j - 1][1]:  # Check if the current word should be concatenated\n",
    "                current_word += original_word\n",
    "                token_list.append(token)\n",
    "            else:\n",
    "                if current_word:\n",
    "                    if current_word in word_dict:\n",
    "                        word_dict[current_word].append(token_list)\n",
    "                    else:\n",
    "                        word_dict[current_word] = [token_list]\n",
    "                current_word = original_word\n",
    "                token_list = [token]\n",
    "\n",
    "    # Add the last word\n",
    "    if current_word:\n",
    "        if current_word in word_dict:\n",
    "            word_dict[current_word].append(token_list)\n",
    "        else:\n",
    "            word_dict[current_word] = [token_list]\n",
    "\n",
    "    return {word: [item for sublist in tokens for item in sublist]  for word, tokens in word_dict.items() }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-25 11:52:42.560449: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-25 11:52:43.040984: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-25 11:52:44.807674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "import torch\n",
    "import copy\n",
    "model = BertForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import numpy as np\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = []\n",
    "        with open(csv_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                data_dict = json.loads(line)\n",
    "                data_dict['token_id'] = json.loads(data_dict['token_id'])\n",
    "                data_dict['attention_mask'] = json.loads(data_dict['attention_mask'])\n",
    "                data_dict['token_type_ids'] = json.loads(data_dict['token_type_ids'])\n",
    "                data_dict['labels'] = json.loads(data_dict['labels'])\n",
    "                self.data.append(data_dict)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        token_id = torch.tensor(sample['token_id'], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(sample['attention_mask'], dtype=torch.long)\n",
    "        token_type_ids = torch.tensor(sample['token_type_ids'], dtype=torch.long)\n",
    "        labels = torch.tensor(sample['labels'], dtype=torch.long)\n",
    "        return token_id, attention_mask, token_type_ids, labels\n",
    "\n",
    "# Example usage:\n",
    "csv_file = '/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/mlm_output_2/mlm_abolish_full.csv'\n",
    "dataset = CustomDataset(csv_file)\n",
    "\n",
    "# Now you can use PyTorch DataLoader to iterate over the dataset\n",
    "# For example:\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# for token_id, attention_mask, token_type_ids, labels in dataloader:\n",
    "#     # Your training/validation loop here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [101, 170, 103, 118, 1106, 118, 170, 6468, 112...\n",
      "1       [101, 170, 176, 118, 1106, 118, 170, 103, 1120...\n",
      "2       [101, 170, 176, 118, 1106, 118, 170, 6468, 112...\n",
      "3       [101, 170, 176, 118, 1106, 118, 170, 6468, 112...\n",
      "4       [101, 170, 176, 118, 1106, 118, 170, 6468, 112...\n",
      "                              ...                        \n",
      "1059    [101, 1142, 27553, 7770, 17895, 117, 1134, 145...\n",
      "1060    [101, 1142, 27553, 7770, 17895, 117, 1134, 145...\n",
      "1061    [101, 1142, 27553, 7770, 17895, 117, 1134, 145...\n",
      "1062    [101, 1142, 27553, 7770, 17895, 117, 1134, 145...\n",
      "1063    [101, 1142, 27553, 7770, 17895, 117, 1134, 145...\n",
      "Name: token_id, Length: 1064, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "def read_csv_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file with the specified format.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): The path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    - data (list of tuples): A list containing tuples of token_id, attention_mask, token_type_ids, and labels.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            token_id = list(map(int, row['token_id'][1:-1].split(', ')))\n",
    "            attention_mask = list(map(int, row['attention_mask'][1:-1].split(', ')))\n",
    "            token_type_ids = list(map(int, row['token_type_ids'][1:-1].split(', ')))\n",
    "            labels = list(map(int, row['labels'][1:-1].split(', ')))\n",
    "            \n",
    "            data.append((token_id, attention_mask, token_type_ids, labels))\n",
    "    \n",
    "    # Convert list of tuples to DataFrame\n",
    "    df = pd.DataFrame(data, columns=['token_id', 'attention_mask', 'token_type_ids', 'labels'])\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "file_path = '/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/mlm_output_2/mlm_abolish_full.csv'\n",
    "\n",
    "data = read_csv_file(file_path)\n",
    "print(data['token_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset, RandomSampler, SequentialSampler\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Prepare validation dataloader\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_dataset_thu \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m(csv_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/mlm_prepared_data_2/train_mlm.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset_thu))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomDataset' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "# Prepare validation dataloader\n",
    "train_dataset_thu = CustomDataset(csv_file='/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/mlm_prepared_data_2/train_mlm.json')\n",
    "\n",
    "print(len(train_dataset_thu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from prepared_for_mlm import get_pos_tag, mask_content_words\n",
    "\n",
    "def masking_sentence_word(words, input_ids, offsets, tokenizer):\n",
    "    '''\n",
    "    Input: \n",
    "        words = ['The', 'capital', 'of', 'France', 'is', 'Paris', '.']\n",
    "    Output: \n",
    "        masked_sentences = [tensor([  101,  1103, 175, 10555,  1110,   103,   119,   102]), [  101,  1103,  2364, 10555,  103,   103,   119,   102]] \n",
    "        label_ids = [tensor(6468)], [tensor(1568), tensor(13892)]\n",
    "    '''\n",
    "    # get a list of token for the word\n",
    "    word_dict = get_tokens_for_words(words, input_ids, offsets)\n",
    "   \n",
    "    # masked the tokens if the word is the content word\n",
    "    masked_sentences, label_ids, masked_indices = mask_content_words(input_ids, word_dict, tokenizer)\n",
    "    \n",
    "    return masked_sentences, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['101', '1103', '173', '1605', '1104', '103', '22940', '1174', '20942', '113', '103', '103', '103', '114', '1336', '1138', '1151', '1215', '1106', '2423', '11303', '174', '119', '1884', '2646', '119', '102']\n"
     ]
    }
   ],
   "source": [
    "token_ids = [101, 1103, 173, 1605, 1104, 103, 22940, 1174, 20942, 113, 103, 103, 103, 114, 1336, 1138, 1151, 1215, 1106, 2423, 11303, 174, 119, 1884, 2646, 119, 102]\n",
    "\n",
    "sample_token_id = [str(x) for x in token_ids]\n",
    "\n",
    "print(sample_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:  linomide and antibody - targeted superantigen therapy can abolish formation of liver metastases in mice .\n",
      "WORD_DICT:  {'linomide': [tensor(181), tensor(4559), tensor(3080), tensor(2007)], 'and': [tensor(1105)], 'antibody': [tensor(2848), tensor(14637)], '-': [tensor(118)], 'targeted': [tensor(9271)], 'superantigen': [tensor(7688), tensor(25711), tensor(4915)], 'therapy': [tensor(7606)], 'can': [tensor(1169)], 'abolish': [tensor(170), tensor(15792), tensor(2944)], 'formation': [tensor(3855)], 'of': [tensor(1104)], 'liver': [tensor(11911)], 'metastases': [tensor(27154), tensor(8419), tensor(8830)], 'in': [tensor(1107)], 'mice': [tensor(14105)], '.': [tensor(119)]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 128\u001b[0m\n\u001b[1;32m    124\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenized_sentence[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]  \n\u001b[1;32m    126\u001b[0m offsets \u001b[38;5;241m=\u001b[39m tokenized_sentence[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Character offsets in the original input text\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m masked_sens, label_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmasking_sentence_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# test mask 0 với mask 3 \u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# labels  = copy.deepcopy(input_ids) #this is the part I changed\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# print(\"labels\", labels)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# print(\"attention mask\", input.attention_mask)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasked_sens\u001b[39m\u001b[38;5;124m\"\u001b[39m, masked_sens[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn [5], line 101\u001b[0m, in \u001b[0;36mmasking_sentence_word\u001b[0;34m(words, input_ids, offsets, tokenizer)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWORD_DICT: \u001b[39m\u001b[38;5;124m\"\u001b[39m, word_dict)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# masked the tokens if the word is the content word\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m masked_sentences, label_ids, masked_indices \u001b[38;5;241m=\u001b[39m \u001b[43mmask_content_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m masked_sentences, label_ids, masked_indices, word_dict\n",
      "Cell \u001b[0;32mIn [5], line 81\u001b[0m, in \u001b[0;36mmask_content_words\u001b[0;34m(ids, word_dict, tokenizer)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;66;03m# get the index of the masked token\u001b[39;00m\n\u001b[1;32m     79\u001b[0m         masked_indice[masked_ids \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmask_token_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmasked_indice\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     83\u001b[0m         label[\u001b[38;5;241m0\u001b[39m][idx] \u001b[38;5;241m=\u001b[39m ids[\u001b[38;5;241m0\u001b[39m][idx]\n",
      "File \u001b[0;32m~/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/_tensor.py:930\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;66;03m# NB: we use 'imap' and not 'map' here, so that in Python 2 we get a\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;66;03m# generator and don't eagerly perform all the indexes.  This could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;66;03m# NB: We have intentionally skipped __torch_function__ dispatch here.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;66;03m# See gh-54457\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 930\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration over a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    932\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    939\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "# encode xong mới mask\n",
    "### THEO CAI LOSS NAY NHA PHAT OIIIIIII\n",
    "# https://discuss.huggingface.co/t/bertformaskedlm-s-loss-and-scores-how-the-loss-is-computed/607/2\n",
    "import spacy\n",
    "\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_tokens_for_words(words, input_ids, offsets, tokenizer):\n",
    "    '''\n",
    "    Input:\n",
    "        words = ['The', 'capital', 'of', 'the', 'France', 'is', 'Paris', '.']\n",
    "        input_ids = [101, 1109, 3007, 1104, 2605, 1110, 3000, 119, 102]\n",
    "        offsets = [(None, None), (0, 3), (4, 11), (12, 14), (15, 17), (18, 23), (24, 25), (None, None)]\n",
    "    Output: \n",
    "        word_dict = {'The': [tensor(101), tensor(170), tensor(170)], 'capital': [tensor(1109), tensor(3007)], ....}\n",
    "    '''\n",
    "    \n",
    "\n",
    "    word_dict = {}\n",
    "    current_word = ''\n",
    "    token_list = []\n",
    "    sentence = ' '.join(words)\n",
    "    print(\"sentence: \", sentence)\n",
    "    except_tokens = [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id, tokenizer.unk_token_id]\n",
    "    for j, (token, offset) in enumerate(zip(input_ids, offsets)):\n",
    "       \n",
    "        if offset[0] is not None:  # If the token is associated with a word in the original text\n",
    "            \n",
    "            start, end = offset\n",
    "            original_word = sentence[start:end]\n",
    "            # original_word = tokenizer.decode(input_ids[j], skip_special_tokens=True)\n",
    "            if j > 0 and offset[0] == offsets[j - 1][1]:  # Check if the current word should be concatenated\n",
    "                current_word += original_word\n",
    "                token_list.append(token)\n",
    "            else:\n",
    "                if current_word:\n",
    "                    word_dict[current_word] = [token_list]\n",
    "                current_word = original_word\n",
    "                token_list = [token]\n",
    "\n",
    "    # Add the last word\n",
    "    if current_word:\n",
    "        word_dict[current_word] = [token_list]\n",
    "\n",
    "    return {word: [item for sublist in tokens \n",
    "                            for item in sublist \n",
    "                                if item not in except_tokens ]  for word_idx, (word, tokens) in enumerate(word_dict.items()) }\n",
    "\n",
    "\n",
    "\n",
    "def masking_sentence_word(words, input_ids, offsets, tokenizer):\n",
    "    '''\n",
    "    Input: \n",
    "        words = ['The', 'capital', 'of', 'France', 'is', 'Paris', '.']\n",
    "    Output: \n",
    "        masked_sentences = [tensor([  101,  1103, 175, 10555,  1110,   103,   119,   102]), [  101,  1103,  2364, 10555,  103,   103,   119,   102]] \n",
    "        label_ids = [tensor(6468)], [tensor(1568), tensor(13892)]\n",
    "    '''\n",
    "    # get a list of token for the word\n",
    "    word_dict = get_tokens_for_words(words, input_ids, offsets, tokenizer)\n",
    "    print(\"WORD_DICT: \", word_dict)\n",
    "    # masked the tokens if the word is the content word\n",
    "    masked_sentences, label_ids, masked_indices = mask_content_words(input_ids, word_dict, tokenizer)\n",
    "    \n",
    "    return masked_sentences, label_ids, masked_indices, word_dict\n",
    "## HÀM MAIN CỦA masking_sentence_word\n",
    "\n",
    "# Example usage:\n",
    "# sentence = \"Cytokines altered the expression and activity of the multidrug resistance transporters in human hepatoma cell lines; analysis using RT-PCR and cDNA microarrays.\"\n",
    "sentence = \"A G-to-A transition at the first nucleotide of intron 2 of patient 1 abolished normal splicing.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Get the POS tag for each word in the text\n",
    "#words_str = [word.text for word in [token for token in doc]]\n",
    "words_str = ['linomide', 'and', 'antibody', '-', 'targeted', 'superantigen', 'therapy', 'can', 'abolish', 'formation', 'of', 'liver', 'metastases', 'in', 'mice', '.']\n",
    "tokenized_sentence = tokenizer.encode_plus(\n",
    "    ' '.join(words_str),\n",
    "    max_length=64,\n",
    "    padding='max_length',  # Pad to the maximum sequence length\n",
    "    truncation=True,  # Truncate to the maximum sequence length if necessary\n",
    "    add_special_tokens = True,\n",
    "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    return_attention_mask = True,\n",
    "    return_offsets_mapping=True  # Return offset mappings\n",
    ")\n",
    "input_ids = tokenized_sentence['input_ids'][0]  \n",
    "\n",
    "offsets = tokenized_sentence['offset_mapping'][0]  # Character offsets in the original input text\n",
    "\n",
    "masked_sens, label_ids = masking_sentence_word(words_str, input_ids, offsets, tokenizer)\n",
    "\n",
    "# test mask 0 với mask 3 \n",
    "\n",
    "# labels  = copy.deepcopy(input_ids) #this is the part I changed\n",
    "# input_ids[0][23] = tokenizer.mask_token_id\n",
    "# labels[input_ids != tokenizer.mask_token_id] = -100 \n",
    "\n",
    "# print(\"input_ids\", input_ids)\n",
    "# print(\"labels\", labels)\n",
    "# print(\"attention mask\", input.attention_mask)\n",
    "print(\"masked_sens\", masked_sens[0])\n",
    "print(\"label_ids\", label_ids[0])\n",
    "\n",
    "# convert 1-d tensor to 2-d tensor\n",
    "masked_sens_2d = torch.unsqueeze(masked_sens[0], 0)\n",
    "label_ids_2d = torch.unsqueeze(label_ids[0], 0)\n",
    "\n",
    "output = model(input_ids = masked_sens[0], attention_mask = tokenized_sentence['attention_mask'], token_type_ids=tokenized_sentence['token_type_ids'], labels=label_ids[0])\n",
    "\n",
    "for i in range(29):\n",
    "    pred = torch.argmax(output.logits[0][i]).item()\n",
    "    print(i, pred, tokenizer.decode(pred))\n",
    "    \n",
    "# cách 1\n",
    "# logSoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "# NLLLos = torch.nn.NLLLoss()\n",
    "# print(NLLLos( logSoftmax(torch.unsqueeze(output.logits[0][7], 0)), torch.tensor([pred]))) #the same as F.cross_entropy(scores.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "\n",
    "# cách 2: calculate loss manually\n",
    "import torch.nn.functional as F\n",
    "loss2 = F.cross_entropy(output.logits.view(-1, tokenizer.vocab_size), label_ids_2d.view(-1))\n",
    "print(loss2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(101) == torch.tensor(tokenizer.cls_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(101)\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([  101,   170,   103,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100,  176, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "1 tensor([  101,   170,   176,   118,  1106,   118,   170,   103,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100, -100, -100, -100, -100, -100, 6468, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "2 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "          103,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1148, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "3 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   103,   103,   103,   103,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,   183, 21977, 26918, 23767,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100])\n",
      "4 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104,   103,   103,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100, 27553,  1179,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100])\n",
      "5 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "          103,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, 5351, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "6 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,   103,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8632, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "7 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,   103,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2999,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "8 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   103,   103,   103,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,   188,  1643, 22548,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(masked_sens)):\n",
    "    print(i, masked_sens[i], label_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1203), started 0:00:21 ago. (Use '!kill 1203' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-73b2a4c3adedff5c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-73b2a4c3adedff5c\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "%tensorboard --logdir \"/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/logs_tb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MASK XONG MOI ENCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_sen[i] altered\n",
      "tmp_sen[i] expression\n",
      "tmp_sen[i] activity\n",
      "tmp_sen[i] resistance\n",
      "tmp_sen[i] cell\n",
      "tmp_sen[i] lines\n",
      "tmp_sen[i] analysis\n",
      "tmp_sen[i] using\n",
      "mask_indexs 18\n",
      "mask_sens ['Cytokines', 'altered', 'the', 'expression', 'and', 'activity', 'of', 'the', 'multidrug', 'resistance', 'transporters', 'in', 'human', 'hepatoma', 'cell', 'lines', ';', 'analysis', '[MASK]', 'RT', '-', 'PCR', 'and', 'cDNA', 'microarrays', '.']\n",
      "input id:  tensor([[  101,   172, 25669, 21420,  3965,  8599,  1103,  2838,  1105,  3246,\n",
      "          1104,  1103,  4321, 23632,  9610,  4789,  3936,  1468,  1107,  1769,\n",
      "          1119,  4163, 18778,  1161,  2765,  2442,   132,  3622,   103,   187,\n",
      "          1204,   118,   185,  1665,  1197,  1105,   172, 22834,  1161, 17599,\n",
      "         25203,  6834,   119,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "labels sens:  tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, 1606, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100]])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "0 . 119\n",
      "1 c 172\n",
      "2 ##yt 25669\n",
      "3 ##oki 21420\n",
      "4 ##nes 3965\n",
      "5 altered 8599\n",
      "6 the 1103\n",
      "7 expression 2838\n",
      "8 and 1105\n",
      "9 activity 3246\n",
      "10 of 1104\n",
      "11 the 1103\n",
      "12 multi 4321\n",
      "13 ##dr 23632\n",
      "14 ##ug 9610\n",
      "15 resistance 4789\n",
      "16 transport 3936\n",
      "17 ##ers 1468\n",
      "18 in 1107\n",
      "19 human 1769\n",
      "20 he 1119\n",
      "21 ##pa 4163\n",
      "22 ##tom 18778\n",
      "23 ##a 1161\n",
      "24 cell 2765\n",
      "25 lines 2442\n",
      "26 ; 132\n",
      "27 analysis 3622\n",
      "28 using 1606\n",
      "output logits:  torch.Size([64, 28996])\n",
      "labels sens:  torch.Size([64])\n",
      "tensor(1.1051, grad_fn=<NllLossBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mask xong mới encode\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "import torch\n",
    "model = BertForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "\n",
    "# text = \"A G-to-A transition at the first nucleotide of intron 2 of patient 1 abolished normal splicing.\".lower()\n",
    "text = \"Cytokines altered the expression and activity of the multidrug resistance transporters in human hepatoma cell lines; analysis using RT-PCR and cDNA microarrays.\"\n",
    "# Get the POS tag for each word in the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get the POS tag for each word in the text\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "words = [token for token in doc]\n",
    "mask_sens, labels_sens_, test_label = masking_sentence_word(words, tokenizer, pos_tags)  # mask là masked_sentence\n",
    "        \n",
    "print(\"mask_sens\", mask_sens[-1])\n",
    "# print(\"labels_sens\", labels_sens[0])\n",
    "           \n",
    "input = tokenizer.encode_plus(text = ' '.join(mask_sens[-1]), return_tensors=\"pt\", add_special_tokens = True, truncation=True, padding='max_length',\n",
    "                                         return_attention_mask = True,  max_length=64) \n",
    "labels_sens  = copy.deepcopy(input['input_ids']) #this is the part I changed\n",
    "labels_sens[input['input_ids'] == tokenizer.mask_token_id] = test_label\n",
    "labels_sens[input['input_ids'] != tokenizer.mask_token_id] = -100 \n",
    "\n",
    "print(\"input id: \" ,input.input_ids)\n",
    "print(\"labels sens: \", labels_sens)\n",
    "print(\"attention mask\", input.attention_mask)\n",
    "\n",
    "# print len attention mask when it is not padding\n",
    "\n",
    "output = model(input_ids = input['input_ids'], attention_mask = input['attention_mask'] , token_type_ids=input['token_type_ids'] , labels=labels_sens.clone().detach())\n",
    "\n",
    "for i in range(29):\n",
    "    pred = torch.argmax(output.logits[0][i]).item()\n",
    "    print(i, tokenizer.decode(pred), pred)\n",
    "\n",
    "print(\"output logits: \", output.logits.view(-1, tokenizer.vocab_size).shape)\n",
    "print(\"labels sens: \", labels_sens.view(-1).shape)\n",
    "# # loss \n",
    "import torch.nn.functional as F\n",
    "loss2 = F.cross_entropy(output.logits.view(-1, tokenizer.vocab_size), labels_sens.view(-1))\n",
    "print(loss2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 28996])\n"
     ]
    }
   ],
   "source": [
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test loss cross entropy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
