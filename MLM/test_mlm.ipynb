{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(line):\n",
    "    line = re.sub(r'-+',' ',line)\n",
    "    #line = re.sub(r'[^a-zA-Z, ]+',\" \",line)\n",
    "    line = re.sub(r'[ ]+',\" \",line)\n",
    "    line += \".\"\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = 'In contrast, further deletion of the C-terminal transactivation domain in the Pax5 mutants B8 and B9 can abolish transcriptional stimulation, whereas internal deletion of the conserved octapeptide motif (OP) or the partial homeodomain (HD) of Pax5 did not have any effect (Figure 3B).'\n",
    "print(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "print(\"inputs\", inputs)\n",
    "print(\"len(inputs.input_ids[0])\", len(inputs.input_ids[0]))\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "print(\"labels\", labels)\n",
    "print(\"len(labels[0])\", len(labels[0]))\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "def get_random_indices(sentence):\n",
    "    # Get the length of the sentence\n",
    "    sentence_length = len(sentence.split())\n",
    "  \n",
    "    masked_idx = random.sample(range(sentence_length), 10)\n",
    "    print(\"masked_idx\", masked_idx)\n",
    "    # Shuffle the indices of the sentence\n",
    "    indices = np.arange(0, sentence_length-1, 1)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Select the first 10 unique indices\n",
    "    random_indices = indices[:10]\n",
    "\n",
    "    # Ensure that the selected indices are distinct\n",
    "    while len(np.unique(random_indices)) < 10:\n",
    "        np.random.shuffle(indices)\n",
    "        random_indices = indices[:10]\n",
    "\n",
    "    return random_indices\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"This is a sample sentence This is a sample sentence This is a sample sentence.\",\n",
    "    \"Another example sentence with more words This is a sample sentence This is a sample sentence.\",\n",
    "    \"A third sentence to demonstrate the process This is a sample sentence This is a sample sentence.\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    random_indices = get_random_indices(sentence)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Random Indices: {random_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentences = [[\"This is the first sentence.\"], ['This is the sencond sentence.']] \n",
    "encoded_inputs = tokenizer(sentences, padding=\"max_length\", truncation=True)\n",
    "\n",
    "print(encoded_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example shapes\u001b[39;00m\n\u001b[1;32m      5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m85\u001b[39m)  \u001b[38;5;66;03m# Assuming 32 batches, each with a sequence length of 85\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Example logits, shape [85, 28996]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m28996\u001b[39m, (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m85\u001b[39m))  \u001b[38;5;66;03m# Assuming labels for each position in the sequence\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Ensure that logits match the number of classes in your task\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example shapes\n",
    "input_ids = torch.randn(32, 85)  # Assuming 32 batches, each with a sequence length of 85\n",
    "logits = outputs.logits[0]  # Example logits, shape [85, 28996]\n",
    "labels = torch.randint(0, 28996, (32, 85))  # Assuming labels for each position in the sequence\n",
    "\n",
    "# Ensure that logits match the number of classes in your task\n",
    "num_classes = logits.size(1)\n",
    "\n",
    "# Transpose logits to be [sequence_length, batch_size, num_classes]\n",
    "logits = logits.transpose(0, 1)\n",
    "\n",
    "# Flatten the logits and labels for the loss calculation\n",
    "logits_flat = logits.contiguous().view(-1, num_classes)\n",
    "labels_flat = labels.view(-1)\n",
    "\n",
    "# Assuming a simple sequence classification task\n",
    "# You can use CrossEntropyLoss along the sequence dimension\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = loss_fn(logits_flat, labels_flat)\n",
    "\n",
    "# Your training/update step here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor with size [32, 85]\n",
    "original_tensor = torch.randn(32, 85)\n",
    "print(original_tensor)  \n",
    "# Sum along the second dimension (axis 1)\n",
    "summed_tensor = torch.sum(original_tensor, dim=1)\n",
    "print(summed_tensor)\n",
    "# Check the size of the resulting tensor\n",
    "print(summed_tensor.size())  # Should print torch.Size([32])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Input text with a [MASK] token\n",
    "text = \"The [MASK] is a large mammal native to North America.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get the model's output logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Assuming you have a [MASK] token in the input, find its position\n",
    "mask_position = tokens['input_ids'][0].tolist().index(tokenizer.mask_token_id)\n",
    "print(\"mask_position\", mask_position)\n",
    "# Extract the logits for the masked position\n",
    "masked_logits = logits[0, mask_position]\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
    "\n",
    "# Get the token with the highest probability (predicted token)\n",
    "predicted_token_id = torch.argmax(probabilities).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "# Replace the [MASK] token with the predicted token in the original text\n",
    "result_text = text.replace('[MASK]', predicted_token)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Result Text:\", result_text)\n",
    "print(\"Predicted Token:\", predicted_token)\n",
    "print(\"Probabilities:\", probabilities)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
