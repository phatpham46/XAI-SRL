{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(line):\n",
    "    line = re.sub(r'-+',' ',line)\n",
    "    #line = re.sub(r'[^a-zA-Z, ]+',\" \",line)\n",
    "    line = re.sub(r'[ ]+',\" \",line)\n",
    "    line += \".\"\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = 'In contrast, further deletion of the C-terminal transactivation domain in the Pax5 mutants B8 and B9 can abolish transcriptional stimulation, whereas internal deletion of the conserved octapeptide motif (OP) or the partial homeodomain (HD) of Pax5 did not have any effect (Figure 3B).'\n",
    "print(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "print(\"inputs\", inputs)\n",
    "print(\"len(inputs.input_ids[0])\", len(inputs.input_ids[0]))\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "print(\"labels\", labels)\n",
    "print(\"len(labels[0])\", len(labels[0]))\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "def get_random_indices(sentence):\n",
    "    # Get the length of the sentence\n",
    "    sentence_length = len(sentence.split())\n",
    "  \n",
    "    masked_idx = random.sample(range(sentence_length), 10)\n",
    "    print(\"masked_idx\", masked_idx)\n",
    "    # Shuffle the indices of the sentence\n",
    "    indices = np.arange(0, sentence_length-1, 1)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Select the first 10 unique indices\n",
    "    random_indices = indices[:10]\n",
    "\n",
    "    # Ensure that the selected indices are distinct\n",
    "    while len(np.unique(random_indices)) < 10:\n",
    "        np.random.shuffle(indices)\n",
    "        random_indices = indices[:10]\n",
    "\n",
    "    return random_indices\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"This is a sample sentence This is a sample sentence This is a sample sentence.\",\n",
    "    \"Another example sentence with more words This is a sample sentence This is a sample sentence.\",\n",
    "    \"A third sentence to demonstrate the process This is a sample sentence This is a sample sentence.\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    random_indices = get_random_indices(sentence)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Random Indices: {random_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentences = [[\"This is the first sentence.\"], ['This is the sencond sentence.']] \n",
    "encoded_inputs = tokenizer(sentences, padding=\"max_length\", truncation=True)\n",
    "\n",
    "print(encoded_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example shapes\u001b[39;00m\n\u001b[1;32m      5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m85\u001b[39m)  \u001b[38;5;66;03m# Assuming 32 batches, each with a sequence length of 85\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Example logits, shape [85, 28996]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m28996\u001b[39m, (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m85\u001b[39m))  \u001b[38;5;66;03m# Assuming labels for each position in the sequence\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Ensure that logits match the number of classes in your task\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example shapes\n",
    "input_ids = torch.randn(32, 85)  # Assuming 32 batches, each with a sequence length of 85\n",
    "logits = outputs.logits[0]  # Example logits, shape [85, 28996]\n",
    "labels = torch.randint(0, 28996, (32, 85))  # Assuming labels for each position in the sequence\n",
    "\n",
    "# Ensure that logits match the number of classes in your task\n",
    "num_classes = logits.size(1)\n",
    "\n",
    "# Transpose logits to be [sequence_length, batch_size, num_classes]\n",
    "logits = logits.transpose(0, 1)\n",
    "\n",
    "# Flatten the logits and labels for the loss calculation\n",
    "logits_flat = logits.contiguous().view(-1, num_classes)\n",
    "labels_flat = labels.view(-1)\n",
    "\n",
    "# Assuming a simple sequence classification task\n",
    "# You can use CrossEntropyLoss along the sequence dimension\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = loss_fn(logits_flat, labels_flat)\n",
    "\n",
    "# Your training/update step here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor with size [32, 85]\n",
    "original_tensor = torch.randn(32, 85)\n",
    "print(original_tensor)  \n",
    "# Sum along the second dimension (axis 1)\n",
    "summed_tensor = torch.sum(original_tensor, dim=1)\n",
    "print(summed_tensor)\n",
    "# Check the size of the resulting tensor\n",
    "print(summed_tensor.size())  # Should print torch.Size([32])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'dmis-lab/biobert-base-cased-v1.2'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Input text with a [MASK] token\n",
    "text = \"The [MASK] is a large mammal native to North America.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get the model's output logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Assuming you have a [MASK] token in the input, find its position\n",
    "mask_position = tokens['input_ids'][0].tolist().index(tokenizer.mask_token_id)\n",
    "print(\"mask_position\", mask_position)\n",
    "# Extract the logits for the masked position\n",
    "masked_logits = logits[0, mask_position]\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
    "\n",
    "# Get the token with the highest probability (predicted token)\n",
    "predicted_token_id = torch.argmax(probabilities).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "# Replace the [MASK] token with the predicted token in the original text\n",
    "result_text = text.replace('[MASK]', predicted_token)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Result Text:\", result_text)\n",
    "print(\"Predicted Token:\", predicted_token)\n",
    "print(\"Probabilities:\", probabilities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "labels tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110, 14247,  1548,   119,\n",
      "           102]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.01"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = \"The capital of France is [MASK].\"\n",
    "# inputs = tokenizer.encode_plus(mask, add_special_tokens=True,\n",
    "#                                     truncation_strategy ='only_first',\n",
    "#                                     max_length = 11, padding='max_length', return_tensors=\"pt\") \n",
    "inputs = tokenizer(mask, return_tensors=\"pt\", max_length=11, padding='max_length')\n",
    "print(len(inputs.input_ids))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\", max_length=11)[\"input_ids\"]\n",
    "print(\"labels\", labels)\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)\n",
    "\n",
    "# [ 101, 1996, 3007, 1997, 2605, 2003,  103, 1012,  102]\n",
    "\n",
    "# [101, 1103, 2364, 1104, 175, 10555, 1110, 103, 119, 102] dmislab token của ngta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1056, 1044, 1041, 1039, 1037, 1052, 1045, 1056, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110,   103,   119,   102,\n",
      "             0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
      "unknown\n",
      "predicted token id:  tensor([3655])\n",
      "labels:  tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110, 14247,  1548,   119,\n",
      "           102]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.01"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "model = BertForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\", max_length=11, padding='max_length')\n",
    "print(inputs)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "print(tokenizer.decode(predicted_token_id))\n",
    "print(\"predicted token id: \", predicted_token_id)\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\", max_length=11, padding='max_length')[\"input_ids\"]\n",
    "\n",
    "print(\"labels: \", labels)\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)\n",
    "\n",
    "#tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110,   103,   119,   102, 0]]\n",
    "#tensor([[  101,  1103,  2364,  1104,   175, 10555,  1110, 14247,  1548,   119, 102]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-14 13:08:01.125343: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-14 13:08:03.440760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-14 13:08:06.716099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/phatpham/anaconda3/envs/min_ds-env/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2024-03-14 13:08:18.571125: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "import math\n",
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple, defaultdict\n",
    "from tempfile import TemporaryDirectory\n",
    "from sklearn.metrics import accuracy_score\n",
    "from babel.dates import format_time\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "# sys.path.insert(1, '/content/SRLPredictionEasel')\n",
    "\n",
    "# from bert_mlm_finetune import BertForMLMPreTraining \n",
    "from transformers import BertTokenizer, BertConfig, AdamW, get_linear_schedule_with_warmup, BertForMaskedLM \n",
    "from utils_mlm import count_num_cpu_gpu\n",
    "import spacy\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", do_lower_case=True)\n",
    "\n",
    "MLM_IGNORE_LABEL_IDX = -1\n",
    "VOCAB_SIZE = 28996 \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "MAX_SEQ_LEN = 85\n",
    "NUM_CPU = count_num_cpu_gpu()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define get pos tag func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(text, index):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Check if the index is within the bounds\n",
    "    if index < 0 or index >= len(doc):\n",
    "       return \"Index out of bounds\"\n",
    "\n",
    "    # Get the POS tag for the word at the specified index\n",
    "    pos_tag = doc[index].pos_\n",
    "    return pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is POS match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_POS_match(logits, input_ids, lm_label_ids):\n",
    "    '''\n",
    "    Function to check if the POS tag of the masked token in the logits is the same as the POS tag of the masked token in the original text.\n",
    "    Note: This function assumes that the logits are of shape # ([85, 28996]) \n",
    "    lm_label_ids: shape (batch_size, sequence_length)\n",
    "    '''\n",
    "    \n",
    "    origin_input_id = input_ids.clone() # Origin input id:  torch.Size([85])\n",
    "   \n",
    "    # Find the index of the masked token from lm_label_ids\n",
    "    masked_idx = torch.where(lm_label_ids != -100)[0]\n",
    "    masked_idx_input = torch.where(input_ids == tokenizer.mask_token_id)[0]\n",
    "   \n",
    "    origin_input_id[masked_idx_input] = lm_label_ids[masked_idx] \n",
    "    \n",
    "    # get pos tag of origin text\n",
    "    text = tokenizer.decode(input_ids)\n",
    "    origin_text = tokenizer.decode(origin_input_id) \n",
    "    print(\"ORIGIN TEXT: \", origin_text)\n",
    "    pos_tag_origin = get_pos_tag(origin_text, masked_idx_input)\n",
    "    print(\"POS TAG ORIGIN: \", pos_tag_origin)\n",
    "   \n",
    "    # Extract the logits for the masked position\n",
    "    masked_logits = logits[0, masked_idx]\n",
    "    print(\"MASKED LOGITS: \", masked_logits) # torch.Size([28996])\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
    "\n",
    "    # Get the token with the highest probability (predicted token)\n",
    "    predicted_token_id = torch.argmax(probabilities).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_token_id])[0]\n",
    "\n",
    "    # Replace the [MASK] token with the predicted token in the original text\n",
    "    result_text = text.replace('[MASK]', predicted_token) \n",
    "    print(\"RESULT TEXT: \", result_text)\n",
    "    \n",
    "    # get pos tag of logits\n",
    "    logits_tag = get_pos_tag(result_text, masked_idx)\n",
    "    print(\"LOGITS TAGS: \", logits_tag)\n",
    "    return pos_tag_origin == logits_tag    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custome loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def custom_loss(input_ids, logits, labels):\n",
    "  \n",
    "    # Cross-entropy term\n",
    "    cross_entropy_term = F.cross_entropy(logits, labels, reduction='none')\n",
    "    print(\"Cross entropy term shape: \", cross_entropy_term.shape)      ##Cross entropy term shape:  torch.Size([2720])\n",
    "    # logits_shape = (32, 85, VOCAB_SIZE)\n",
    "    # logits_tensor = logits.view(*logits_shape)\n",
    "    \n",
    "    # labels_shape = (32, 85)\n",
    "    # labels_tensor = labels.view(*labels_shape)\n",
    "    \n",
    "    \n",
    "    # Custom matching term\n",
    "    matching_term_lst = []\n",
    "    # for logit, input_id, label in zip(logits_tensor, input_ids, labels_tensor):\n",
    "       \n",
    "    matching_term = is_POS_match(logits=logits, input_ids=input_ids, lm_label_ids=labels)\n",
    "    print(\"Matching term: \", matching_term) \n",
    "    matching_term_lst.append(matching_term) \n",
    "    # hay mình thử sửa cái POS cho cái batch luôn kh, tại cái logit với cái label truyền vô là cái batch á\n",
    "    # mà t sợ nhiều khi mình reshape sai nên nó tính sai \n",
    "    matching_term = torch.tensor(matching_term_lst)\n",
    "    # Combine terms\n",
    "    loss = 0.5 * cross_entropy_term + (1 - matching_term)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train with costom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tokens_for_words(words, input_ids, offsets):\n",
    "    '''\n",
    "    Input:\n",
    "        words = ['The', 'capital', 'of', 'the', 'France', 'is', 'Paris', '.']\n",
    "        input_ids = [101, 1109, 3007, 1104, 2605, 1110, 3000, 119, 102]\n",
    "        offsets = [(None, None), (0, 3), (4, 11), (12, 14), (15, 17), (18, 23), (24, 25), (None, None)]\n",
    "    Output: \n",
    "        word_dict = {'The': [tensor(101), tensor(170), tensor(170)], 'capital': [tensor(1109), tensor(3007)], ....}\n",
    "    '''\n",
    "    word_dict = {}\n",
    "    current_word = ''\n",
    "    token_list = []\n",
    "    sentence = ' '.join(words)\n",
    "    print(\"input id shape: \", input_ids.shape)\n",
    "    for j, (token, offset) in enumerate(zip(input_ids, offsets)):\n",
    "        \n",
    "        if offset[0] is not None:  # If the token is associated with a word in the original text\n",
    "            start, end = offset\n",
    "            original_word = sentence[start:end]\n",
    "            # original_word = tokenizer.decode(input_ids[j], skip_special_tokens=True)\n",
    "            if j > 0 and offset[0] == offsets[j - 1][1]:  # Check if the current word should be concatenated\n",
    "                current_word += original_word\n",
    "                token_list.append(token)\n",
    "            else:\n",
    "                if current_word:\n",
    "                    if current_word in word_dict:\n",
    "                        word_dict[current_word].append(token_list)\n",
    "                    else:\n",
    "                        word_dict[current_word] = [token_list]\n",
    "                current_word = original_word\n",
    "                token_list = [token]\n",
    "\n",
    "    # Add the last word\n",
    "    if current_word:\n",
    "        if current_word in word_dict:\n",
    "            word_dict[current_word].append(token_list)\n",
    "        else:\n",
    "            word_dict[current_word] = [token_list]\n",
    "\n",
    "    return {word: [item for sublist in tokens for item in sublist]  for word, tokens in word_dict.items() }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "import torch\n",
    "import copy\n",
    "model = BertForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/mnt/c/Users/Phat Pham/Documents/THESIS/SRLPredictionEasel/MLM/mlm_output/mlm_abolish_full.csv'\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# read the data\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_id</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>[101, 3855, 1104, 11911, 27154, 8419, 8830, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>[101, 1103, 170, 120, 189, 118, 3987, 182, 381...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>[101, 103, 118, 8318, 173, 1306, 117, 1134, 11...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-100, 1884, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>[101, 1107, 1901, 117, 3687, 26883, 1320, 1104...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>[101, 5190, 4789, 1110, 8632, 1107, 19255, 118...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              token_id  \\\n",
       "72   [101, 3855, 1104, 11911, 27154, 8419, 8830, 11...   \n",
       "903  [101, 1103, 170, 120, 189, 118, 3987, 182, 381...   \n",
       "44   [101, 103, 118, 8318, 173, 1306, 117, 1134, 11...   \n",
       "309  [101, 1107, 1901, 117, 3687, 26883, 1320, 1104...   \n",
       "788  [101, 5190, 4789, 1110, 8632, 1107, 19255, 118...   \n",
       "\n",
       "                                        attention_mask  \\\n",
       "72   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "903  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "44   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "309  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "788  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                        token_type_ids  \\\n",
       "72   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "903  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "44   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "309  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "788  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                labels  \n",
       "72   [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "903  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "44   [-100, 1884, -100, -100, -100, -100, -100, -10...  \n",
       "309  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "788  [-100, -100, -100, -100, -100, -100, -100, -10...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(torch.tensor(176)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def get_pos_tag(word, text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.text == word:\n",
    "            return token.pos_\n",
    "    return None\n",
    "\n",
    "def mask_content_words(ids, word_dict, tokenizer):\n",
    "    labels = []\n",
    "    masked_sentences = []\n",
    "    masked_indices = []\n",
    "    except_tokens = [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id, tokenizer.unk_token_id]\n",
    "    \n",
    "    for (key, value) in word_dict.items():\n",
    "        masked_ids = ids.clone()\n",
    "        label = torch.full_like(masked_ids, fill_value=-100)\n",
    "        \n",
    "        if get_pos_tag(key, ' '.join(word_dict.keys())) in ['NOUN', 'VERB', 'ADJ', 'ADV']:\n",
    "           \n",
    "            masked_indice = torch.full_like(masked_ids, fill_value=0)\n",
    "            for idx_value in value:\n",
    "                if torch.isin(idx_value, masked_ids) and idx_value not in except_tokens:\n",
    "                    masked_ids[masked_ids == idx_value.clone().detach()] = tokenizer.mask_token_id\n",
    "                    # get the index of the masked token\n",
    "                    masked_indice[masked_ids == tokenizer.mask_token_id] = 1\n",
    "                    \n",
    "            for idx, mask in enumerate(masked_indice):\n",
    "                if mask == 1:\n",
    "                    label[idx] = ids[idx]\n",
    "            masked_sentences.append(masked_ids)\n",
    "            labels.append(label)\n",
    "            masked_indices.append(masked_indice)\n",
    "    return masked_sentences, labels, masked_indices\n",
    "\n",
    "def masking_sentence_word(words, input_ids, offsets, tokenizer):\n",
    "    '''\n",
    "    Input: \n",
    "        words = ['The', 'capital', 'of', 'France', 'is', 'Paris', '.']\n",
    "    Output: \n",
    "        masked_sentences = [tensor([  101,  1103, 175, 10555,  1110,   103,   119,   102]), [  101,  1103,  2364, 10555,  103,   103,   119,   102]] \n",
    "        label_ids = [tensor(6468)], [tensor(1568), tensor(13892)]\n",
    "    '''\n",
    "    # get a list of token for the word\n",
    "    word_dict = get_tokens_for_words(words, input_ids, offsets)\n",
    "   \n",
    "    # masked the tokens if the word is the content word\n",
    "    masked_sentences, label_ids, masked_indices = mask_content_words(input_ids, word_dict, tokenizer)\n",
    "    \n",
    "    return masked_sentences, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input id shape:  torch.Size([64])\n",
      "masked_sens tensor([  101,   170,   103,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0])\n",
      "label_ids tensor([-100, -100,  176, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "0 114 )\n",
      "1 138 A\n",
      "2 144 G\n",
      "3 118 -\n",
      "4 1106 to\n",
      "5 118 -\n",
      "6 170 a\n",
      "7 6468 transition\n",
      "8 1120 at\n",
      "9 1103 the\n",
      "10 1148 first\n",
      "11 183 n\n",
      "12 21977 ##uc\n",
      "13 26918 ##leo\n",
      "14 23767 ##tide\n",
      "15 1104 of\n",
      "16 27553 intro\n",
      "17 1179 ##n\n",
      "18 123 2\n",
      "19 1104 of\n",
      "20 5351 patient\n",
      "21 122 1\n",
      "22 8632 abolished\n",
      "23 2999 normal\n",
      "24 188 s\n",
      "25 1643 ##p\n",
      "26 22548 ##licing\n",
      "27 119 .\n",
      "28 119 .\n",
      "tensor(4.4969, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# encode xong mới mask\n",
    "### THEO CAI LOSS NAY NHA PHAT OIIIIIII\n",
    "# https://discuss.huggingface.co/t/bertformaskedlm-s-loss-and-scores-how-the-loss-is-computed/607/2\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "  \n",
    "## HÀM MAIN CỦA masking_sentence_word\n",
    "\n",
    "# Example usage:\n",
    "# sentence = \"Cytokines altered the expression and activity of the multidrug resistance transporters in human hepatoma cell lines; analysis using RT-PCR and cDNA microarrays.\"\n",
    "sentence = \"A G-to-A transition at the first nucleotide of intron 2 of patient 1 abolished normal splicing.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Get the POS tag for each word in the text\n",
    "words_str = [word.text for word in [token for token in doc]]\n",
    "\n",
    "tokenized_sentence = tokenizer.encode_plus(\n",
    "    ' '.join(words_str),\n",
    "    max_length=64,\n",
    "    padding='max_length',  # Pad to the maximum sequence length\n",
    "    truncation=True,  # Truncate to the maximum sequence length if necessary\n",
    "    add_special_tokens = True,\n",
    "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    return_attention_mask = True,\n",
    "    return_offsets_mapping=True  # Return offset mappings\n",
    ")\n",
    "input_ids = tokenized_sentence['input_ids'][0]  \n",
    "\n",
    "offsets = tokenized_sentence['offset_mapping'][0]  # Character offsets in the original input text\n",
    "\n",
    "masked_sens, label_ids = masking_sentence_word(words_str, input_ids, offsets, tokenizer)\n",
    "\n",
    "# test mask 0 với mask 3 \n",
    "\n",
    "# labels  = copy.deepcopy(input_ids) #this is the part I changed\n",
    "# input_ids[0][23] = tokenizer.mask_token_id\n",
    "# labels[input_ids != tokenizer.mask_token_id] = -100 \n",
    "\n",
    "# print(\"input_ids\", input_ids)\n",
    "# print(\"labels\", labels)\n",
    "# print(\"attention mask\", input.attention_mask)\n",
    "print(\"masked_sens\", masked_sens[0])\n",
    "print(\"label_ids\", label_ids[0])\n",
    "\n",
    "# convert 1-d tensor to 2-d tensor\n",
    "masked_sens_2d = torch.unsqueeze(masked_sens[0], 0)\n",
    "label_ids_2d = torch.unsqueeze(label_ids[0], 0)\n",
    "\n",
    "output = model(input_ids = masked_sens_2d, attention_mask = tokenized_sentence['attention_mask'], token_type_ids=tokenized_sentence['token_type_ids'], labels=label_ids_2d)\n",
    "\n",
    "for i in range(29):\n",
    "    pred = torch.argmax(output.logits[0][i]).item()\n",
    "    print(i, pred, tokenizer.decode(pred))\n",
    "    \n",
    "# cách 1\n",
    "# logSoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "# NLLLos = torch.nn.NLLLoss()\n",
    "# print(NLLLos( logSoftmax(torch.unsqueeze(output.logits[0][7], 0)), torch.tensor([pred]))) #the same as F.cross_entropy(scores.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "\n",
    "# cách 2: calculate loss manually\n",
    "import torch.nn.functional as F\n",
    "loss2 = F.cross_entropy(output.logits.view(-1, tokenizer.vocab_size), label_ids_2d.view(-1))\n",
    "print(loss2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(torch.tensor(170)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([  101,   170,   103,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100,  176, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "1 tensor([  101,   170,   176,   118,  1106,   118,   170,   103,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100, -100, -100, -100, -100, -100, 6468, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "2 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "          103,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1148, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "3 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   103,   103,   103,   103,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,   183, 21977, 26918, 23767,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100])\n",
      "4 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104,   103,   103,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100, 27553,  1179,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100])\n",
      "5 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "          103,   122,  8632,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, 5351, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "6 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,   103,  2999,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8632, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "7 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,   103,   188,  1643, 22548,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2999,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "8 tensor([  101,   170,   176,   118,  1106,   118,   170,  6468,  1120,  1103,\n",
      "         1148,   183, 21977, 26918, 23767,  1104, 27553,  1179,   123,  1104,\n",
      "         5351,   122,  8632,  2999,   103,   103,   103,   119,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0]) tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,   188,  1643, 22548,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(masked_sens)):\n",
    "    print(i, masked_sens[i], label_ids[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MASK XONG MOI ENCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_sen[i] altered\n",
      "tmp_sen[i] expression\n",
      "tmp_sen[i] activity\n",
      "tmp_sen[i] resistance\n",
      "tmp_sen[i] cell\n",
      "tmp_sen[i] lines\n",
      "tmp_sen[i] analysis\n",
      "tmp_sen[i] using\n",
      "mask_indexs 18\n",
      "mask_sens ['Cytokines', 'altered', 'the', 'expression', 'and', 'activity', 'of', 'the', 'multidrug', 'resistance', 'transporters', 'in', 'human', 'hepatoma', 'cell', 'lines', ';', 'analysis', '[MASK]', 'RT', '-', 'PCR', 'and', 'cDNA', 'microarrays', '.']\n",
      "input id:  tensor([[  101,   172, 25669, 21420,  3965,  8599,  1103,  2838,  1105,  3246,\n",
      "          1104,  1103,  4321, 23632,  9610,  4789,  3936,  1468,  1107,  1769,\n",
      "          1119,  4163, 18778,  1161,  2765,  2442,   132,  3622,   103,   187,\n",
      "          1204,   118,   185,  1665,  1197,  1105,   172, 22834,  1161, 17599,\n",
      "         25203,  6834,   119,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "labels sens:  tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, 1606, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100]])\n",
      "attention mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "0 . 119\n",
      "1 c 172\n",
      "2 ##yt 25669\n",
      "3 ##oki 21420\n",
      "4 ##nes 3965\n",
      "5 altered 8599\n",
      "6 the 1103\n",
      "7 expression 2838\n",
      "8 and 1105\n",
      "9 activity 3246\n",
      "10 of 1104\n",
      "11 the 1103\n",
      "12 multi 4321\n",
      "13 ##dr 23632\n",
      "14 ##ug 9610\n",
      "15 resistance 4789\n",
      "16 transport 3936\n",
      "17 ##ers 1468\n",
      "18 in 1107\n",
      "19 human 1769\n",
      "20 he 1119\n",
      "21 ##pa 4163\n",
      "22 ##tom 18778\n",
      "23 ##a 1161\n",
      "24 cell 2765\n",
      "25 lines 2442\n",
      "26 ; 132\n",
      "27 analysis 3622\n",
      "28 using 1606\n",
      "output logits:  torch.Size([64, 28996])\n",
      "labels sens:  torch.Size([64])\n",
      "tensor(1.1051, grad_fn=<NllLossBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mask xong mới encode\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "import torch\n",
    "model = BertForMaskedLM.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "\n",
    "# text = \"A G-to-A transition at the first nucleotide of intron 2 of patient 1 abolished normal splicing.\".lower()\n",
    "text = \"Cytokines altered the expression and activity of the multidrug resistance transporters in human hepatoma cell lines; analysis using RT-PCR and cDNA microarrays.\"\n",
    "# Get the POS tag for each word in the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get the POS tag for each word in the text\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "words = [token for token in doc]\n",
    "mask_sens, labels_sens_, test_label = masking_sentence_word(words, tokenizer, pos_tags)  # mask là masked_sentence\n",
    "        \n",
    "print(\"mask_sens\", mask_sens[-1])\n",
    "# print(\"labels_sens\", labels_sens[0])\n",
    "           \n",
    "input = tokenizer.encode_plus(text = ' '.join(mask_sens[-1]), return_tensors=\"pt\", add_special_tokens = True, truncation=True, padding='max_length',\n",
    "                                         return_attention_mask = True,  max_length=64) \n",
    "labels_sens  = copy.deepcopy(input['input_ids']) #this is the part I changed\n",
    "labels_sens[input['input_ids'] == tokenizer.mask_token_id] = test_label\n",
    "labels_sens[input['input_ids'] != tokenizer.mask_token_id] = -100 \n",
    "\n",
    "print(\"input id: \" ,input.input_ids)\n",
    "print(\"labels sens: \", labels_sens)\n",
    "print(\"attention mask\", input.attention_mask)\n",
    "\n",
    "# print len attention mask when it is not padding\n",
    "\n",
    "output = model(input_ids = input['input_ids'], attention_mask = input['attention_mask'] , token_type_ids=input['token_type_ids'] , labels=labels_sens.clone().detach())\n",
    "\n",
    "for i in range(29):\n",
    "    pred = torch.argmax(output.logits[0][i]).item()\n",
    "    print(i, tokenizer.decode(pred), pred)\n",
    "\n",
    "print(\"output logits: \", output.logits.view(-1, tokenizer.vocab_size).shape)\n",
    "print(\"labels sens: \", labels_sens.view(-1).shape)\n",
    "# # loss \n",
    "import torch.nn.functional as F\n",
    "loss2 = F.cross_entropy(output.logits.view(-1, tokenizer.vocab_size), labels_sens.view(-1))\n",
    "print(loss2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 28996])\n"
     ]
    }
   ],
   "source": [
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test loss cross entropy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
