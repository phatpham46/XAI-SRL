{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking_sentence(token_ids, tokenizer):\n",
    "    '''\n",
    "    Function to mask each token in a sentence and return the masked sentence and the corresponding label\n",
    "    '''\n",
    "    except_tokens = [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]\n",
    "    \n",
    "    mask = np.random.binomial(1, 0.1, (len(token_ids),))   # Masking each token with 15% probability\n",
    "    print(\"mask:\",  np.where(mask)[0])\n",
    "    token_ids_copy = copy.deepcopy(token_ids)\n",
    "    masked_sentence = pd.DataFrame()\n",
    "    labels = [-100] * len(token_ids)\n",
    "    for idx, token in enumerate(token_ids):\n",
    "        \n",
    "        if (idx in np.where(mask)[0]) and (idx not in except_tokens):\n",
    "            token_ids_copy[idx] = tokenizer.mask_token_id\n",
    "            labels[idx] = token\n",
    "        \n",
    "    masked_sentence['masked_token_id'] = [token_ids_copy]\n",
    "    masked_sentence['label_id'] = [labels]\n",
    "         \n",
    "    return masked_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = [101,1107,5014,117,1748,3687,26883,1320,1104,1103,172,118,6020,14715,11179,16617,5777,1107,1103,185,7897,1571,21392,1116,171,1604,1105,171,1580,8632,15416,1348,23842,117,6142,4422,3687,26883,1320,1104,1103,21996,184,5822,11478,27105,17853,113,11769,102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load the BioBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")\n",
    "\n",
    "# Load the BioBERT model for token classification (POS tagging)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags_in_biobert_vocab(text, vocab):\n",
    "    # Tokenize the text using BioBERT's tokenizer\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "    \n",
    "    # Get the BIO-style POS tags using BioBERT\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs)\n",
    "    pos_tags = torch.argmax(outputs.logits, dim=2).tolist()[0]\n",
    "    \n",
    "    # Create a list to store POS tags and their indices\n",
    "    pos_tags_with_indices = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Check if the token is in the vocabulary\n",
    "        if token in vocab:\n",
    "            # Get the token's index in the vocabulary\n",
    "            vocab_index = vocab.index(token)\n",
    "\n",
    "            # Get the corresponding POS tag from BioBERT\n",
    "            pos_tag_id = pos_tags[i]\n",
    "            pos_tag = tokenizer.convert_ids_to_tokens(pos_tag_id)\n",
    "            print(pos_tag)\n",
    "            # Add the token, its index, and the POS tag to the list\n",
    "            pos_tags_with_indices.append((token, vocab_index, pos_tag))\n",
    "\n",
    "    return pos_tags_with_indices\n",
    "\n",
    "# Example usage:\n",
    "text = \"A G-to-A transition at the first nucleotide of intron 2 of patient 1 abolished normal splicing.\"\n",
    "biobert_vocab = [\"[CLS]\", \"[SEP]\", \"example\", \"tags\"]\n",
    "\n",
    "pos_tags_in_biobert_vocab = get_pos_tags_in_biobert_vocab(text, biobert_vocab)\n",
    "for token, index, pos_tag in pos_tags_in_biobert_vocab:\n",
    "    print(f\"Token: {token}, Vocabulary Index: {index}, POS Tag: {pos_tag}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
